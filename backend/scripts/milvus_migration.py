#!/usr/bin/env python3
"""
Milvusè¿ç§»è„šæœ¬
æ‰§è¡Œä»Elasticsearchåˆ°Milvusçš„å®Œæ•´æ•°æ®è¿ç§»
"""

import asyncio
import argparse
import logging
import sys
import os
from datetime import datetime
from typing import Dict, Any, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.milvus import (
    MilvusService, DataMigrationService, HybridSearchService, MilvusOptimizationService
)
from app.services.milvus.models import MigrationResult
from elasticsearch import AsyncElasticsearch

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'milvus_migration_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)


class MilvusMigrationManager:
    """Milvusè¿ç§»ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è¿ç§»ç®¡ç†å™¨

        Args:
            config: è¿ç§»é…ç½®
        """
        self.config = config
        self.milvus_service = None
        self.es_client = None
        self.migration_service = None
        self.hybrid_search_service = None
        self.optimization_service = None

    async def initialize_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–è¿ç§»æœåŠ¡...")

            # 1. åˆå§‹åŒ–MilvusæœåŠ¡
            self.milvus_service = MilvusService(
                host=self.config['milvus']['host'],
                port=self.config['milvus']['port'],
                user=self.config['milvus'].get('user', ''),
                password=self.config['milvus'].get('password', ''),
                db_name=self.config['milvus'].get('db_name', 'default')
            )

            # è¿æ¥åˆ°Milvus
            milvus_connected = await self.milvus_service.connect()
            if not milvus_connected:
                raise Exception("æ— æ³•è¿æ¥åˆ°MilvusæœåŠ¡å™¨")

            logger.info("âœ… MilvusæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            # 2. åˆå§‹åŒ–ESå®¢æˆ·ç«¯
            self.es_client = AsyncElasticsearch(
                hosts=[{
                    'host': self.config['elasticsearch']['host'],
                    'port': self.config['elasticsearch']['port'],
                    'scheme': 'http'
                }],
                basic_auth=(
                    self.config['elasticsearch'].get('user', ''),
                    self.config['elasticsearch'].get('password', '')
                ) if self.config['elasticsearch'].get('user') else None
            )

            # æµ‹è¯•ESè¿æ¥
            es_connected = await self.es_client.ping()
            if not es_connected:
                raise Exception("æ— æ³•è¿æ¥åˆ°ElasticsearchæœåŠ¡å™¨")

            logger.info("âœ… ElasticsearchæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            # 3. åˆå§‹åŒ–è¿ç§»æœåŠ¡
            self.migration_service = DataMigrationService(
                es_client=self.es_client,
                milvus_service=self.milvus_service,
                batch_size=self.config['migration']['batch_size'],
                max_workers=self.config['migration']['max_workers'],
                validation_sample_rate=self.config['migration']['validation_sample_rate']
            )

            logger.info("âœ… æ•°æ®è¿ç§»æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            # 4. åˆå§‹åŒ–æ··åˆæœç´¢æœåŠ¡
            self.hybrid_search_service = HybridSearchService(
                milvus_service=self.milvus_service,
                es_client=self.es_client,
                vector_weight=self.config['search']['vector_weight'],
                text_weight=self.config['search']['text_weight']
            )

            logger.info("âœ… æ··åˆæœç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            # 5. åˆå§‹åŒ–ä¼˜åŒ–æœåŠ¡
            self.optimization_service = MilvusOptimizationService(self.milvus_service)

            logger.info("âœ… ä¼˜åŒ–æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            logger.info("ğŸ‰ æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

    async def migrate_user(self, user_id: int, dry_run: bool = False) -> MigrationResult:
        """
        è¿ç§»å•ä¸ªç”¨æˆ·çš„æ•°æ®

        Args:
            user_id: ç”¨æˆ·ID
            dry_run: æ˜¯å¦åªè¿›è¡Œé¢„æ£€æŸ¥

        Returns:
            è¿ç§»ç»“æœ
        """
        try:
            logger.info(f"{'[é¢„æ£€æŸ¥] ' if dry_run else ''}å¼€å§‹è¿ç§»ç”¨æˆ· {user_id} çš„æ•°æ®...")

            if dry_run:
                # é¢„æ£€æŸ¥æ¨¡å¼
                result = await self._dry_run_migration(user_id)
            else:
                # å®é™…è¿ç§»
                result = await self.migration_service.migrate_user_data(user_id)

            logger.info(f"ç”¨æˆ· {user_id} è¿ç§»å®Œæˆ:")
            logger.info(f"  ğŸ“Š æ€»è®°å½•æ•°: {result.total_migrated}")
            logger.info(f"  âœ… æˆåŠŸ: {result.success_count}")
            logger.info(f"  âŒ å¤±è´¥: {result.failed_count}")
            logger.info(f"  âœ“ éªŒè¯é€šè¿‡: {result.validation_passed}")
            logger.info(f"  â±ï¸  è€—æ—¶: {result.migration_time:.2f}ç§’")

            if result.errors:
                logger.error(f"  âš ï¸  é”™è¯¯æ•°: {len(result.errors)}")
                for i, error in enumerate(result.errors[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    logger.error(f"    - {error}")

            return result

        except Exception as e:
            logger.error(f"ç”¨æˆ· {user_id} è¿ç§»å¤±è´¥: {e}")
            return MigrationResult(
                user_id=str(user_id),
                total_migrated=0,
                success_count=0,
                failed_count=0,
                validation_passed=False,
                migration_time=0.0,
                start_time=datetime.now(),
                end_time=datetime.now(),
                errors=[str(e)]
            )

    async def _dry_run_migration(self, user_id: int) -> MigrationResult:
        """é¢„æ£€æŸ¥è¿ç§»"""
        try:
            logger.info(f"æ‰§è¡Œç”¨æˆ· {user_id} çš„é¢„æ£€æŸ¥...")

            # 1. æ£€æŸ¥ESä¸­çš„æ•°æ®
            es_stats = await self.migration_service.get_es_statistics(user_id)
            total_documents = es_stats.get('total_documents', 0)

            if total_documents == 0:
                logger.warning(f"ç”¨æˆ· {user_id} åœ¨ESä¸­æ²¡æœ‰æ•°æ®")
                return MigrationResult(
                    user_id=str(user_id),
                    total_migrated=0,
                    success_count=0,
                    failed_count=0,
                    validation_passed=False,
                    migration_time=0.0,
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    errors=["No data found in Elasticsearch"]
                )

            logger.info(f"é¢„æ£€æŸ¥é€šè¿‡ - å‘ç° {total_documents} æ¡è®°å½•")

            # 2. æ£€æŸ¥Milvusè¿æ¥
            health_check = await self.milvus_service.health_check()
            if health_check.get("status") != "healthy":
                raise Exception("MilvusæœåŠ¡ä¸å¥åº·")

            # 3. æ¨¡æ‹Ÿè¿ç§»ç»“æœ
            return MigrationResult(
                user_id=str(user_id),
                total_migrated=total_documents,
                success_count=total_documents,
                failed_count=0,
                validation_passed=True,
                migration_time=0.0,
                start_time=datetime.now(),
                end_time=datetime.now(),
                errors=[]
            )

        except Exception as e:
            logger.error(f"é¢„æ£€æŸ¥å¤±è´¥: {e}")
            return MigrationResult(
                user_id=str(user_id),
                total_migrated=0,
                success_count=0,
                failed_count=0,
                validation_passed=False,
                migration_time=0.0,
                start_time=datetime.now(),
                end_time=datetime.now(),
                errors=[str(e)]
            )

    async def migrate_multiple_users(self, user_ids: List[int],
                                   batch_size: int = 1,
                                   dry_run: bool = False) -> Dict[str, Any]:
        """
        æ‰¹é‡è¿ç§»å¤šä¸ªç”¨æˆ·

        Args:
            user_ids: ç”¨æˆ·IDåˆ—è¡¨
            batch_size: å¹¶å‘æ‰¹æ¬¡å¤§å°
            dry_run: æ˜¯å¦åªè¿›è¡Œé¢„æ£€æŸ¥

        Returns:
            æ‰¹é‡è¿ç§»ç»“æœ
        """
        try:
            total_users = len(user_ids)
            logger.info(f"å¼€å§‹æ‰¹é‡è¿ç§» {total_users} ä¸ªç”¨æˆ·...")

            results = []
            failed_users = []
            start_time = datetime.now()

            # åˆ†æ‰¹å¤„ç†
            for i in range(0, total_users, batch_size):
                batch = user_ids[i:i + batch_size]
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(total_users + batch_size - 1)//batch_size}")

                # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_tasks = [
                    self.migrate_user(user_id, dry_run) for user_id in batch
                ]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

                # å¤„ç†ç»“æœ
                for user_id, result in zip(batch, batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"ç”¨æˆ· {user_id} è¿ç§»å¼‚å¸¸: {result}")
                        failed_users.append(user_id)
                        results.append(MigrationResult(
                            user_id=str(user_id),
                            total_migrated=0,
                            success_count=0,
                            failed_count=0,
                            validation_passed=False,
                            migration_time=0.0,
                            start_time=datetime.now(),
                            end_time=datetime.now(),
                            errors=[str(result)]
                        ))
                    else:
                        results.append(result)
                        if not result.validation_passed or result.failed_count > 0:
                            failed_users.append(user_id)

            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()

            # ç»Ÿè®¡æ±‡æ€»
            total_migrated = sum(r.total_migrated for r in results)
            total_success = sum(r.success_count for r in results)
            total_failed = sum(r.failed_count for r in results)

            summary = {
                "total_users": total_users,
                "successful_users": total_users - len(failed_users),
                "failed_users": len(failed_users),
                "total_migrated": total_migrated,
                "total_success": total_success,
                "total_failed": total_failed,
                "success_rate": (total_users - len(failed_users)) / total_users * 100 if total_users > 0 else 0,
                "total_time": total_time,
                "failed_user_ids": failed_users,
                "individual_results": results
            }

            logger.info(f"æ‰¹é‡è¿ç§»å®Œæˆ:")
            logger.info(f"  ğŸ“Š æ€»ç”¨æˆ·æ•°: {summary['total_users']}")
            logger.info(f"  âœ… æˆåŠŸç”¨æˆ·: {summary['successful_users']}")
            logger.info(f"  âŒ å¤±è´¥ç”¨æˆ·: {summary['failed_users']}")
            logger.info(f"  ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            logger.info(f"  ğŸ“‹ æ€»è®°å½•æ•°: {summary['total_migrated']}")
            logger.info(f"  â±ï¸  æ€»è€—æ—¶: {summary['total_time']:.2f}ç§’")

            return summary

        except Exception as e:
            logger.error(f"æ‰¹é‡è¿ç§»å¤±è´¥: {e}")
            raise e

    async def optimize_collections(self, collection_names: List[str],
                                 optimization_level: str = "balanced") -> Dict[str, Any]:
        """
        ä¼˜åŒ–å¤šä¸ªé›†åˆ

        Args:
            collection_names: é›†åˆåç§°åˆ—è¡¨
            optimization_level: ä¼˜åŒ–çº§åˆ«

        Returns:
            ä¼˜åŒ–ç»“æœæ±‡æ€»
        """
        try:
            logger.info(f"å¼€å§‹ä¼˜åŒ– {len(collection_names)} ä¸ªé›†åˆ...")

            optimization_results = []
            start_time = datetime.now()

            # é€ä¸ªä¼˜åŒ–é›†åˆ
            for collection_name in collection_names:
                try:
                    logger.info(f"ä¼˜åŒ–é›†åˆ: {collection_name}")
                    result = await self.optimization_service.optimize_collection(
                        collection_name, optimization_level
                    )
                    optimization_results.append(result)
                except Exception as e:
                    logger.error(f"é›†åˆ {collection_name} ä¼˜åŒ–å¤±è´¥: {e}")

            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()

            # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            report = self._generate_optimization_report(optimization_results, total_time)

            logger.info(f"é›†åˆä¼˜åŒ–å®Œæˆ - æ€»è€—æ—¶: {total_time:.2f}ç§’")
            return report

        except Exception as e:
            logger.error(f"é›†åˆä¼˜åŒ–å¤±è´¥: {e}")
            raise e

    def _generate_optimization_report(self, results: List, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        try:
            if not results:
                return {"status": "no_results", "total_time": total_time}

            # ç»Ÿè®¡æ”¹è¿›æƒ…å†µ
            total_improvements = {}
            recommendations = []

            for result in results:
                for metric, improvement in result.improvement_ratio.items():
                    if metric not in total_improvements:
                        total_improvements[metric] = []
                    total_improvements[metric].append(improvement)

                recommendations.extend(result.recommendations)

            # è®¡ç®—å¹³å‡æ”¹è¿›
            avg_improvements = {}
            for metric, improvements in total_improvements.items():
                avg_improvements[metric] = sum(improvements) / len(improvements)

            report = {
                "total_collections": len(results),
                "total_time": total_time,
                "average_improvements": avg_improvements,
                "recommendations": list(set(recommendations)),  # å»é‡
                "individual_results": [result.__dict__ for result in results]
            }

            return report

        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e), "total_time": total_time}

    async def test_search_performance(self, user_id: int, test_queries: List[str]) -> Dict[str, Any]:
        """
        æµ‹è¯•æœç´¢æ€§èƒ½

        Args:
            user_id: ç”¨æˆ·ID
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨

        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æœç´¢æ€§èƒ½æµ‹è¯• - ç”¨æˆ·: {user_id}")

            results = {
                "user_id": user_id,
                "test_queries": [],
                "average_latency": 0.0,
                "total_tests": len(test_queries)
            }

            total_latency = 0.0

            # æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
            for query in test_queries:
                try:
                    start_time = datetime.now()

                    # æ‰§è¡Œæ··åˆæœç´¢
                    search_request = type('SearchRequest', (), {
                        'query': query,
                        'kb_id': str(user_id),
                        'top_k': 10,
                        'offset': 0,
                        'similarity_threshold': None
                    })()

                    response = await self.hybrid_search_service.search(search_request)

                    end_time = datetime.now()
                    latency = (end_time - start_time).total_seconds() * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

                    total_latency += latency

                    test_result = {
                        "query": query,
                        "latency_ms": latency,
                        "results_count": len(response.results),
                        "success": True
                    }

                    results["test_queries"].append(test_result)
                    logger.info(f"æŸ¥è¯¢ '{query}' - å»¶è¿Ÿ: {latency:.2f}ms, ç»“æœæ•°: {len(response.results)}")

                except Exception as e:
                    logger.error(f"æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
                    results["test_queries"].append({
                        "query": query,
                        "latency_ms": 0,
                        "results_count": 0,
                        "success": False,
                        "error": str(e)
                    })

            # è®¡ç®—å¹³å‡å»¶è¿Ÿ
            successful_tests = [t for t in results["test_queries"] if t["success"]]
            if successful_tests:
                results["average_latency"] = total_latency / len(successful_tests)

            logger.info(f"æ€§èƒ½æµ‹è¯•å®Œæˆ - å¹³å‡å»¶è¿Ÿ: {results['average_latency']:.2f}ms")
            return results

        except Exception as e:
            logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†è¿ç§»ç®¡ç†å™¨èµ„æº...")

            # æ¸…ç†å„æœåŠ¡
            if self.migration_service:
                await self.migration_service.cleanup()

            if self.hybrid_search_service:
                await self.hybrid_search_service.cleanup()

            if self.optimization_service:
                await self.optimization_service.cleanup()

            # æ–­å¼€è¿æ¥
            if self.milvus_service:
                await self.milvus_service.disconnect()

            if self.es_client:
                await self.es_client.close()

            logger.info("âœ… è¿ç§»ç®¡ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        raise e


def save_results(results: Dict[str, Any], output_path: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Milvusè¿ç§»è„šæœ¬')
    parser.add_argument('--config', '-c', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--user-id', '-u', type=int, help='è¦è¿ç§»çš„ç”¨æˆ·ID')
    parser.add_argument('--user-ids', help='è¦è¿ç§»çš„ç”¨æˆ·IDåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='åªè¿›è¡Œé¢„æ£€æŸ¥')
    parser.add_argument('--batch-size', type=int, default=1, help='å¹¶å‘æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--optimize', action='store_true', help='è¿ç§»åä¼˜åŒ–é›†åˆ')
    parser.add_argument('--test-search', action='store_true', help='æµ‹è¯•æœç´¢æ€§èƒ½')
    parser.add_argument('--output', '-o', help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)

        # åˆ›å»ºè¿ç§»ç®¡ç†å™¨
        migration_manager = MilvusMigrationManager(config)
        await migration_manager.initialize_services()

        # æ‰§è¡Œè¿ç§»
        if args.user_id:
            # å•ä¸ªç”¨æˆ·è¿ç§»
            result = await migration_manager.migrate_user(args.user_id, args.dry_run)
            results = {"single_user": result}

        elif args.user_ids:
            # å¤šä¸ªç”¨æˆ·è¿ç§»
            user_ids = [int(uid.strip()) for uid in args.user_ids.split(',')]
            results = await migration_manager.migrate_multiple_users(
                user_ids, args.batch_size, args.dry_run
            )

        else:
            logger.error("å¿…é¡»æŒ‡å®š --user-id æˆ– --user-ids")
            return

        # åç»­å¤„ç†
        if not args.dry_run:
            # è·å–è¿ç§»çš„é›†åˆåˆ—è¡¨
            migrated_collections = []
            if args.user_id:
                migrated_collections.append(f"user_{args.user_id}_documents")
            elif args.user_ids:
                user_ids = [int(uid.strip()) for uid in args.user_ids.split(',')]
                migrated_collections = [f"user_{uid}_documents" for uid in user_ids]

            # ä¼˜åŒ–é›†åˆ
            if args.optimize and migrated_collections:
                logger.info("å¼€å§‹ä¼˜åŒ–é›†åˆ...")
                optimization_report = await migration_manager.optimize_collections(
                    migrated_collections, optimization_level="balanced"
                )
                results["optimization"] = optimization_report

            # æµ‹è¯•æœç´¢æ€§èƒ½
            if args.test_search and args.user_id:
                logger.info("å¼€å§‹æµ‹è¯•æœç´¢æ€§èƒ½...")
                test_queries = [
                    "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•",
                    "æœºå™¨å­¦ä¹ ç®—æ³•",
                    "æ·±åº¦å­¦ä¹ åº”ç”¨",
                    "è‡ªç„¶è¯­è¨€å¤„ç†",
                    "è®¡ç®—æœºè§†è§‰æŠ€æœ¯"
                ]
                performance_results = await migration_manager.test_search_performance(
                    args.user_id, test_queries
                )
                results["performance_test"] = performance_results

        # ä¿å­˜ç»“æœ
        if args.output:
            save_results(results, args.output)

        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("è¿ç§»ç»“æœæ‘˜è¦")
        print("="*60)

        if args.user_id:
            result = results["single_user"]
            print(f"ç”¨æˆ·ID: {result.user_id}")
            print(f"æ€»è®°å½•æ•°: {result.total_migrated}")
            print(f"æˆåŠŸ: {result.success_count}")
            print(f"å¤±è´¥: {result.failed_count}")
            print(f"éªŒè¯é€šè¿‡: {result.validation_passed}")
            print(f"è€—æ—¶: {result.migration_time:.2f}ç§’")
        else:
            summary = results
            print(f"æ€»ç”¨æˆ·æ•°: {summary['total_users']}")
            print(f"æˆåŠŸç”¨æˆ·: {summary['successful_users']}")
            print(f"å¤±è´¥ç”¨æˆ·: {summary['failed_users']}")
            print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            print(f"æ€»è®°å½•æ•°: {summary['total_migrated']}")
            print(f"æ€»è€—æ—¶: {summary['total_time']:.2f}ç§’")

        print("="*60)

    except Exception as e:
        logger.error(f"è¿ç§»è¿‡ç¨‹å¤±è´¥: {e}")
        sys.exit(1)

    finally:
        # æ¸…ç†èµ„æº
        if 'migration_manager' in locals():
            await migration_manager.cleanup()


if __name__ == "__main__":
    asyncio.run(main())