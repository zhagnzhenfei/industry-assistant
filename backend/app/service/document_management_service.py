import os
import time
import hashlib
from typing import List, Optional
from fastapi import HTTPException, status
from utils.database import default_manager
from models import Document, User
from schemas.document import MilvusSearchResponse, MilvusSearchResult
from service.core.file_parse import execute_insert_process


class DocumentManagementService:
    """æ–‡æ¡£ç®¡ç†æœåŠ¡"""
    
    def __init__(self):
        pass
    
    def calculate_file_hash(self, file_name: str) -> str:
        """è®¡ç®—æ–‡ä»¶åçš„SHA256å“ˆå¸Œå€¼"""
        hash_sha256 = hashlib.sha256()
        hash_sha256.update(file_name.encode('utf-8'))
        return hash_sha256.hexdigest()
    
    def check_duplicate_file(self, user_id: str, file_hash: str) -> Optional[Document]:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡ç›¸åŒæ–‡ä»¶åçš„æ–‡ä»¶"""
        try:
            db = default_manager.session_factory()
            try:
                existing_doc = db.query(Document).filter(
                    Document.user_id == user_id,
                    Document.file_hash == file_hash
                ).first()
                return existing_doc
            finally:
                db.close()
        except Exception as e:
            print(f"æ£€æŸ¥é‡å¤æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def create_document(self, user_id: str, file_name: str, file_path: str, 
                       file_size: int = None, file_type: str = None, file_hash: str = None) -> Document:
        """åˆ›å»ºæ–‡æ¡£è®°å½•"""
        try:
            db = default_manager.session_factory()
            try:
                # åˆ›å»ºæ–°æ–‡æ¡£è®°å½•
                new_document = Document(
                    user_id=user_id,
                    file_name=file_name,
                    file_path=file_path,
                    file_size=file_size,
                    file_type=file_type,
                    file_hash=file_hash,
                    status='uploading'
                )
                
                db.add(new_document)
                db.commit()
                db.refresh(new_document)
                
                return new_document
                
            finally:
                db.close()
                
        except Exception as e:
            print(f"åˆ›å»ºæ–‡æ¡£è®°å½•å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"åˆ›å»ºæ–‡æ¡£è®°å½•å¤±è´¥: {str(e)}"
            )
    
    def update_document_status(self, document_id: str, status: str, 
                             chunk_count: int = None, error_message: str = None) -> Document:
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
        try:
            db = default_manager.session_factory()
            try:
                document = db.query(Document).filter(Document.document_id == document_id).first()
                if not document:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail="æ–‡æ¡£ä¸å­˜åœ¨"
                    )
                
                document.status = status
                document.updated_at = int(time.time())
                
                if chunk_count is not None:
                    document.chunk_count = chunk_count
                if error_message is not None:
                    document.error_message = error_message
                
                db.commit()
                db.refresh(document)
                
                return document
                
            finally:
                db.close()
                
        except Exception as e:
            print(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}"
            )
    
    def process_document_to_vector_store(self, file_path: str, file_name: str, user_id: str) -> int:
        """å¤„ç†æ–‡æ¡£å¹¶å­˜å…¥Milvuså‘é‡æ•°æ®åº“"""
        try:
            import time
            from service.core.file_parse import parse
            from service.core.rag.nlp.model import generate_embedding
            from services.milvus.milvus_service import MilvusService
            from services.milvus.models import DocumentChunk, CollectionConfig
            import xxhash

            print(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_name} ç”¨æˆ·ID: {user_id}")

            # è§£ææ–‡æ¡£
            chunks = parse(file_path)
            if not chunks:
                raise ValueError("æ–‡æ¡£è§£æå¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°ä»»ä½•å†…å®¹")

            print(f"æ–‡æ¡£è§£ææˆåŠŸï¼Œè·å¾— {len(chunks)} ä¸ªåˆ†å—")


            # åˆå§‹åŒ–MilvusæœåŠ¡
            try:
                milvus_service = MilvusService(
                    host=os.getenv("MILVUS_HOST", "milvus-standalone"),
                    port=os.getenv("MILVUS_PORT", "19530")
                )

                # è¿æ¥Milvusï¼ˆä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼‰
                connected = milvus_service.connect_sync()
                if connected:
                    print("âœ… Milvusè¿æ¥æˆåŠŸ")
                    use_milvus = True
                else:
                    print("âŒ Milvusè¿æ¥å¤±è´¥")
                    use_milvus = False
            except Exception as e:
                print(f"Milvusåˆå§‹åŒ–å¤±è´¥: {e}")
                use_milvus = False

            # æ„å»ºé›†åˆåç§°
            collection_name = f"user_collection_{user_id}".replace("-", "_")
            print(f"é›†åˆåç§°: {collection_name}")

            # ç¡®ä¿Milvusé›†åˆå­˜åœ¨ - ä½¿ç”¨å…¨å±€è¿æ¥
            if use_milvus:
                try:
                    # ä½¿ç”¨å…¨å±€è¿æ¥åç§°è¿æ¥
                    from pymilvus import connections, utility

                    # ç¡®ä¿ä½¿ç”¨é»˜è®¤è¿æ¥
                    if not connections.has_connection("default"):
                        print("åˆ›å»ºé»˜è®¤Milvusè¿æ¥...")
                        connections.connect(
                            alias="default",
                            host=os.getenv("MILVUS_HOST", "milvus-standalone"),
                            port=os.getenv("MILVUS_PORT", "19530")
                        )

                    print(f"ğŸ” æ£€æŸ¥Milvusé›†åˆ: {collection_name}")

                    # å¼ºåˆ¶è°ƒç”¨create_collection_syncæ¥æ£€æŸ¥å’Œå¯èƒ½é‡å»ºé›†åˆ
                    # æ— è®ºé›†åˆæ˜¯å¦å­˜åœ¨ï¼Œéƒ½è¦æ£€æŸ¥schemaæ˜¯å¦æ­£ç¡®
                    config = CollectionConfig(collection_name=collection_name)
                    success = milvus_service.create_collection_sync(collection_name, config)

                    if success:
                        if not utility.has_collection(collection_name):
                            print("âœ… Milvusé›†åˆåˆ›å»ºæˆåŠŸ")
                        else:
                            print("âœ… Milvusé›†åˆæ£€æŸ¥/é‡å»ºæˆåŠŸ")

                        # åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        index_success = milvus_service.create_index_sync(collection_name, "vector")
                        if index_success:
                            print("âœ… Milvusç´¢å¼•åˆ›å»ºæˆåŠŸ")
                        else:
                            print("âŒ Milvusç´¢å¼•åˆ›å»ºå¤±è´¥")
                    else:
                        print("âŒ Milvusé›†åˆåˆ›å»ºå¤±è´¥")
                except Exception as e:
                    print(f"Milvusé›†åˆæ“ä½œå¤±è´¥: {e}")
                    use_milvus = False

            # å¤„ç†æ¯ä¸ªåˆ†å—
            milvus_chunks = []

            for i, chunk_data in enumerate(chunks):
                try:
                    print(f"å¤„ç†åˆ†å— {i+1}/{len(chunks)}...")
                    print(chunk_data)

                    # ç”Ÿæˆå”¯ä¸€çš„chunk_idï¼ŒåŒ…å«æ–‡æ¡£å”¯ä¸€æ ‡è¯†å’Œåˆ†å—ç´¢å¼•
                    doc_unique_id = chunk_data.get('doc_id', file_name) or file_name
                    unique_string = f"{doc_unique_id}_{user_id}_{i}_{chunk_data['content_with_weight']}"
                    chunk_id = xxhash.xxh64(unique_string.encode("utf-8")).hexdigest()

                    # ç”Ÿæˆå‘é‡åµŒå…¥
                    embedding = generate_embedding(chunk_data['content_with_weight'])

                    # æ„å»ºMilvusåˆ†å—
                    if use_milvus:
                        milvus_chunk = DocumentChunk(
                            vector=embedding,
                            content=chunk_data['content_with_weight'],
                            content_ltks=chunk_data.get('content_ltks', ''),
                            doc_id=chunk_data.get('doc_id', chunk_data.get('docnm_kwd', '')),
                            doc_name=file_name,
                            kb_id=user_id,
                            chunk_id=chunk_id,
                            category="document",
                            timestamp=int(time.time()),
                            source=file_name,
                            keywords="",
                            metadata={"chunk_index": i, "chunk_id": chunk_id}
                        )
                        milvus_chunks.append(milvus_chunk)

                except Exception as e:
                    print(f"å¤„ç†åˆ†å— {i} å¤±è´¥: {e}")
                    continue

            print(f"å¤„ç†å®Œæˆ, Milvusåˆ†å—: {len(milvus_chunks)}")

            # æ‰¹é‡æ’å…¥åˆ°Milvus
            if use_milvus and milvus_chunks:
                print(f"æ’å…¥æ•°æ®åˆ°Milvusï¼Œå…± {len(milvus_chunks)} æ¡...")
                success = milvus_service.insert_data_sync(collection_name, milvus_chunks)
                if success:
                    print("âœ… Milvusæ’å…¥æˆåŠŸ")
                else:
                    print("âŒ Milvusæ’å…¥å¤±è´¥")
                    # å¦‚æœMilvusæ’å…¥å¤±è´¥ï¼Œæ¸…ç©ºmilvus_chunksè¡¨ç¤ºå¤„ç†å¤±è´¥
                    milvus_chunks = []

            print(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç† {len(milvus_chunks)} ä¸ªåˆ†å—")

            # è¿”å›æˆåŠŸå¤„ç†çš„å—æ•°
            return len(milvus_chunks)

        except Exception as e:
            print(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
            )

    async def search_documents_with_milvus(self, user_id: str, query: str, top_k: int = 10,
                                         similarity_threshold: float = 0.2, category: Optional[str] = None,
                                         enable_hybrid_search: bool = False,
                                         vector_weight: float = 0.5,
                                         text_threshold: float = 0.3) -> MilvusSearchResponse:
        """ä½¿ç”¨Milvusè¿›è¡Œå‘é‡æœç´¢æˆ–æ··åˆæœç´¢ï¼ˆå‘é‡+æ–‡æœ¬ï¼‰

        Args:
            user_id: ç”¨æˆ·ID
            query: æœç´¢æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼
            category: æ–‡æ¡£ç±»åˆ«è¿‡æ»¤
            enable_hybrid_search: æ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢
            vector_weight: å‘é‡æ£€ç´¢æƒé‡ï¼ŒèŒƒå›´0-1ï¼Œæ–‡æœ¬æƒé‡è‡ªåŠ¨è®¡ç®—ä¸º1-vector_weight
            text_threshold: æ–‡æœ¬ç›¸å…³æ€§é˜ˆå€¼

        Returns:
            MilvusSearchResponse: æœç´¢ç»“æœå“åº”
        """
        try:
            import time
            import os
            from services.milvus.milvus_service import MilvusService

            search_type = "hybrid" if enable_hybrid_search else "vector"
            print(f"å¼€å§‹{search_type}æœç´¢: {query}")
            print(f"ğŸ”§ æ··åˆæœç´¢å‚æ•°è°ƒè¯•:")
            print(f"  enable_hybrid_search: {enable_hybrid_search}")
            print(f"  vector_weight: {vector_weight}")
            print(f"  text_threshold: {text_threshold}")
            if enable_hybrid_search:
                text_weight = 1.0 - vector_weight
                print(f"æ··åˆæœç´¢å‚æ•° - å‘é‡æƒé‡: {vector_weight}, æ–‡æœ¬æƒé‡: {text_weight:.3f}, æ–‡æœ¬é˜ˆå€¼: {text_threshold}")

            # åˆå§‹åŒ–MilvusæœåŠ¡
            milvus_service = MilvusService(
                host=os.getenv("MILVUS_HOST", "milvus-standalone"),
                port=os.getenv("MILVUS_PORT", "19530")
            )

            # è¿æ¥Milvus
            if not await milvus_service.connect():
                print("âŒ æ— æ³•è¿æ¥åˆ°MilvusæœåŠ¡")
                return MilvusSearchResponse(
                    query=query,
                    total=0,
                    results=[],
                    search_time=0.0,
                    search_type=search_type,
                    vector_results_count=0,
                    text_results_count=0
                )

            # æ„å»ºé›†åˆåç§°
            collection_name = f"user_collection_{user_id}".replace("-", "_")

            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            from pymilvus import utility
            if not utility.has_collection(collection_name):
                print(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
                return MilvusSearchResponse(
                    query=query,
                    total=0,
                    results=[],
                    search_time=0.0,
                    search_type=search_type,
                    vector_results_count=0,
                    text_results_count=0
                )

            # è‡ªåŠ¨è®¡ç®—æ–‡æœ¬æƒé‡
            text_weight = 1.0 - vector_weight
            print(f"âš–ï¸  æƒé‡åˆ†é…: vector_weight={vector_weight}, text_weight={text_weight:.3f}")

            # æ£€æŸ¥æƒé‡æœ‰æ•ˆæ€§
            if vector_weight < 0 or vector_weight > 1:
                print("âš ï¸  å‘é‡æƒé‡è¶…å‡ºèŒƒå›´0-1")
                return MilvusSearchResponse(
                    query=query,
                    total=0,
                    results=[],
                    search_time=0.0,
                    search_type="error",
                    vector_results_count=0,
                    text_results_count=0
                )

            # å¯¼å…¥å¿…è¦çš„ç±»ï¼ˆé¿å…æ¡ä»¶å¯¼å…¥å¯¼è‡´çš„é—®é¢˜ï¼‰
            from schemas.document import MilvusSearchResult

            # åˆå§‹åŒ–å˜é‡
            vector_filtered_results = []
            search_time = 0.0

            # æ ¹æ®æƒé‡å†³å®šæ˜¯å¦æ‰§è¡Œå‘é‡æœç´¢
            if vector_weight > 0:
                print(f"ğŸ”„ å¼€å§‹æ‰§è¡Œå‘é‡æœç´¢...")

                start_time = time.time()

                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                try:
                    from service.core.rag.nlp.model import generate_embedding
                    embed_start = time.time()
                    query_vector = generate_embedding(query)
                    embed_time = time.time() - embed_start
                    print(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)} (è€—æ—¶: {embed_time:.3f}s)")
                except Exception as e:
                    print(f"âŒ ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}")
                    if not enable_hybrid_search:
                        # çº¯å‘é‡æœç´¢å¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯
                        return MilvusSearchResponse(
                            query=query,
                            total=0,
                            results=[],
                            search_time=0.0,
                            search_type=search_type,
                            vector_results_count=0,
                            text_results_count=0
                        )
                    else:
                        # æ··åˆæœç´¢æ¨¡å¼ï¼Œç»§ç»­æ‰§è¡Œæ–‡æœ¬æœç´¢
                        query_vector = None

                # æ„å»ºè¿‡æ»¤æ¡ä»¶
                filter_parts = []
                if category:
                    filter_parts.append(f'category == "{category}"')

                filter_expr = " AND ".join(filter_parts) if filter_parts else None

                # æ‰§è¡Œå‘é‡æœç´¢
                from services.milvus.models import SearchResult as MilvusEntityResult

                if query_vector:
                    # ç¡®å®šå‘é‡æœç´¢è·å–è¶³å¤Ÿçš„å€™é€‰ç»“æœ
                    # æ··åˆæœç´¢æ—¶è·å–æ›´å¤šå€™é€‰è¿›è¡Œrerankï¼Œçº¯å‘é‡æœç´¢æ—¶ä½¿ç”¨top_k
                    vector_search_size = top_k if not enable_hybrid_search else min(50, top_k * 3)
                    
                    milvus_start = time.time()
                    search_results = await milvus_service.search(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        top_k=vector_search_size,
                        filter_expr=filter_expr,
                        output_fields=["content", "doc_id", "doc_name", "category", "source", "metadata", "chunk_id"],
                        search_params={
                            "metric_type": "COSINE",
                            "params": {"ef": 64}
                        }
                    )
                    milvus_time = time.time() - milvus_start
                    print(f"âœ… Milvusæœç´¢æ‰§è¡Œå®Œæˆ (è€—æ—¶: {milvus_time:.3f}s)")

                    search_time = time.time() - start_time

                    # å¯¹äºæ··åˆæœç´¢ï¼Œä¸åœ¨è¿™é‡Œè¿‡æ»¤ï¼Œè®©rerankå†³å®šæœ€ç»ˆç»“æœ
                    # å¯¹äºçº¯å‘é‡æœç´¢ï¼Œä»ç„¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                    if enable_hybrid_search:
                        vector_filtered_results = search_results  # æ··åˆæœç´¢æ—¶ä¸åº”ç”¨é˜ˆå€¼ï¼Œè®©rerankå†³å®š
                    else:
                        # çº¯å‘é‡æœç´¢æ—¶åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                        vector_filtered_results = [
                            result for result in search_results
                            if result.score >= similarity_threshold
                        ]
                        vector_filtered_results = vector_filtered_results[:top_k]
                    print(f"âœ… å‘é‡æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(vector_filtered_results)} ä¸ªç»“æœ")
                else:
                    print(f"âš ï¸  è·³è¿‡å‘é‡æœç´¢ï¼ˆæŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥ï¼‰")
            else:
                print(f"â­ï¸  å‘é‡æƒé‡ä¸º0ï¼Œè·³è¿‡å‘é‡æœç´¢")

            # å¦‚æœå¯ç”¨æ··åˆæœç´¢ï¼Œæ ¹æ®æƒé‡å†³å®šæ‰§è¡Œå“ªäº›æœç´¢å¹¶åˆå¹¶ç»“æœ
            text_search_results = []
            hybrid_results = vector_filtered_results

            print(f"ğŸ” æ··åˆæœç´¢æ¡ä»¶æ£€æŸ¥: enable_hybrid_search={enable_hybrid_search}")

            if enable_hybrid_search:
                try:
                    # æ ¹æ®æƒé‡å†³å®šæ˜¯å¦æ‰§è¡Œæ–‡æœ¬æœç´¢
                    if text_weight > 0:
                        print(f"ğŸ”„ å¼€å§‹æ‰§è¡ŒBM25æ–‡æœ¬æœç´¢...")
                        # æ–‡æœ¬æœç´¢ä¹Ÿè·å–top_kä¸ªç»“æœ
                        text_search_results = self._perform_text_search(
                            user_id, query, top_k, text_threshold
                        )
                        print(f"âœ… BM25æ–‡æœ¬æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(text_search_results)} ä¸ªç»“æœ")
                    else:
                        print(f"â­ï¸  æ–‡æœ¬æƒé‡ä¸º0ï¼Œè·³è¿‡BM25æ–‡æœ¬æœç´¢")

                    # åˆå¹¶æœç´¢ç»“æœï¼Œåº”ç”¨æ··åˆå¾—åˆ†é˜ˆå€¼è¿‡æ»¤
                    hybrid_results = self._merge_search_results(
                        vector_filtered_results,
                        text_search_results,
                        vector_weight,
                        text_weight,
                        top_k,
                        query,  # æ·»åŠ  query å‚æ•°
                        hybrid_threshold=text_threshold  # ä½¿ç”¨text_thresholdä½œä¸ºæ··åˆå¾—åˆ†é˜ˆå€¼
                    )
                    print(f"âœ… æ··åˆæœç´¢å®Œæˆï¼Œåˆå¹¶åå¾—åˆ° {len(hybrid_results)} ä¸ªç»“æœ")

                except Exception as hybrid_error:
                    print(f"âš ï¸  æ··åˆæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å‘é‡æœç´¢: {hybrid_error}")
                    hybrid_results = vector_filtered_results

            # è½¬æ¢ç»“æœæ ¼å¼
            milvus_results = []
            for result in hybrid_results:
                # å¤„ç†ä¸åŒç±»å‹çš„ç»“æœå¯¹è±¡ï¼ˆå¯èƒ½æ˜¯å¯¹è±¡æˆ–å­—å…¸ï¼‰
                if isinstance(result, dict):
                    # å­—å…¸ç±»å‹çš„å¤„ç†
                    milvus_result = MilvusSearchResult(
                        id=result.get('id', 0),
                        score=result.get('score', 0.0),
                        content=result.get('content', ''),
                        doc_id=result.get('doc_id', ''),
                        doc_name=result.get('doc_name', ''),
                        category=result.get('category', ''),
                        source=result.get('source', ''),
                        chunk_id=result.get('chunk_id', ''),
                        text_score=result.get('text_score', 0.0),
                        hybrid_score=result.get('hybrid_score', result.get('score', 0.0))
                    )
                else:
                    # å¯¹è±¡ç±»å‹çš„å¤„ç†
                    hybrid_score = getattr(result, 'hybrid_score', getattr(result, 'score', 0.0))

                    milvus_result = MilvusSearchResult(
                        id=getattr(result, 'id', 0),
                        score=hybrid_score,
                        content=getattr(result, 'content', ''),
                        doc_id=getattr(result, 'doc_id', ''),
                        doc_name=getattr(result, 'doc_name', ''),
                        category=getattr(result, 'category', ''),
                        source=getattr(result, 'source', ''),
                        chunk_id=getattr(result, 'chunk_id', ''),
                        text_score=getattr(result, 'text_score', 0.0),
                        hybrid_score=hybrid_score
                    )
                milvus_results.append(milvus_result)

            return MilvusSearchResponse(
                query=query,
                total=len(milvus_results),
                results=milvus_results,
                search_time=search_time,
                search_type=search_type,
                vector_results_count=len(vector_filtered_results),
                text_results_count=len(text_search_results)
            )

        except Exception as e:
            print(f"Milvuså‘é‡æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return MilvusSearchResponse(
                query=query,
                total=0,
                results=[],
                search_time=0.0,
                search_type="error",
                vector_results_count=0,
                text_results_count=0
            )

    def _perform_text_search(self, user_id: str, query: str, top_k: int, text_threshold: float) -> List[dict]:
        """æ‰§è¡ŒåŸºäºBM25çš„æ–‡æœ¬æœç´¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print(f"ğŸ” å¼€å§‹BM25æ–‡æœ¬æœç´¢: '{query}' (é˜ˆå€¼: {text_threshold})")
        
        import time
        text_search_start = time.time()
        
        try:
            from pymilvus import Collection, utility
            import sys
            from pathlib import Path
            
            # å¯¼å…¥BM25æœç´¢å™¨
            sys.path.append(str(Path(__file__).parent.parent))
            from utils.bm25_searcher import BM25Searcher
            
            collection_name = f"user_collection_{user_id}".replace("-", "_")
            collection = Collection(collection_name)
            
            # ä¼˜åŒ–1ï¼šæ£€æŸ¥åŠ è½½çŠ¶æ€ï¼Œé¿å…é‡å¤åŠ è½½
            try:
                load_start = time.time()
                load_state = utility.load_state(collection_name)
                if load_state.name not in ['Loaded', 'Loading']:
                    collection.load()
                load_time = time.time() - load_start
                if load_time > 1.0:
                    print(f"âš ï¸ é›†åˆåŠ è½½æ£€æŸ¥/åŠ è½½è€—æ—¶: {load_time:.3f}s")
            except:
                collection.load()
            
            # ä¼˜åŒ–2ï¼šæ™ºèƒ½å€™é€‰æ–‡æ¡£é€‰æ‹©ç­–ç•¥
            # å…ˆé€šè¿‡å‘é‡æœç´¢è·å–ç›¸å…³å€™é€‰ï¼Œå†å¯¹å€™é€‰è¿›è¡ŒBM25è¯„åˆ†
            candidate_limit = min(50, top_k * 5)  # åŠ¨æ€è°ƒæ•´ï¼Œä½†æœ€å¤š50ä¸ª
            
            # ç­–ç•¥1ï¼šå…ˆè¿›è¡Œå¿«é€Ÿå‘é‡æœç´¢è·å–ç›¸å…³å€™é€‰æ–‡æ¡£
            print(f"ğŸ” ä½¿ç”¨å‘é‡æœç´¢é¢„ç­›é€‰å€™é€‰æ–‡æ¡£...")
            
            pre_filter_start = time.time()
            try:
                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                from service.core.rag.nlp.model import generate_embedding
                
                embed_start = time.time()
                query_vector = generate_embedding(query)
                embed_time = time.time() - embed_start
                
                # ç›´æ¥ä½¿ç”¨ Collection è¿›è¡ŒåŒæ­¥å‘é‡æœç´¢
                search_params = {
                    "metric_type": "COSINE",
                    "params": {"ef": 32}  # é™ä½efå‚æ•°ï¼Œæé«˜é€Ÿåº¦
                }
                
                # æ‰§è¡Œå‘é‡æœç´¢
                vec_search_start = time.time()
                search_results = collection.search(
                    data=[query_vector],
                    anns_field="vector",
                    param=search_params,
                    limit=candidate_limit,
                    output_fields=["id", "content", "content_ltks", "doc_id", "doc_name", "category", "source", "metadata"]
                )
                vec_search_time = time.time() - vec_search_start
                
                # æå–æœç´¢ç»“æœ
                if search_results and len(search_results) > 0:
                    results = []
                    for hit in search_results[0]:
                        # ä¼˜å…ˆä½¿ç”¨chunk_idä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨Milvuså†…éƒ¨ID
                        chunk_id = hit.entity.get('chunk_id', '')
                        unique_id = chunk_id if chunk_id else str(hit.id)

                        result_dict = {
                            'id': unique_id,
                            'content': hit.entity.get('content', ''),
                            'content_ltks': hit.entity.get('content_ltks', ''),
                            'doc_id': hit.entity.get('doc_id', ''),
                            'doc_name': hit.entity.get('doc_name', ''),
                            'category': hit.entity.get('category', ''),
                            'source': hit.entity.get('source', ''),
                            'metadata': hit.entity.get('metadata', {})
                        }
                        results.append(result_dict)
                    
                    pre_filter_time = time.time() - pre_filter_start
                    print(f"âœ… å‘é‡é¢„ç­›é€‰è·å¾— {len(results)} ä¸ªå€™é€‰æ–‡æ¡£ (æ€»è€—æ—¶: {pre_filter_time:.3f}s, Embedding: {embed_time:.3f}s, æœç´¢: {vec_search_time:.3f}s)")
                else:
                    results = []
                    
            except Exception as e:
                print(f"âš ï¸ å‘é‡é¢„ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨éšæœºå€™é€‰: {e}")
                import traceback
                traceback.print_exc()
                # é™çº§æ–¹æ¡ˆï¼šéšæœºé€‰æ‹©
                results = collection.query(
                    expr="id >= 0",
                    output_fields=["id", "content", "content_ltks", "doc_id", "doc_name", "category", "source", "metadata"],
                    limit=candidate_limit
                )
            
            if not results:
                return []
            
            # ä½¿ç”¨BM25ç®—æ³•è¿›è¡Œæ–‡æœ¬æœç´¢
            print(f"ğŸ“Š ä½¿ç”¨BM25ç®—æ³•å¤„ç† {len(results)} ä¸ªå€™é€‰æ–‡æ¡£")
            
            bm25_start = time.time()
            bm25_searcher = BM25Searcher(k1=1.2, b=0.75)
            
            # æ‰§è¡ŒBM25æœç´¢ï¼Œä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œè·å–æ‰€æœ‰å€™é€‰è¿›è¡Œrerank
            scored_results = bm25_searcher.search_documents(
                query=query,
                documents=results,
                text_threshold=0.0  # ä¸è¿›è¡Œé˜ˆå€¼è¿‡æ»¤ï¼Œè®©rerankå†³å®š
            )
            bm25_time = time.time() - bm25_start
            
            # å¤„ç†BM25æœç´¢ç»“æœ
            text_results = []
            for result in scored_results[:top_k]:
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
                enhanced_result = result.copy()
                enhanced_result['chunk_id'] = result.get('metadata', {}).get('chunk_id', str(result.get('id', '')))
                
                text_results.append(enhanced_result)
            
            text_search_total_time = time.time() - text_search_start
            print(f"âœ… BM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(text_results)} ä¸ªåŒ¹é…ç»“æœ (BM25è®¡ç®—è€—æ—¶: {bm25_time:.3f}s, æ€»è€—æ—¶: {text_search_total_time:.3f}s)")
            
            return text_results

        except Exception as e:
            print(f"âŒ BM25æ–‡æœ¬æœç´¢å¤±è´¥: {e}")
            return []

    def _merge_search_results(self, vector_results: List, text_results: List,
                            vector_weight: float, text_weight: float, top_k: int,
                            query: str, hybrid_threshold: float = None) -> List:
        """åˆå¹¶å‘é‡å’Œæ–‡æœ¬æœç´¢ç»“æœ"""
        try:
            # åˆ›å»ºç»“æœå­—å…¸ç”¨äºå»é‡
            merged_dict = {}

            # å¤„ç†å‘é‡æœç´¢ç»“æœï¼Œä½¿ç”¨å†…å®¹å»é‡
            for i, result in enumerate(vector_results):
                chunk_id = getattr(result, 'id', f"vector_{i}")
                content = result.content
                
                # ä½¿ç”¨å†…å®¹ä½œä¸ºå»é‡é”®ï¼Œé¿å…ç›¸åŒå†…å®¹çš„ä¸åŒchunk_id
                content_key = content[:200] if len(content) > 200 else content  # ä½¿ç”¨å‰200å­—ç¬¦ä½œä¸ºå»é‡é”®
                
                # å¦‚æœå†…å®¹å·²å­˜åœ¨ï¼Œä¿ç•™å¾—åˆ†æ›´é«˜çš„
                if content_key in merged_dict:
                    existing_score = merged_dict[content_key]['vector_score']
                    if result.score > existing_score:
                        merged_dict[content_key] = {
                            'result': result,
                            'vector_score': result.score,
                            'text_score': 0.0,
                            'hybrid_score': result.score * vector_weight,
                            'source': 'vector'
                        }
                else:
                    merged_dict[content_key] = {
                        'result': result,
                        'vector_score': result.score,
                        'text_score': 0.0,
                        'hybrid_score': result.score * vector_weight,
                        'source': 'vector'
                    }

            # å¤„ç†æ–‡æœ¬æœç´¢ç»“æœï¼Œä½¿ç”¨å†…å®¹å»é‡
            for text_result in text_results:
                text_content = text_result.get('content', '')
                text_id = text_result.get('id')
                
                # ä½¿ç”¨å†…å®¹ä½œä¸ºå»é‡é”®
                content_key = text_content[:200] if len(text_content) > 200 else text_content
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
                if content_key in merged_dict:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°æ–‡æœ¬å¾—åˆ†ï¼ˆä¿ç•™æ›´é«˜çš„å¾—åˆ†ï¼‰
                    existing_text_score = merged_dict[content_key]['text_score']
                    new_text_score = text_result.get('text_score', 0.0)
                    
                    if new_text_score > existing_text_score:
                        merged_item = merged_dict[content_key]
                        merged_item['text_score'] = new_text_score
                        merged_item['hybrid_score'] = (
                            merged_item['vector_score'] * vector_weight +
                            merged_item['text_score'] * text_weight
                        )
                        merged_item['source'] = 'hybrid'
                else:
                    # å¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ æ–°çš„æ–‡æœ¬ç»“æœ
                    from services.milvus.models import SearchResult
                    pseudo_result = SearchResult(
                        id=text_id,
                        score=text_result.get('text_score', 0.0),
                        content=text_content,
                        doc_id=text_result.get('doc_id', ''),
                        doc_name=text_result.get('doc_name', ''),
                        category=text_result.get('category', ''),
                        source=text_result.get('source', ''),
                        metadata=text_result.get('metadata', {})
                    )

                    merged_dict[content_key] = {
                        'result': pseudo_result,
                        'vector_score': 0.0,
                        'text_score': text_result.get('text_score', 0.0),
                        'hybrid_score': text_result.get('text_score', 0.0) * text_weight,
                        'source': 'text'
                    }

            # ä½¿ç”¨é˜¿é‡Œ DashScope Rerank æ¨¡å‹è¿›è¡Œæ™ºèƒ½é‡æ’
            print(f"ğŸ”„ ä½¿ç”¨ DashScope Rerank æ¨¡å‹é‡æ’: åˆå¹¶ {len(merged_dict)} ä¸ªç»“æœ")
            
            try:
                # å‡†å¤‡é‡æ’æ•°æ®
                rerank_candidates = []
                for item in merged_dict.values():
                    rerank_candidates.append({
                        'content': item['result'].content,
                        'item': item
                    })
                
                # è°ƒç”¨é˜¿é‡Œ DashScope Rerank
                from service.core.rag.nlp.model import rerank_similarity
                texts = [candidate['content'] for candidate in rerank_candidates]
                rerank_scores, _ = rerank_similarity(query, texts)
                
                # å®Œå…¨ä½¿ç”¨ DashScope Rerank çš„ç»“æœ
                for i, candidate in enumerate(rerank_candidates):
                    rerank_score = float(rerank_scores[i])
                    
                    # ç›´æ¥ä½¿ç”¨ rerank å¾—åˆ†ä½œä¸ºæœ€ç»ˆå¾—åˆ†
                    candidate['item']['hybrid_score'] = rerank_score
                    candidate['item']['rerank_score'] = rerank_score
                
                # æŒ‰æœ€ç»ˆå¾—åˆ†æ’åº
                sorted_results = sorted(
                    [candidate['item'] for candidate in rerank_candidates],
                    key=lambda x: x['hybrid_score'],
                    reverse=True
                )
                
                print(f"âœ… DashScope Rerank å®Œæˆï¼Œå–å‰ {top_k} ä¸ªç»“æœ")
                
            except Exception as e:
                print(f"âš ï¸ DashScope Rerank å¤±è´¥ï¼Œä½¿ç”¨ç®€å•é‡æ’: {e}")
                # é™çº§åˆ°ç®€å•é‡æ’
                sorted_results = sorted(
                    merged_dict.values(),
                    key=lambda x: x['hybrid_score'],
                    reverse=True
                )
                print(f"ğŸ”„ ç®€å•é‡æ’: åˆå¹¶ {len(merged_dict)} ä¸ªç»“æœï¼Œå–å‰ {top_k} ä¸ª")

            # æ›´æ–°åŸå§‹ç»“æœçš„åˆ†æ•°å¹¶è¿”å›
            final_results = []
            for item in sorted_results[:top_k]:
                result = item['result']
                # æ›´æ–°åˆ†æ•°ä¿¡æ¯
                result.score = item['hybrid_score']
                # å°†é¢å¤–ä¿¡æ¯å­˜å‚¨åˆ°metadataä¸­
                if not hasattr(result, 'metadata') or not result.metadata:
                    result.metadata = {}
                result.metadata.update({
                    'vector_score': item['vector_score'],
                    'text_score': item['text_score'],
                    'hybrid_score': item['hybrid_score'],
                    'source_type': item['source']
                })
                
                # å¦‚æœä½¿ç”¨äº† DashScope Rerankï¼Œæ·»åŠ  rerank å¾—åˆ†
                if 'rerank_score' in item:
                    result.metadata['rerank_score'] = item['rerank_score']
                final_results.append(result)

            return final_results

        except Exception as e:
            print(f"åˆå¹¶æœç´¢ç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œè¿”å›å‘é‡æœç´¢ç»“æœ
            return vector_results

    def get_user_documents(self, user_id: str, page: int = 1, page_size: int = 10) -> dict:
        try:
            db = default_manager.session_factory()
            try:
                # è®¡ç®—åç§»é‡
                offset = (page - 1) * page_size
                
                # è·å–æ€»æ•°
                total = db.query(Document).filter(Document.user_id == user_id).count()
                
                # è·å–åˆ†é¡µæ•°æ®
                documents = db.query(Document).filter(
                    Document.user_id == user_id
                ).order_by(
                    Document.created_at.desc()
                ).offset(offset).limit(page_size).all()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                docs_list = [doc.to_dict() for doc in documents]
                
                return {
                    "total": total,
                    "page": page,
                    "page_size": page_size,
                    "documents": docs_list
                }
                
            finally:
                db.close()
                
        except Exception as e:
            print(f"è·å–ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"è·å–ç”¨æˆ·æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"
            )
    
    def get_document_by_id(self, document_id: str, user_id: str) -> Document:
        """æ ¹æ®IDè·å–æ–‡æ¡£ï¼ˆç¡®ä¿ç”¨æˆ·æƒé™ï¼‰"""
        try:
            db = default_manager.session_factory()
            try:
                document = db.query(Document).filter(
                    Document.document_id == document_id,
                    Document.user_id == user_id
                ).first()
                
                if not document:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail="æ–‡æ¡£ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®"
                    )
                
                return document
                
            finally:
                db.close()
                
        except Exception as e:
            print(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"è·å–æ–‡æ¡£å¤±è´¥: {str(e)}"
            )
    
    def delete_document(self, document_id: str, user_id: str) -> dict:
        """
        åˆ é™¤æ–‡æ¡£ï¼ˆåˆ é™¤PGã€Milvuså’Œç‰©ç†æ–‡ä»¶ï¼‰
        é‡‡ç”¨æœ€å¤§åŠªåŠ›åˆ é™¤ç­–ç•¥ï¼Œè®°å½•è¿‡ç¨‹ä¸­çš„é—®é¢˜ä½†ä¸é˜»æ­¢åˆ é™¤
        """
        deletion_results = {
            'document_deleted': False,
            'milvus_deleted': False,
            'es_deleted': False,
            'file_deleted': False,
            'errors': []
        }
        
        try:
            db = default_manager.session_factory()
            try:
                # è·å–æ–‡æ¡£ä¿¡æ¯
                document = db.query(Document).filter(
                    Document.document_id == document_id,
                    Document.user_id == user_id
                ).first()
                
                if not document:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail="æ–‡æ¡£ä¸å­˜åœ¨æˆ–æ— æƒé™åˆ é™¤"
                    )
  
                # 1. å°è¯•åˆ é™¤Milvusä¸­çš„æ•°æ®
                try:
                    from services.milvus.milvus_service import MilvusService
                    import os

                    collection_name = f"user_collection_{user_id}".replace("-", "_")
                    print(f"å°è¯•åˆ é™¤Milvusé›†åˆ {collection_name} ä¸­æ–‡æ¡£ {document.file_name} çš„æ•°æ®")

                    # åˆå§‹åŒ–MilvusæœåŠ¡
                    milvus_service = MilvusService(
                        host=os.getenv("MILVUS_HOST", "milvus-standalone"),
                        port=os.getenv("MILVUS_PORT", "19530")
                    )

                    # è¿æ¥Milvus
                    connected = milvus_service.connect_sync()
                    if connected:
                        # åˆ é™¤æŒ‡å®šæ–‡æ¡£åçš„æ‰€æœ‰å‘é‡æ•°æ®
                        delete_expr = f'doc_name == "{document.file_name}"'
                        deleted_count = milvus_service.delete_data_sync(collection_name, delete_expr)

                        if deleted_count > 0:
                            deletion_results['milvus_deleted'] = True
                            print(f"âœ… æˆåŠŸä»Milvusåˆ é™¤ {deleted_count} æ¡å‘é‡æ•°æ®")
                        else:
                            deletion_results['milvus_deleted'] = True  # æ²¡æœ‰æ•°æ®ä¹Ÿç®—æˆåŠŸ
                            print(f"âœ… Milvusä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å‘é‡æ•°æ®")
                    else:
                        deletion_results['errors'].append("Milvusè¿æ¥å¤±è´¥ï¼Œæ— æ³•åˆ é™¤å‘é‡æ•°æ®")
                        print("âŒ Milvusè¿æ¥å¤±è´¥")

                except Exception as milvus_error:
                    deletion_results['errors'].append(f"Milvusåˆ é™¤å¤±è´¥: {str(milvus_error)}")
                    print(f"åˆ é™¤Milvusæ•°æ®å¤±è´¥: {milvus_error}")

                # 2. å°è¯•åˆ é™¤ESä¸­çš„æ•°æ®ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ é™¤ç­–ç•¥ï¼‰
                try:
                    from service.core.file_parse import get_es_connection
                    es_connection = get_es_connection()
                    
                    if es_connection.es_available:
                        # ä½¿ç”¨æ–°çš„æ™ºèƒ½åˆ é™¤æ–¹æ³•
                        es_result = es_connection.delete_user_document_data(
                            file_name=document.file_name,
                            index_name=user_id
                        )
                        
                        # åˆ†æåˆ é™¤ç»“æœ
                        if es_result['strategy_used'] in ['index_not_exists', 'index_empty']:
                            # ç´¢å¼•ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè¿™å®é™…ä¸Šæ˜¯æ­£å¸¸çš„
                            deletion_results['es_deleted'] = True
                            deletion_results['errors'].append(f"ESçŠ¶æ€: {es_result['strategy_used']}")
                        elif es_result['deleted_count'] > 0:
                            # æˆåŠŸåˆ é™¤äº†æ–‡æ¡£
                            deletion_results['es_deleted'] = True
                        elif es_result['errors']:
                            # æœ‰é”™è¯¯å‘ç”Ÿ
                            deletion_results['es_deleted'] = False
                            deletion_results['errors'].extend(es_result['errors'])
                        else:
                            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£ï¼Œä½†è¿™ä¸ç®—é”™è¯¯
                            deletion_results['es_deleted'] = True
                            deletion_results['errors'].append("ESä¸­æœªæ‰¾åˆ°åŒ¹é…æ–‡æ¡£")
                            
                        print(f"ESåˆ é™¤ç»“æœ: {es_result}")
                    else:
                        deletion_results['errors'].append("ESè¿æ¥ä¸å¯ç”¨")
                        deletion_results['es_deleted'] = False
                        
                except Exception as es_error:
                    deletion_results['errors'].append(f"ESåˆ é™¤å¤±è´¥: {str(es_error)}")
                    deletion_results['es_deleted'] = False
                    print(f"åˆ é™¤ESæ•°æ®å¤±è´¥: {es_error}")
                
                # 2. å°è¯•åˆ é™¤ç‰©ç†æ–‡ä»¶
                try:
                    if os.path.exists(document.file_path):
                        os.remove(document.file_path)
                        deletion_results['file_deleted'] = True
                    else:
                        deletion_results['errors'].append("ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨")
                except Exception as file_error:
                    deletion_results['errors'].append(f"ç‰©ç†æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(file_error)}")
                    print(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {file_error}")
                
                # 3. åˆ é™¤PGæ•°æ®åº“è®°å½•ï¼ˆè¿™æ˜¯æœ€é‡è¦çš„ï¼‰
                db.delete(document)
                db.commit()
                deletion_results['document_deleted'] = True
                
                print(f"æ–‡æ¡£åˆ é™¤å®Œæˆ: {document_id}, ç»“æœ: {deletion_results}")
                return deletion_results
                
            finally:
                db.close()
                
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}"
            )
    
    def process_document_to_es(self, file_path: str, file_name: str, user_id: str) -> int:
        """å¤„ç†æ–‡æ¡£åˆ°ESï¼ˆä¸æ¶‰åŠPGæ•°æ®åº“ï¼‰"""
        try:
            # ä½¿ç”¨file_parse.pyå¤„ç†æ–‡æ¡£
            result = execute_insert_process(
                file_path=file_path,
                file_name=file_name,
                session_id=user_id,  # ä½¿ç”¨ç”¨æˆ·IDä½œä¸ºESç´¢å¼•å
                index_name=user_id   # ç¡®ä¿ESç´¢å¼•åæ˜¯ç”¨æˆ·ID
            )
            
            # è¿”å›å¤„ç†ç»“æœ
            return len(result) if result else 0
            
        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£åˆ°ESå¤±è´¥: {e}")
            raise e
    
    def process_document(self, document_id: str, user_id: str) -> bool:
        """å¤„ç†æ–‡æ¡£ï¼ˆè§£æå¹¶å­˜å…¥ESï¼‰"""
        try:
            db = default_manager.session_factory()
            try:
                # è·å–æ–‡æ¡£ä¿¡æ¯
                document = db.query(Document).filter(
                    Document.document_id == document_id,
                    Document.user_id == user_id
                ).first()
                
                if not document:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail="æ–‡æ¡£ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®"
                    )
                
                # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
                document.status = 'processing'
                document.updated_at = int(time.time())
                db.commit()
                
                try:
                    # ä½¿ç”¨file_parse.pyå¤„ç†æ–‡æ¡£
                    result = execute_insert_process(
                        file_path=document.file_path,
                        file_name=document.file_name,
                        session_id=user_id,  # ä½¿ç”¨ç”¨æˆ·IDä½œä¸ºESç´¢å¼•å
                        index_name=user_id   # ç¡®ä¿ESç´¢å¼•åæ˜¯ç”¨æˆ·ID
                    )
                    
                    # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                    document.status = 'completed'
                    document.chunk_count = len(result) if result else 0
                    document.updated_at = int(time.time())
                    db.commit()
                    
                    return len(result) if result else 0
                    
                except Exception as process_error:
                    # å¤„ç†å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€
                    document.status = 'failed'
                    document.error_message = str(process_error)
                    document.updated_at = int(time.time())
                    db.commit()
                    
                    raise process_error
                
            finally:
                db.close()

        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}"
            )

  