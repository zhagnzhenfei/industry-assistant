"""
ä¼˜åŒ–ç‰ˆå¢å¼ºç ”ç©¶æŠ¥å‘ŠAPIè·¯ç”±

ä½¿ç”¨è®°å¿†è£…é¥°å™¨æ¡†æ¶ï¼Œå¤§å¹…ç®€åŒ–è®°å¿†å¤„ç†é€»è¾‘
"""
import logging
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime
import asyncio
import json

from services.research_service import execute_research_task_stream
from services.agent_orchestration.odr_orchestrator import ResearchResult
from service.auth_service import get_current_user
from models.user_models import User

# è®°å¿†è£…é¥°å™¨æ¡†æ¶ï¼ˆæ–°å¢ï¼‰
from services.memory.decorators import research_memory

logger = logging.getLogger(__name__)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/enhanced-research", tags=["enhanced-research"])


class EnhancedResearchRequest(BaseModel):
    """å¢å¼ºç‰ˆç ”ç©¶æŠ¥å‘Šè¯·æ±‚æ¨¡å‹"""
    question: str = Field(..., description="ç ”ç©¶é—®é¢˜", min_length=5, max_length=500)
    user_context: Optional[Dict[str, Any]] = Field(default=None, description="ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯")
    allow_clarification: bool = Field(default=False, description="æ˜¯å¦å…è®¸è¯·æ±‚æ¾„æ¸…")
    research_depth: str = Field(default="comprehensive", description="ç ”ç©¶æ·±åº¦: basic/standard/comprehensive")
    memory_mode: str = Field(default="smart", description="è®°å¿†æ¨¡å¼: none/short_term/long_term/smart")

    class Config:
        schema_extra = {
            "example": {
                "question": "Pythoné«˜çº§ç¼–ç¨‹æŠ€å·§å’Œè®¾è®¡æ¨¡å¼çš„åº”ç”¨",
                "allow_clarification": False,
                "research_depth": "standard",
                "memory_mode": "smart"
            }
        }


class ResearchReportResponse(BaseModel):
    """ç ”ç©¶æŠ¥å‘Šå“åº”æ¨¡å‹"""
    research_id: str
    question: str
    status: str
    final_report: str
    key_findings: list
    metadata: Dict[str, Any]
    quality_score: float
    duration: float
    created_at: str


@router.post("/generate")
@research_memory(
    memory_mode_param="memory_mode",
    user_context_param="enhanced_context",
    auto_save=True
)
async def generate_enhanced_research_report(
    request: EnhancedResearchRequest,
    current_user: User = Depends(get_current_user),
    enhanced_context: Optional[Dict[str, Any]] = None
):
    """
    ç”Ÿæˆä¼˜åŒ–ç‰ˆå¢å¼ºç ”ç©¶æŠ¥å‘Š - ä½¿ç”¨è®°å¿†è£…é¥°å™¨æ¡†æ¶

    ç‰¹æ€§ï¼š
    - ğŸ§  è‡ªåŠ¨è®°å¿†åŠŸèƒ½ï¼šæ™ºèƒ½åŠ è½½å†å²ç ”ç©¶è®°å¿†
    - ğŸš€ å³æ’å³ç”¨ï¼šè£…é¥°å™¨è‡ªåŠ¨å¤„ç†æ‰€æœ‰è®°å¿†é€»è¾‘
    - ğŸ“Š ä¸Šä¸‹æ–‡å¢å¼ºï¼šåŸºäºå†å²è®°å¿†æä¾›æ›´ç›¸å…³çš„ç ”ç©¶ç»“æœ
    - ğŸ’¾ è‡ªåŠ¨ä¿å­˜ï¼šç ”ç©¶å®Œæˆåè‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†ç³»ç»Ÿ
    - ğŸ”„ å‘åå…¼å®¹ï¼šä¸åŸæ¥å£æ ¼å¼å®Œå…¨ä¸€è‡´

    Args:
        request: ç ”ç©¶è¯·æ±‚ï¼ŒåŒ…å«é—®é¢˜ã€é…ç½®å’Œè®°å¿†æ¨¡å¼
        current_user: å½“å‰è®¤è¯ç”¨æˆ·
        enhanced_context: ç”±è®°å¿†è£…é¥°å™¨æ³¨å…¥çš„å¢å¼ºä¸Šä¸‹æ–‡

    Returns:
        æµå¼å“åº”ï¼ŒåŒ…å«ç ”ç©¶è¿‡ç¨‹å’Œæœ€ç»ˆç»“æœ
    """
    research_id = f"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(request.question) % 10000}"
    user_id = current_user.user_id

    logger.info(f"ğŸš€ [OPTIMIZED_RESEARCH] å¼€å§‹å¤„ç†ç ”ç©¶è¯·æ±‚ {research_id}")
    logger.info(f"ğŸ“ [OPTIMIZED_RESEARCH] é—®é¢˜: {request.question}")
    logger.info(f"âš™ï¸ [OPTIMIZED_RESEARCH] æ·±åº¦: {request.research_depth}, æ¾„æ¸…: {request.allow_clarification}")

    # è®°å¿†ä¿¡æ¯æ—¥å¿—
    if enhanced_context:
        memory_count = enhanced_context.get("memory_count", 0)
        logger.info(f"ğŸ§  [OPTIMIZED_RESEARCH] è®°å¿†çŠ¶æ€: å·²å¯ç”¨ï¼Œæ‰¾åˆ° {memory_count} æ¡ç›¸å…³è®°å¿†")
        if memory_count > 0:
            logger.info(f"ğŸ“‹ [OPTIMIZED_RESEARCH] è®°å¿†é¢„è§ˆ: {enhanced_context.get('memory_context', '')[:100]}...")
    else:
        logger.info(f"ğŸ§  [OPTIMIZED_RESEARCH] è®°å¿†çŠ¶æ€: æœªå¯ç”¨æˆ–æ— ç›¸å…³è®°å¿†")

    # å¢å¼ºç”¨æˆ·ä¸Šä¸‹æ–‡
    final_context = request.user_context or {}

    # åˆå¹¶è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if enhanced_context and enhanced_context.get("has_memories"):
        final_context.update({
            "memory_enabled": True,
            "memory_count": enhanced_context.get("memory_count", 0),
            "memory_context": enhanced_context.get("memory_context", ""),
            "historical_memories": enhanced_context.get("memories", [])
        })

        # ä¸ºç ”ç©¶ä»»åŠ¡æ·»åŠ è®°å¿†æç¤º
        memory_prompt = f"""
=== å†å²ç ”ç©¶è®°å¿† ===
ç”¨æˆ·ä¹‹å‰æœ‰ç›¸å…³çš„ç ”ç©¶èƒŒæ™¯ï¼š
{enhanced_context.get('memory_context', 'æ— å†å²è®°å¿†')}

=== ç ”ç©¶å»ºè®® ===
åŸºäºä»¥ä¸Šå†å²è®°å¿†ï¼Œè¯·ï¼š
1. é¿å…é‡å¤å·²çŸ¥çš„ä¿¡æ¯
2. é‡ç‚¹å…³æ³¨æ–°çš„å‘ç°å’Œè¿›å±•
3. åœ¨é€‚å½“çš„æ—¶å€™å¼•ç”¨å†å²ç ”ç©¶æˆæœ
4. æä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„æ·±åº¦åˆ†æ
"""

        final_context["memory_prompt"] = memory_prompt
        logger.info(f"ğŸ¯ [OPTIMIZED_RESEARCH] å·²æ·»åŠ è®°å¿†æç¤ºï¼Œé•¿åº¦: {len(memory_prompt)} å­—ç¬¦")

    async def generate_stream():
        """ç”Ÿæˆæµå¼å“åº”"""
        try:
            final_result = None

            # å‘é€åˆå§‹ä¿¡æ¯
            initial_data = {
                'type': 'start',
                'research_id': research_id,
                'question': request.question,
                'message': 'ğŸš€ å¼€å§‹å¤„ç†ä¼˜åŒ–ç‰ˆç ”ç©¶è¯·æ±‚ï¼ˆæ”¯æŒè®°å¿†åŠŸèƒ½ï¼‰',
                'memory_info': {
                    'enabled': enhanced_context is not None,
                    'memory_count': enhanced_context.get('memory_count', 0) if enhanced_context else 0
                } if enhanced_context else None,
                'timestamp': datetime.now().isoformat()
            }
            logger.info(f"ğŸ“¤ [STREAM_START] {research_id}: {initial_data['message']}")
            yield f"data: {json.dumps(initial_data, ensure_ascii=False)}\n\n"

            # æ‰§è¡Œç ”ç©¶ä»»åŠ¡ï¼ˆä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼‰
            async for progress_data in execute_research_task_stream(
                research_id=research_id,
                question=request.question,
                user_context=final_context,
                allow_clarification=request.allow_clarification,
                research_depth=request.research_depth,
                memory_mode=request.memory_mode,
                memory_service=None  # è£…é¥°å™¨å·²å¤„ç†è®°å¿†æœåŠ¡
            ):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç»“æœ
                if progress_data.get('type') == 'result':
                    final_result = progress_data.get('result') or progress_data.get('final_result')
                    if final_result and not isinstance(final_result, ResearchResult):
                        final_result = ResearchResult(
                            question=final_result.get('question', request.question),
                            final_report=final_result.get('final_report', ''),
                            status=final_result.get('status', 'completed'),
                            key_findings=final_result.get('key_findings', []),
                            raw_notes=final_result.get('raw_notes', []),
                            metadata=final_result.get('metadata', {}),
                            progress=final_result.get('progress', 100.0)
                        )

                # æ·»åŠ è®°å¿†ä¿¡æ¯åˆ°è¿›åº¦æ•°æ®
                if enhanced_context and enhanced_context.get("has_memories"):
                    progress_data['memory_enhanced'] = True
                    progress_data['memory_count'] = enhanced_context.get('memory_count', 0)

                # å‘é€è¿›åº¦æ•°æ®
                progress_type = progress_data.get('type')
                if progress_type == 'progress':
                    logger.info(f"ğŸ“Š [STREAM_PROGRESS] {research_id}: {progress_data.get('message', '')} ({progress_data.get('progress', 0):.1f}%)")
                elif progress_type == 'result':
                    quality_score = progress_data.get('quality_score', 0)
                    logger.info(f"ğŸ“‹ [STREAM_RESULT] {research_id}: ç ”ç©¶å®Œæˆï¼Œè´¨é‡è¯„åˆ† {quality_score:.1f}åˆ†")

                yield f"data: {json.dumps(progress_data, ensure_ascii=False)}\n\n"

            # å‘é€å®Œæˆä¿¡æ¯
            complete_data = {
                'type': 'complete',
                'research_id': research_id,
                'message': 'âœ… ä¼˜åŒ–ç‰ˆç ”ç©¶ä»»åŠ¡å®Œæˆï¼ˆè®°å¿†åŠŸèƒ½å·²è‡ªåŠ¨ä¿å­˜ï¼‰',
                'memory_saved': enhanced_context is not None if enhanced_context else False,
                'timestamp': datetime.now().isoformat()
            }
            logger.info(f"âœ… [STREAM_COMPLETE] {research_id}: æµå¼å“åº”å®Œæˆ")
            yield f"data: {json.dumps(complete_data, ensure_ascii=False)}\n\n"

        except Exception as e:
            error_message = f'ä¼˜åŒ–ç‰ˆç ”ç©¶è¯·æ±‚å¤±è´¥: {str(e)}'
            logger.error(f"ğŸ’¥ [STREAM_ERROR] {research_id}: {error_message}")

            error_data = {
                'type': 'error',
                'research_id': research_id,
                'message': error_message,
                'timestamp': datetime.now().isoformat()
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Research-Version": "v2-optimized-with-memory"
        }
    )


@router.get("/memory-status")
async def get_memory_status(current_user: User = Depends(get_current_user)):
    """
    è·å–ç”¨æˆ·è®°å¿†çŠ¶æ€

    Args:
        current_user: å½“å‰è®¤è¯ç”¨æˆ·

    Returns:
        è®°å¿†çŠ¶æ€ä¿¡æ¯
    """
    try:
        from services.memory.manager import MemoryManager

        memory_manager = MemoryManager()
        await memory_manager.initialize()

        stats = await memory_manager.get_memory_stats(current_user.user_id)

        return {
            "status": "success",
            "memory_enabled": stats.get("enabled", False),
            "total_memories": stats.get("total_memories", 0),
            "memories_by_type": stats.get("by_type", {}),
            "available_strategies": stats.get("strategies_available", [])
        }

    except Exception as e:
        logger.error(f"è·å–è®°å¿†çŠ¶æ€å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": str(e),
            "memory_enabled": False
        }


@router.post("/test-memory")
@research_memory(memory_mode_param="memory_mode")
async def test_memory_functionality(
    request: EnhancedResearchRequest,
    current_user: User = Depends(get_current_user),
    enhanced_context: Optional[Dict[str, Any]] = None
):
    """
    æµ‹è¯•è®°å¿†åŠŸèƒ½

    Args:
        request: æµ‹è¯•è¯·æ±‚
        current_user: å½“å‰ç”¨æˆ·
        enhanced_context: å¢å¼ºä¸Šä¸‹æ–‡

    Returns:
        æµ‹è¯•ç»“æœ
    """
    return {
        "status": "success",
        "message": "è®°å¿†åŠŸèƒ½æµ‹è¯•æˆåŠŸ",
        "memory_enabled": enhanced_context is not None,
        "memory_count": enhanced_context.get("memory_count", 0) if enhanced_context else 0,
        "question": request.question,
        "memory_mode": request.memory_mode,
        "user_id": current_user.user_id
    }