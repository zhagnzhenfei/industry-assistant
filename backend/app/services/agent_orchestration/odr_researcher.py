"""
Open Deep Research ç ”ç©¶è€…å­å›¾
åŸºäºå®˜æ–¹æ–‡æ¡£çš„å®Œæ•´ç ”ç©¶è€…å®ç°
"""
import asyncio
from typing import Literal
from .qwen_model import init_qwen_model
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    filter_messages,
)
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.types import Command

from .odr_configuration import Configuration
from .odr_prompts import (
    compress_research_simple_human_message,
    compress_research_system_prompt,
    research_system_prompt,
)
from .odr_state import (
    ResearcherOutputState,
    ResearcherState,
)
from .odr_utils import (
    anthropic_websearch_called,
    get_all_tools,
    get_tools_for_researcher,
    get_api_key_for_model,
    get_today_str,
    is_token_limit_exceeded,
    openai_websearch_called,
    remove_up_to_last_ai_message,
)

# åˆå§‹åŒ–å¯é…ç½®æ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªæ™ºèƒ½ä½“ä¸­ä½¿ç”¨
# æ¨¡å‹åç§°ä»ç¯å¢ƒå˜é‡è¯»å–
import logging
logger = logging.getLogger(__name__)

configurable_model = init_qwen_model(
    model=None,  # ä»ç¯å¢ƒå˜é‡LLM_MODELè¯»å–
    max_tokens=4000
)

logger.info(f"ğŸ¤– ç ”ç©¶è€…æ¨¡å‹åˆå§‹åŒ–: model={configurable_model.model_name}")


async def researcher(state: ResearcherState, config: RunnableConfig) -> Command[Literal["researcher_tools"]]:
    """ä¸ªä½“ç ”ç©¶è€…ï¼Œå¯¹ç‰¹å®šä¸»é¢˜è¿›è¡Œä¸“æ³¨ç ”ç©¶ã€‚
    
    æ­¤ç ”ç©¶è€…è¢«ç›‘ç£è€…ç»™äºˆç‰¹å®šç ”ç©¶ä¸»é¢˜ï¼Œå¹¶ä½¿ç”¨å¯ç”¨å·¥å…·ï¼ˆæœç´¢ã€MCPå·¥å…·ï¼‰
    æ”¶é›†å…¨é¢ä¿¡æ¯ã€‚
    
    Args:
        state: å½“å‰ç ”ç©¶è€…çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å’Œä¸»é¢˜ä¸Šä¸‹æ–‡
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«æ¨¡å‹è®¾ç½®å’Œå·¥å…·å¯ç”¨æ€§
        
    Returns:
        å‘½ä»¤ï¼ŒæŒ‡ç¤ºç»§ç»­åˆ°researcher_toolsè¿›è¡Œå·¥å…·æ‰§è¡Œ
    """
    # æ­¥éª¤1ï¼šåŠ è½½é…ç½®å¹¶éªŒè¯å·¥å…·å¯ç”¨æ€§
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    
    # æ·»åŠ æ—¥å¿—ç›‘æ§
    import logging
    logger = logging.getLogger(__name__)
    current_iteration = state.get("tool_call_iterations", 0) + 1
    current_searches = state.get("total_searches", 0)
    logger.info(
        f"[RESEARCHER] ğŸ“Š ç¬¬{current_iteration}è½® | "
        f"å·²æœç´¢{current_searches}æ¬¡ | "
        f"é™åˆ¶:{configurable.max_react_tool_calls}è½®/{configurable.max_total_searches_per_researcher}æ¬¡æœç´¢"
    )
    
    # è·å–ç ”ç©¶è€…ä¸“ç”¨çš„å·¥å…·é›†åˆï¼ˆæœç´¢ã€MCPå·¥å…·ï¼‰
    tools = await get_tools_for_researcher(config)
    
    # æ·»åŠ è¯¦ç»†çš„å·¥å…·åˆ—è¡¨æ—¥å¿—
    logger.info(f"[RESEARCHER] ğŸ”§ å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")
    for i, tool in enumerate(tools, 1):
        tool_name = tool.name if hasattr(tool, 'name') else str(tool)
        tool_desc = tool.description[:50] if hasattr(tool, 'description') else "æ— æè¿°"
        logger.info(f"[RESEARCHER] ğŸ”§ å·¥å…·{i}: {tool_name} - {tool_desc}...")
    
    if len(tools) == 0:
        raise ValueError(
            "æœªæ‰¾åˆ°è¿›è¡Œç ”ç©¶æ‰€éœ€çš„å·¥å…·ï¼šè¯·åœ¨é…ç½®ä¸­é…ç½®æœç´¢APIæˆ–æ·»åŠ MCPå·¥å…·ã€‚"
        )
    
    # æ­¥éª¤2ï¼šé…ç½®ç ”ç©¶è€…æ¨¡å‹å’Œå·¥å…·
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
        "tags": ["langsmith:nostream"]
    }
    
    # å‡†å¤‡ç³»ç»Ÿæç¤ºï¼Œå¦‚æœå¯ç”¨åˆ™åŒ…å«MCPä¸Šä¸‹æ–‡
    researcher_prompt = research_system_prompt.format(
        mcp_prompt=configurable.mcp_prompt or "", 
        date=get_today_str()
    )
    
    # é…ç½®æ¨¡å‹ï¼Œç»‘å®šå·¥å…·ï¼Œé‡è¯•é€»è¾‘å’Œè®¾ç½®
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # æ­¥éª¤3ï¼šä½¿ç”¨ç³»ç»Ÿä¸Šä¸‹æ–‡ç”Ÿæˆç ”ç©¶è€…å“åº”
    messages = [SystemMessage(content=researcher_prompt)] + researcher_messages
    response = await research_model.ainvoke(messages)
    
    # æ­¥éª¤4ï¼šä»ChatResultä¸­æå–AIMessage
    ai_message = response.generations[0].message
    
    # æ­¥éª¤5ï¼šæ›´æ–°çŠ¶æ€å¹¶ç»§ç»­åˆ°å·¥å…·æ‰§è¡Œ
    return Command(
        goto="researcher_tools",
        update={
            "researcher_messages": [ai_message],
            "tool_call_iterations": state.get("tool_call_iterations", 0) + 1
        }
    )


# å·¥å…·æ‰§è¡Œè¾…åŠ©å‡½æ•°
async def execute_tool_safely(tool, args, config):
    """å®‰å…¨æ‰§è¡Œå·¥å…·ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œè¶…æ—¶è®¾ç½®"""
    try:
        # æ·»åŠ 30ç§’è¶…æ—¶
        return await asyncio.wait_for(tool.ainvoke(args, config), timeout=30.0)
    except asyncio.TimeoutError:
        return f"å·¥å…·æ‰§è¡Œè¶…æ—¶: {tool.name if hasattr(tool, 'name') else 'unknown'}"
    except Exception as e:
        return f"æ‰§è¡Œå·¥å…·é”™è¯¯: {str(e)}"


async def researcher_tools(state: ResearcherState, config: RunnableConfig) -> Command[Literal["researcher", "compress_research"]]:
    """æ‰§è¡Œç ”ç©¶è€…è°ƒç”¨çš„å·¥å…·ï¼ŒåŒ…æ‹¬æœç´¢å·¥å…·ã€‚
    
    æ­¤å‡½æ•°å¤„ç†å„ç§ç±»å‹çš„ç ”ç©¶è€…å·¥å…·è°ƒç”¨ï¼š
    1. æœç´¢å·¥å…·ï¼ˆserper_searchã€tavily_searchã€web_searchï¼‰- ä¿¡æ¯æ”¶é›†
    2. MCPå·¥å…· - å¤–éƒ¨å·¥å…·é›†æˆ
    3. ResearchComplete - è¡¨ç¤ºä¸ªä½“ç ”ç©¶ä»»åŠ¡å®Œæˆ
    
    Args:
        state: å½“å‰ç ”ç©¶è€…çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å’Œè¿­ä»£è®¡æ•°
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«ç ”ç©¶é™åˆ¶å’Œå·¥å…·è®¾ç½®
        
    Returns:
        å‘½ä»¤ï¼ŒæŒ‡ç¤ºç»§ç»­ç ”ç©¶å¾ªç¯æˆ–ç»§ç»­åˆ°å‹ç¼©
    """
    # æ­¥éª¤1ï¼šæå–å½“å‰çŠ¶æ€å¹¶æ£€æŸ¥æ—©æœŸé€€å‡ºæ¡ä»¶
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    most_recent_message = researcher_messages[-1]
    
    # æ—©æœŸé€€å‡ºå¦‚æœæ²¡æœ‰è¿›è¡Œå·¥å…·è°ƒç”¨ï¼ˆåŒ…æ‹¬åŸç”Ÿç½‘ç»œæœç´¢ï¼‰
    has_tool_calls = bool(most_recent_message.tool_calls)
    has_native_search = (
        openai_websearch_called(most_recent_message) or 
        anthropic_websearch_called(most_recent_message)
    )
    
    if not has_tool_calls and not has_native_search:
        return Command(goto="compress_research")
    
    # æ­¥éª¤2ï¼šå¤„ç†å…¶ä»–å·¥å…·è°ƒç”¨ï¼ˆæœç´¢ã€MCPå·¥å…·ç­‰ï¼‰
    tools = await get_tools_for_researcher(config)
    tools_by_name = {
        tool.name if hasattr(tool, "name") else tool.get("name", "web_search"): tool 
        for tool in tools
    }
    
    # è·å–å·¥å…·è°ƒç”¨å¹¶é™åˆ¶å¹¶è¡Œæœç´¢æ•°é‡
    tool_calls = most_recent_message.tool_calls
    
    # ç»Ÿè®¡æœç´¢å·¥å…·è°ƒç”¨
    search_tool_calls = [
        tc for tc in tool_calls 
        if any(keyword in tc["name"].lower() for keyword in ["search", "tavily", "serper"])
    ]
    other_tool_calls = [tc for tc in tool_calls if tc not in search_tool_calls]
    
    # é™åˆ¶æ¯è½®å¹¶è¡Œæœç´¢æ•°é‡
    max_searches_per_iter = configurable.max_searches_per_iteration
    if len(search_tool_calls) > max_searches_per_iter:
        # åªä¿ç•™å‰Nä¸ªæœç´¢å·¥å…·è°ƒç”¨
        search_tool_calls = search_tool_calls[:max_searches_per_iter]
        # æ·»åŠ è­¦å‘Šæ¶ˆæ¯
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(
            f"[RESEARCHER] âš ï¸ é™åˆ¶å¹¶è¡Œæœç´¢ï¼šåŸè®¡åˆ’{len(tool_calls)}ä¸ªæœç´¢ï¼Œ"
            f"é™åˆ¶ä¸º{max_searches_per_iter}ä¸ª"
        )
    
    # åˆå¹¶å›æ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_calls = search_tool_calls + other_tool_calls
    
    # æ£€æŸ¥æ€»æœç´¢æ¬¡æ•°é™åˆ¶
    total_searches = state.get("total_searches", 0) + len(search_tool_calls)
    max_total = configurable.max_total_searches_per_researcher
    
    if total_searches > max_total:
        # è¶…è¿‡æ€»æœç´¢é™åˆ¶ï¼Œç«‹å³ç»“æŸç ”ç©¶
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(
            f"[RESEARCHER] â›” è¾¾åˆ°æœç´¢æ¬¡æ•°é™åˆ¶ï¼š{total_searches}/{max_total}ï¼Œç»“æŸç ”ç©¶"
        )
        # åˆ›å»ºæˆªæ–­æ¶ˆæ¯
        truncated_msg = ToolMessage(
            content=f"å·²è¾¾åˆ°æœ€å¤§æœç´¢æ¬¡æ•°é™åˆ¶({max_total})ï¼Œç ”ç©¶ç»“æŸã€‚",
            name="system",
            tool_call_id="truncate_id"
        )
        return Command(
            goto="compress_research",
            update={
                "researcher_messages": [truncated_msg],
                "total_searches": total_searches
            }
        )
    
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_execution_tasks = [
        execute_tool_safely(tools_by_name[tool_call["name"]], tool_call["args"], config) 
        for tool_call in tool_calls
    ]
    # æ·»åŠ 60ç§’æ€»è¶…æ—¶ï¼Œé˜²æ­¢æ•´ä¸ªå·¥å…·æ‰§è¡Œè¿‡ç¨‹å¡ä½
    try:
        observations = await asyncio.wait_for(
            asyncio.gather(*tool_execution_tasks, return_exceptions=True), 
            timeout=60.0
        )
    except asyncio.TimeoutError:
        observations = ["å·¥å…·æ‰§è¡Œæ€»è¶…æ—¶"] * len(tool_calls)
    
    # ä»æ‰§è¡Œç»“æœåˆ›å»ºå·¥å…·æ¶ˆæ¯
    tool_outputs = [
        ToolMessage(
            content=observation,
            name=tool_call["name"],
            tool_call_id=tool_call["id"]
        ) 
        for observation, tool_call in zip(observations, tool_calls)
    ]
    
    # æ­¥éª¤3ï¼šæ£€æŸ¥æ™šæœŸé€€å‡ºæ¡ä»¶ï¼ˆå¤„ç†å·¥å…·åï¼‰
    exceeded_iterations = state.get("tool_call_iterations", 0) >= configurable.max_react_tool_calls
    research_complete_called = any(
        tool_call["name"] == "ResearchComplete" 
        for tool_call in most_recent_message.tool_calls
    )
    
    if exceeded_iterations or research_complete_called:
        # ç»“æŸç ”ç©¶å¹¶ç»§ç»­åˆ°å‹ç¼©
        return Command(
            goto="compress_research",
            update={
                "researcher_messages": tool_outputs,
                "total_searches": total_searches  # æ›´æ–°æ€»æœç´¢æ¬¡æ•°
            }
        )
    
    # ç»§ç»­ç ”ç©¶å¾ªç¯ï¼ŒåŒ…å«å·¥å…·ç»“æœ
    return Command(
        goto="researcher",
        update={
            "researcher_messages": tool_outputs,
            "total_searches": total_searches  # æ›´æ–°æ€»æœç´¢æ¬¡æ•°
        }
    )


async def compress_research(state: ResearcherState, config: RunnableConfig):
    """å‹ç¼©å¹¶ç»¼åˆç ”ç©¶å‘ç°ä¸ºç®€æ´ã€ç»“æ„åŒ–çš„æ‘˜è¦ã€‚
    
    æ­¤å‡½æ•°è·å–ç ”ç©¶è€…çš„æ‰€æœ‰ç ”ç©¶å‘ç°ã€å·¥å…·è¾“å‡ºå’ŒAIæ¶ˆæ¯ï¼Œ
    å¹¶å°†å®ƒä»¬æç‚¼ä¸ºå¹²å‡€ã€å…¨é¢çš„æ‘˜è¦ï¼ŒåŒæ—¶ä¿ç•™æ‰€æœ‰é‡è¦ä¿¡æ¯å’Œå‘ç°ã€‚
    
    Args:
        state: å½“å‰ç ”ç©¶è€…çŠ¶æ€ï¼ŒåŒ…å«ç´¯ç§¯çš„ç ”ç©¶æ¶ˆæ¯
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«å‹ç¼©æ¨¡å‹è®¾ç½®
        
    Returns:
        åŒ…å«å‹ç¼©ç ”ç©¶æ‘˜è¦å’ŒåŸå§‹ç¬”è®°çš„å­—å…¸
    """
    # æ­¥éª¤1ï¼šé…ç½®å‹ç¼©æ¨¡å‹
    configurable = Configuration.from_runnable_config(config)
    synthesizer_model = configurable_model.with_config({
        "model": configurable.compression_model,
        "max_tokens": configurable.compression_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.compression_model, config),
        "tags": ["langsmith:nostream"]
    })
    
    # æ­¥éª¤2ï¼šå‡†å¤‡å‹ç¼©æ¶ˆæ¯
    researcher_messages = state.get("researcher_messages", [])
    
    # æ·»åŠ æŒ‡ä»¤ï¼Œä»ç ”ç©¶æ¨¡å¼åˆ‡æ¢åˆ°å‹ç¼©æ¨¡å¼
    researcher_messages.append(HumanMessage(content=compress_research_simple_human_message))
    
    # æ­¥éª¤3ï¼šå°è¯•å‹ç¼©ï¼ŒåŒ…å«tokené™åˆ¶é—®é¢˜çš„é‡è¯•é€»è¾‘
    synthesis_attempts = 0
    max_attempts = 3
    
    while synthesis_attempts < max_attempts:
        try:
            # åˆ›å»ºä¸“æ³¨äºå‹ç¼©ä»»åŠ¡çš„ç³»ç»Ÿæç¤º
            compression_prompt = compress_research_system_prompt.format(date=get_today_str())
            messages = [SystemMessage(content=compression_prompt)] + researcher_messages
            
            # æ‰§è¡Œå‹ç¼©
            response = await synthesizer_model.ainvoke(messages)
            
            # ä»æ‰€æœ‰å·¥å…·å’ŒAIæ¶ˆæ¯ä¸­æå–åŸå§‹ç¬”è®°
            raw_notes_content = "\n".join([
                str(message.content) 
                for message in filter_messages(researcher_messages, include_types=["tool", "ai"])
            ])
            
            # è¿”å›æˆåŠŸçš„å‹ç¼©ç»“æœ
            return {
                "compressed_research": str(response.content),
                "raw_notes": [raw_notes_content]
            }
            
        except Exception as e:
            synthesis_attempts += 1
            
            # é€šè¿‡ç§»é™¤æ—§æ¶ˆæ¯å¤„ç†tokené™åˆ¶è¶…å‡º
            if is_token_limit_exceeded(e, configurable.research_model):
                researcher_messages = remove_up_to_last_ai_message(researcher_messages)
                continue
            
            # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œç»§ç»­é‡è¯•
            continue
    
    # æ­¥éª¤4ï¼šå¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ç»“æœ
    raw_notes_content = "\n".join([
        str(message.content) 
        for message in filter_messages(researcher_messages, include_types=["tool", "ai"])
    ])
    
    return {
        "compressed_research": "é”™è¯¯åˆæˆç ”ç©¶æŠ¥å‘Šï¼šè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°",
        "raw_notes": [raw_notes_content]
    }


# ç ”ç©¶è€…å­å›¾æ„å»º
# åˆ›å»ºä¸ªä½“ç ”ç©¶è€…å·¥ä½œæµï¼Œç”¨äºå¯¹ç‰¹å®šä¸»é¢˜è¿›è¡Œä¸“æ³¨ç ”ç©¶
researcher_builder = StateGraph(
    ResearcherState, 
    output=ResearcherOutputState, 
    config_schema=Configuration
)

# æ·»åŠ ç ”ç©¶æ‰§è¡Œå’Œå‹ç¼©çš„ç ”ç©¶è€…èŠ‚ç‚¹
researcher_builder.add_node("researcher", researcher)                 # ä¸»ç ”ç©¶è€…é€»è¾‘
researcher_builder.add_node("researcher_tools", researcher_tools)     # å·¥å…·æ‰§è¡Œå¤„ç†å™¨
researcher_builder.add_node("compress_research", compress_research)   # ç ”ç©¶å‹ç¼©

# å®šä¹‰ç ”ç©¶è€…å·¥ä½œæµè¾¹
researcher_builder.add_edge(START, "researcher")           # ç ”ç©¶è€…å…¥å£ç‚¹
researcher_builder.add_edge("compress_research", END)      # å‹ç¼©åé€€å‡ºç‚¹

# ç¼–è¯‘ç ”ç©¶è€…å­å›¾ä»¥ä¾›ç›‘ç£è€…å¹¶è¡Œæ‰§è¡Œ
researcher_subgraph = researcher_builder.compile()
