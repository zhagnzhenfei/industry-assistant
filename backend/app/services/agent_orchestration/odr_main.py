"""
Open Deep Research ä¸»å·¥ä½œæµå›¾
åŸºäºå®˜æ–¹æ–‡æ¡£çš„å®Œæ•´ä¸»å·¥ä½œæµå®ç°
"""
import asyncio
import logging
from typing import Literal
# ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨OpenAIç›´æ¥è°ƒç”¨
import openai

logger = logging.getLogger(__name__)
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    get_buffer_string,
)
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.types import Command

# å¯¼å…¥èŠå¤©æ¨¡å‹åˆå§‹åŒ–å‡½æ•°
from .qwen_model import init_qwen_model

from .odr_configuration import Configuration
from .odr_prompts import (
    clarify_with_user_instructions,
    final_report_generation_prompt,
    lead_researcher_prompt,
    transform_messages_into_research_topic_prompt,
)
from .odr_state import (
    AgentInputState,
    AgentState,
    ClarifyWithUser,
    ResearchQuestion,
)
from .odr_utils import (
    get_api_key_for_model,
    get_model_token_limit,
    get_today_str,
    is_token_limit_exceeded,
)

# å¯¼å…¥å­å›¾
from .odr_supervisor import supervisor_subgraph

# åˆå§‹åŒ–å¯é…ç½®æ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªæ™ºèƒ½ä½“ä¸­ä½¿ç”¨
# æ¨¡å‹åç§°ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆLLM_MODELæˆ–é»˜è®¤qwen-plusï¼‰
import logging
logger = logging.getLogger(__name__)

configurable_model = init_qwen_model(
    model=None,  # ä»ç¯å¢ƒå˜é‡LLM_MODELè¯»å–
    max_tokens=4000
)

logger.info(f"ğŸ¤– å…¨å±€æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: model={configurable_model.model_name}")


async def clarify_with_user(state: AgentState, config: RunnableConfig) -> Command[Literal["write_research_brief", "__end__"]]:
    """åˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œå¦‚æœç ”ç©¶èŒƒå›´ä¸æ¸…æ¥šåˆ™è¯¢é—®æ¾„æ¸…é—®é¢˜ã€‚
    
    æ­¤å‡½æ•°ç¡®å®šç”¨æˆ·çš„è¯·æ±‚åœ¨ç»§ç»­ç ”ç©¶ä¹‹å‰æ˜¯å¦éœ€è¦æ¾„æ¸…ã€‚
    å¦‚æœæ¾„æ¸…è¢«ç¦ç”¨æˆ–ä¸éœ€è¦ï¼Œå®ƒç›´æ¥ç»§ç»­ç ”ç©¶ã€‚
    
    Args:
        state: å½“å‰æ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«ç”¨æˆ·æ¶ˆæ¯
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«æ¨¡å‹è®¾ç½®å’Œåå¥½
        
    Returns:
        å‘½ä»¤ï¼ŒæŒ‡ç¤ºä»¥æ¾„æ¸…é—®é¢˜ç»“æŸæˆ–ç»§ç»­åˆ°ç ”ç©¶ç®€æŠ¥
    """
    logger.info("=== ç”¨æˆ·æ¾„æ¸…é˜¶æ®µå¼€å§‹ ===")
    
    try:
        # æ­¥éª¤1ï¼šæ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å¯ç”¨äº†æ¾„æ¸…
        configurable = Configuration.from_runnable_config(config)
        logger.info(f"é…ç½®åŠ è½½æˆåŠŸï¼Œå…è®¸æ¾„æ¸…: {configurable.allow_clarification}")
        
        if not configurable.allow_clarification:
            # è·³è¿‡æ¾„æ¸…æ­¥éª¤ï¼Œç›´æ¥ç»§ç»­ç ”ç©¶
            logger.info("æ¾„æ¸…è¢«ç¦ç”¨ï¼Œç›´æ¥è¿›å…¥ç ”ç©¶è§„åˆ’é˜¶æ®µ")
            return Command(goto="write_research_brief")
        
        # æ­¥éª¤2ï¼šä¸ºç»“æ„åŒ–æ¾„æ¸…åˆ†æå‡†å¤‡æ¨¡å‹
        messages = state["messages"]
        logger.info(f"ç”¨æˆ·æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        model_config = {
            "model": configurable.research_model,
            "max_tokens": configurable.research_model_max_tokens,
            "api_key": get_api_key_for_model(configurable.research_model, config),
            "tags": ["langsmith:nostream"]
        }
        logger.info(f"æ¨¡å‹é…ç½®: {model_config}")
        
        # é…ç½®æ¨¡å‹ï¼ŒåŒ…å«ç»“æ„åŒ–è¾“å‡ºå’Œé‡è¯•é€»è¾‘
        logger.info("é…ç½®æ¾„æ¸…æ¨¡å‹...")
        clarification_model = (
            configurable_model
            .with_structured_output(ClarifyWithUser)
            .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
            .with_config(model_config)
        )
        logger.info("æ¾„æ¸…æ¨¡å‹é…ç½®å®Œæˆ")
        
        # æ­¥éª¤3ï¼šåˆ†ææ˜¯å¦éœ€è¦æ¾„æ¸…
        prompt_content = clarify_with_user_instructions.format(
            messages=get_buffer_string(messages), 
            date=get_today_str()
        )
        logger.info(f"æ¾„æ¸…æç¤ºæ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(prompt_content)}")
        
        logger.info("è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¾„æ¸…åˆ†æ...")
        response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])
        logger.info(f"æ¾„æ¸…åˆ†æç»“æœ: {response}")
        
        # æ­¥éª¤4ï¼šåŸºäºæ¾„æ¸…åˆ†æè¿›è¡Œè·¯ç”±
        if response.need_clarification:
            # ä»¥æ¾„æ¸…é—®é¢˜ç»“æŸï¼Œä¾›ç”¨æˆ·å›ç­”
            logger.info("éœ€è¦æ¾„æ¸…ï¼Œè¿”å›æ¾„æ¸…é—®é¢˜")
            return Command(
                goto=END, 
                update={"messages": [AIMessage(content=response.question)]}
            )
        else:
            # ç»§ç»­ç ”ç©¶ï¼ŒåŒ…å«éªŒè¯æ¶ˆæ¯
            logger.info("æ— éœ€æ¾„æ¸…ï¼Œè¿›å…¥ç ”ç©¶è§„åˆ’é˜¶æ®µ")
            return Command(
                goto="write_research_brief", 
                update={"messages": [AIMessage(content=response.verification)]}
            )
            
    except Exception as e:
        logger.error(f"æ¾„æ¸…é˜¶æ®µå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        # å‡ºé”™æ—¶ç›´æ¥è¿›å…¥ç ”ç©¶è§„åˆ’é˜¶æ®µ
        logger.info("å‡ºé”™æ—¶ç›´æ¥è¿›å…¥ç ”ç©¶è§„åˆ’é˜¶æ®µ")
        return Command(goto="write_research_brief")


async def write_research_brief(state: AgentState, config: RunnableConfig) -> Command[Literal["research_supervisor"]]:
    """å°†ç”¨æˆ·æ¶ˆæ¯è½¬æ¢ä¸ºç»“æ„åŒ–ç ”ç©¶ç®€æŠ¥å¹¶åˆå§‹åŒ–ç›‘ç£è€…ã€‚
    
    æ­¤å‡½æ•°åˆ†æç”¨æˆ·çš„æ¶ˆæ¯å¹¶ç”Ÿæˆå°†æŒ‡å¯¼ç ”ç©¶ç›‘ç£è€…çš„ä¸“æ³¨ç ”ç©¶ç®€æŠ¥ã€‚
    å®ƒè¿˜ä½¿ç”¨é€‚å½“çš„æç¤ºå’Œè¯´æ˜è®¾ç½®åˆå§‹ç›‘ç£è€…ä¸Šä¸‹æ–‡ã€‚
    
    Args:
        state: å½“å‰æ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«ç”¨æˆ·æ¶ˆæ¯
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«æ¨¡å‹è®¾ç½®
        
    Returns:
        å‘½ä»¤ï¼ŒæŒ‡ç¤ºç»§ç»­åˆ°ç ”ç©¶ç›‘ç£è€…ï¼ŒåŒ…å«åˆå§‹åŒ–çš„ä¸Šä¸‹æ–‡
    """
    # æ­¥éª¤1ï¼šä¸ºç»“æ„åŒ–è¾“å‡ºè®¾ç½®ç ”ç©¶æ¨¡å‹
    configurable = Configuration.from_runnable_config(config)
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
        "tags": ["langsmith:nostream"]
    }
    
    # é…ç½®æ¨¡å‹ï¼Œç”¨äºç»“æ„åŒ–ç ”ç©¶é—®é¢˜ç”Ÿæˆ
    research_model = (
        configurable_model
        .with_structured_output(ResearchQuestion)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # æ­¥éª¤2ï¼šä»ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆç»“æ„åŒ–ç ”ç©¶ç®€æŠ¥
    prompt_content = transform_messages_into_research_topic_prompt.format(
        messages=get_buffer_string(state.get("messages", [])),
        date=get_today_str()
    )
    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # æ­¥éª¤3ï¼šä½¿ç”¨ç ”ç©¶ç®€æŠ¥å’Œè¯´æ˜åˆå§‹åŒ–ç›‘ç£è€…ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼Œè¿›åº¦ä¿¡æ¯å°†åœ¨supervisorèŠ‚ç‚¹ä¸­åŠ¨æ€æ›´æ–°ï¼‰
    # è®¡ç®—é˜¶æ®µè¾¹ç•Œï¼Œç”¨äºæç¤ºè¯ä¸­çš„æ•°å­¦è¡¨è¾¾å¼
    early_stage_end = configurable.max_researcher_iterations // 3
    middle_stage_start = early_stage_end + 1
    middle_stage_end = 2 * configurable.max_researcher_iterations // 3
    final_stage_start = middle_stage_end + 1

    supervisor_system_prompt = lead_researcher_prompt.format(
        date=get_today_str(),
        max_researcher_iterations=configurable.max_researcher_iterations,
        max_concurrent_research_units=configurable.max_concurrent_research_units,
        current_iteration=1,
        used_research_units=0,
        remaining_iterations=configurable.max_researcher_iterations,
        remaining_units=configurable.max_concurrent_research_units,
        # æä¾›è®¡ç®—å¥½çš„é˜¶æ®µè¾¹ç•Œå€¼ï¼Œé¿å…åœ¨formatä¸­ä½¿ç”¨æ•°å­¦è¡¨è¾¾å¼
        early_stage_end=early_stage_end,
        middle_stage_start=middle_stage_start,
        middle_stage_end=middle_stage_end,
        final_stage_start=final_stage_start,
        mcp_prompt=""
    )
    
    return Command(
        goto="research_supervisor",
        update={
            "research_brief": response.research_brief,
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=supervisor_system_prompt),
                    HumanMessage(content=response.research_brief)
                ]
            },
            # åˆå§‹åŒ–è®¡æ•°å™¨
            "research_iterations": 0,
            "used_research_units": 0
        }
    )


async def final_report_generation(state: AgentState, config: RunnableConfig):
    """ä½¿ç”¨tokené™åˆ¶é‡è¯•é€»è¾‘ç”Ÿæˆæœ€ç»ˆç»¼åˆç ”ç©¶æŠ¥å‘Šã€‚
    
    æ­¤å‡½æ•°è·å–æ‰€æœ‰æ”¶é›†çš„ç ”ç©¶å‘ç°ï¼Œå¹¶ä½¿ç”¨é…ç½®çš„æŠ¥å‘Šç”Ÿæˆæ¨¡å‹
    å°†å®ƒä»¬ç»¼åˆä¸ºç»“æ„è‰¯å¥½ã€å…¨é¢çš„æœ€ç»ˆæŠ¥å‘Šã€‚
    
    Args:
        state: æ™ºèƒ½ä½“çŠ¶æ€ï¼ŒåŒ…å«ç ”ç©¶å‘ç°å’Œä¸Šä¸‹æ–‡
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«æ¨¡å‹è®¾ç½®å’ŒAPIå¯†é’¥
        
    Returns:
        åŒ…å«æœ€ç»ˆæŠ¥å‘Šå’Œæ¸…ç†çŠ¶æ€çš„å­—å…¸
    """
    # æ­¥éª¤1ï¼šæå–ç ”ç©¶å‘ç°å¹¶å‡†å¤‡çŠ¶æ€æ¸…ç†
    notes = state.get("notes", [])
    cleared_state = {"notes": {"type": "override", "value": []}}
    findings = "\n".join(notes)
    
    # æ­¥éª¤2ï¼šé…ç½®æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆæ¨¡å‹
    configurable = Configuration.from_runnable_config(config)
    writer_model_config = {
        "model": configurable.final_report_model,
        "max_tokens": configurable.final_report_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.final_report_model, config),
        "tags": ["langsmith:nostream"]
    }
    
    # æ­¥éª¤3ï¼šå°è¯•æŠ¥å‘Šç”Ÿæˆï¼ŒåŒ…å«tokené™åˆ¶é‡è¯•é€»è¾‘
    max_retries = 3
    current_retry = 0
    findings_token_limit = None
    
    while current_retry <= max_retries:
        try:
            # åˆ›å»ºåŒ…å«æ‰€æœ‰ç ”ç©¶ä¸Šä¸‹æ–‡çš„ç»¼åˆæç¤º
            final_report_prompt = final_report_generation_prompt.format(
                research_brief=state.get("research_brief", ""),
                messages=get_buffer_string(state.get("messages", [])),
                findings=findings,
                date=get_today_str()
            )
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await configurable_model.with_config(writer_model_config).ainvoke([
                HumanMessage(content=final_report_prompt)
            ])
            
            base_report = final_report.content
            
            # å°è¯•æ·»åŠ å¯è§†åŒ–å¢å¼º
            try:
                from services.visualization.report_enhancer import ReportEnhancer
                from configs.visualization_config import get_visualization_settings
                
                logger.info("[REPORT] ğŸ¨ å¼€å§‹å¯è§†åŒ–å¢å¼º...")
                settings = get_visualization_settings()
                enhancer = ReportEnhancer(
                    configurable_model.with_config(writer_model_config),
                    base_url=settings.base_url
                )
                
                result = await enhancer.enhance(base_report)
                enhanced_report = result["enhanced_report"]
                
                logger.info(f"[REPORT] âœ“ å¯è§†åŒ–å®Œæˆ: {result['chart_count']}ä¸ªå›¾è¡¨, è€—æ—¶{result['processing_time']:.2f}ç§’")
                
                # ä½¿ç”¨å¢å¼ºåçš„æŠ¥å‘Š
                final_report_content = enhanced_report
            except Exception as e:
                logger.warning(f"[REPORT] âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
                # é™çº§ï¼šä½¿ç”¨åŸºç¡€æŠ¥å‘Š
                final_report_content = base_report
            
            # è¿”å›æˆåŠŸçš„æŠ¥å‘Šç”Ÿæˆ
            return {
                "final_report": final_report_content, 
                "messages": [final_report],
                **cleared_state
            }
            
        except Exception as e:
            # é€šè¿‡æ¸è¿›æˆªæ–­å¤„ç†tokené™åˆ¶è¶…å‡ºé”™è¯¯
            if is_token_limit_exceeded(e, configurable.final_report_model):
                current_retry += 1
                
                if current_retry == 1:
                    # ç¬¬ä¸€æ¬¡é‡è¯•ï¼šç¡®å®šåˆå§‹æˆªæ–­é™åˆ¶
                    model_token_limit = get_model_token_limit(configurable.final_report_model)
                    if not model_token_limit:
                        return {
                            "final_report": f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šé”™è¯¯ï¼šTokené™åˆ¶è¶…å‡ºï¼Œä½†æ˜¯ï¼Œæˆ‘ä»¬æ— æ³•ç¡®å®šæ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¯·åœ¨deep_researcher/utils.pyä¸­æ›´æ–°æ¨¡å‹æ˜ å°„ï¼ŒåŒ…å«æ­¤ä¿¡æ¯ã€‚{e}",
                            "messages": [AIMessage(content="ç”±äºtokené™åˆ¶ï¼ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥")],
                            **cleared_state
                        }
                    # ä½¿ç”¨4x tokené™åˆ¶ä½œä¸ºæˆªæ–­çš„å­—ç¬¦è¿‘ä¼¼
                    findings_token_limit = model_token_limit * 4
                else:
                    # åç»­é‡è¯•ï¼šæ¯æ¬¡å‡å°‘10%
                    findings_token_limit = int(findings_token_limit * 0.9)
                
                # æˆªæ–­å‘ç°å¹¶é‡è¯•
                findings = findings[:findings_token_limit]
                continue
            else:
                # étokené™åˆ¶é”™è¯¯ï¼šç«‹å³è¿”å›é”™è¯¯
                return {
                    "final_report": f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šé”™è¯¯: {e}",
                    "messages": [AIMessage(content="ç”±äºé”™è¯¯ï¼ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥")],
                    **cleared_state
                }
    
    # æ­¥éª¤4ï¼šå¦‚æœæ‰€æœ‰é‡è¯•éƒ½è€—å°½ï¼Œè¿”å›å¤±è´¥ç»“æœ
    return {
        "final_report": "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šé”™è¯¯ï¼šè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°",
        "messages": [AIMessage(content="åœ¨æœ€å¤§é‡è¯•æ¬¡æ•°åæŠ¥å‘Šç”Ÿæˆå¤±è´¥")],
        **cleared_state
    }




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æ·±åº¦ç ”ç©¶è€…å›¾æ„å»ºï¼ˆæ”¯æŒå¯é€‰è®°å¿†åŠŸèƒ½ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_deep_researcher_graph():
    """æ„å»ºæ·±åº¦ç ”ç©¶è€…å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— è®°å¿†åŠŸèƒ½ï¼‰

    Returns:
        ç¼–è¯‘åçš„å›¾
    """
    logger.info("ğŸ—ï¸ æ„å»ºæ·±åº¦ç ”ç©¶è€…å›¾ï¼ˆæ ‡å‡†æµç¨‹ï¼‰")

    # åˆ›å»ºå›¾æ„å»ºå™¨
    builder = StateGraph(
        AgentState,
        input=AgentInputState,
        config_schema=Configuration
    )

    # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹
    builder.add_node("clarify_with_user", clarify_with_user)
    builder.add_node("write_research_brief", write_research_brief)
    builder.add_node("research_supervisor", supervisor_subgraph)
    builder.add_node("final_report_generation", final_report_generation)

    # å®šä¹‰æ ‡å‡†æµç¨‹è¾¹
    builder.add_edge(START, "clarify_with_user")
    builder.add_edge("research_supervisor", "final_report_generation")
    builder.add_edge("final_report_generation", END)

    logger.info("âœ… æ·±åº¦ç ”ç©¶è€…å›¾æ„å»ºå®Œæˆ")
    return builder.compile()


# åˆ›å»ºé»˜è®¤å›¾å®ä¾‹ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰
deep_researcher = build_deep_researcher_graph()
