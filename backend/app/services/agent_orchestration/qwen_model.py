"""
Qwenæ¨¡å‹é›†æˆ
æ”¯æŒé˜¿é‡Œäº‘é€šä¹‰åƒé—®æ¨¡å‹
"""
import os
import logging
from typing import Any, Dict, Optional
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun
import openai
from openai import OpenAI

logger = logging.getLogger(__name__)

class QwenChatModel(BaseChatModel):
    """é€šç”¨èŠå¤©æ¨¡å‹åŒ…è£…å™¨ï¼ˆæ”¯æŒOpenAIå…¼å®¹APIï¼‰
    
    æ”¯æŒå¤šç§æœåŠ¡ï¼š
    - é˜¿é‡Œäº‘é€šä¹‰åƒé—®
    - ç¡…åŸºæµåŠ¨ï¼ˆSiliconFlowï¼‰
    - å…¶ä»–OpenAIå…¼å®¹æœåŠ¡
    """
    
    model_name: str = "qwen-plus"
    api_key: Optional[str] = None
    base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    max_tokens: int = 1000000
    temperature: float = 0.7
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼ï¼‰
        api_key = self.api_key or os.getenv("DASHSCOPE_API_KEY")
        base_url = os.getenv("DASHSCOPE_BASE_URL") or self.base_url
        
        # æ—¥å¿—è®°å½•å®é™…ä½¿ç”¨çš„é…ç½®
        logger.info(f"ğŸ”§ æ¨¡å‹é…ç½®: model={self.model_name}, base_url={base_url}")
        logger.info(f"ğŸ”‘ API Key: {api_key[:10] if api_key else 'NOT SET'}...")
        
        # åœ¨__init__ä¸­åˆå§‹åŒ–client
        object.__setattr__(self, 'client', OpenAI(
            api_key=api_key,
            base_url=base_url,
        ))
    
    @property
    def _llm_type(self) -> str:
        return "qwen"
    
    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”ŸæˆèŠå¤©å“åº”"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        openai_messages = []
        for message in messages:
            if isinstance(message, SystemMessage):
                openai_messages.append({"role": "system", "content": message.content})
            elif isinstance(message, HumanMessage):
                openai_messages.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                openai_messages.append({"role": "assistant", "content": message.content})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=openai_messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                **kwargs
            )
            
            message = response.choices[0].message
            ai_message = AIMessage(content=message.content)
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if hasattr(message, 'tool_calls') and message.tool_calls:
                ai_message.tool_calls = []
                for tool_call in message.tool_calls:
                    if hasattr(tool_call, 'function'):
                        # OpenAI æ ¼å¼çš„å·¥å…·è°ƒç”¨
                        # è§£æ arguments å­—ç¬¦ä¸²ä¸ºå­—å…¸
                        import json
                        try:
                            args_dict = json.loads(tool_call.function.arguments)
                        except (json.JSONDecodeError, TypeError):
                            args_dict = {}
                        
                        ai_message.tool_calls.append({
                            "name": tool_call.function.name,
                            "args": args_dict,
                            "id": tool_call.id
                        })
                    else:
                        # å…¶ä»–æ ¼å¼çš„å·¥å…·è°ƒç”¨
                        ai_message.tool_calls.append(tool_call)
            
            return ChatResult(generations=[ChatGeneration(message=ai_message)])
            
        except Exception as e:
            raise Exception(f"Qwen APIè°ƒç”¨å¤±è´¥: {e}")
    
    async def _agenerate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """å¼‚æ­¥ç”ŸæˆèŠå¤©å“åº”"""
        # å¯¹äºç®€å•çš„å®ç°ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
        return self._generate(messages, stop, run_manager, **kwargs)
    
    def with_structured_output(self, schema, **kwargs):
        """æ”¯æŒç»“æ„åŒ–è¾“å‡ºçš„åŒ…è£…å™¨"""
        from langchain_core.runnables import RunnableLambda

        def extract_json_from_markdown(content: str) -> str:
            """
            ä»markdownä»£ç å—ä¸­æå–JSONå†…å®¹
            å¦‚æœæ²¡æœ‰markdownæ ‡è®°ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
            """
            import re

            # åŒ¹é… ```json æˆ– ``` åŒ…è£¹çš„å†…å®¹
            pattern = r'```(?:json)?\s*\n?(.*?)\n?```'
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)

            if matches:
                return matches[0].strip()

            return content.strip()

        def parse_output(messages):
            """è§£æè¾“å‡ºä¸ºç»“æ„åŒ–æ ¼å¼"""
            try:
                # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”
                response = self._generate(messages)
                content = response.generations[0].message.content

                # é¦–å…ˆæ¸…ç†markdownæ ¼å¼çš„JSON
                content = extract_json_from_markdown(content)

                # ç„¶åå°è¯•ç›´æ¥è§£æä¸ºJSON
                try:
                    import json
                    data = json.loads(content)
                    return schema(**data)
                except:
                    pass
                
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•Pydanticçš„model_validate_json
                try:
                    return schema.model_validate_json(content)
                except:
                    pass
                
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
                try:
                    # å¯¹äºä¸åŒçš„schemaç±»å‹ï¼Œä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
                    if schema.__name__ == "ClarifyWithUser":
                        # è§£ææ¾„æ¸…è¯·æ±‚
                        need_clarification = "clarification" in content.lower() or "clarify" in content.lower()
                        question = content if need_clarification else ""
                        verification = "æˆ‘ä»¬å°†å¼€å§‹ç ”ç©¶" if not need_clarification else "è¯·æä¾›æ›´å¤šä¿¡æ¯"
                        return schema(
                            need_clarification=need_clarification,
                            question=question,
                            verification=verification
                        )
                    elif schema.__name__ == "ResearchQuestion":
                        # è§£æç ”ç©¶é—®é¢˜
                        return schema(research_brief=content)
                    elif schema.__name__ == "Summary":
                        # è§£ææ‘˜è¦
                        return schema(
                            summary=content,
                            key_excerpts=content
                        )
                    elif schema.__name__ == "ConductResearch":
                        # è§£æç ”ç©¶ä¸»é¢˜
                        return schema(research_topic=content)
                    elif schema.__name__ == "ResearchComplete":
                        # ç ”ç©¶å®Œæˆ
                        return schema()
                    else:
                        # é»˜è®¤å¤„ç†
                        if hasattr(schema, '__fields__'):
                            fields = schema.__fields__
                            result = {}
                            for field_name in fields.keys():
                                result[field_name] = content
                            return schema(**result)
                        return content
                except Exception as e:
                    # æœ€åçš„å…œåº•ç­–ç•¥
                    print(f"ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥: {e}")
                    if schema.__name__ == "ClarifyWithUser":
                        return schema(need_clarification=False, question="", verification="å¼€å§‹ç ”ç©¶")
                    elif schema.__name__ == "ResearchQuestion":
                        return schema(research_brief=content)
                    elif schema.__name__ == "Summary":
                        return schema(summary=content, key_excerpts=content)
                    elif schema.__name__ == "ConductResearch":
                        return schema(research_topic=content)
                    elif schema.__name__ == "ResearchComplete":
                        return schema()
                    else:
                        return content
            except Exception as e:
                print(f"ç»“æ„åŒ–è¾“å‡ºå®Œå…¨å¤±è´¥: {e}")
                # è¿”å›é»˜è®¤å€¼
                if schema.__name__ == "ClarifyWithUser":
                    return schema(need_clarification=False, question="", verification="å¼€å§‹ç ”ç©¶")
                elif schema.__name__ == "ResearchQuestion":
                    return schema(research_brief="é»˜è®¤ç ”ç©¶ç®€æŠ¥")
                elif schema.__name__ == "Summary":
                    return schema(summary="é»˜è®¤æ‘˜è¦", key_excerpts="é»˜è®¤æ‘˜å½•")
                elif schema.__name__ == "ConductResearch":
                    return schema(research_topic="é»˜è®¤ç ”ç©¶ä¸»é¢˜")
                elif schema.__name__ == "ResearchComplete":
                    return schema()
                else:
                    return "è§£æå¤±è´¥"
        
        # è¿”å›ä¸€ä¸ªåŒ…è£…äº†è§£æé€»è¾‘çš„runnable
        return RunnableLambda(lambda x: parse_output(x))
    
    def with_retry(self, **kwargs):
        """æ”¯æŒé‡è¯•çš„åŒ…è£…å™¨"""
        # ç®€å•çš„é‡è¯•å®ç°
        return self
    
    def bind_tools(self, tools, **kwargs):
        """ç»‘å®šå·¥å…·åˆ°æ¨¡å‹"""
        # å¦‚æœæœ‰å·¥å…·ï¼Œå°†å·¥å…·ä¿¡æ¯æ·»åŠ åˆ°APIè°ƒç”¨ä¸­
        if tools:
            # è½¬æ¢å·¥å…·ä¸ºOpenAIæ ¼å¼
            openai_tools = []
            for tool in tools:
                try:
                    # è·å–å·¥å…·ä¿¡æ¯
                    if hasattr(tool, 'name'):
                        tool_name = tool.name
                    elif hasattr(tool, 'tool_name'):
                        tool_name = tool.tool_name
                    elif hasattr(tool, '__name__'):
                        tool_name = tool.__name__
                    else:
                        tool_name = str(tool)
                    
                    if hasattr(tool, 'description'):
                        tool_desc = tool.description
                    elif hasattr(tool, 'desc'):
                        tool_desc = tool.desc
                    else:
                        tool_desc = "æ— æè¿°"
                    
                    # è·å–å‚æ•°schema
                    tool_schema = {}
                    if hasattr(tool, 'args_schema'):
                        tool_schema = tool.args_schema.model_json_schema()
                    elif hasattr(tool, 'parameters'):
                        tool_schema = tool.parameters
                    
                    openai_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool_name,
                            "description": tool_desc,
                            "parameters": tool_schema
                        }
                    })
                except Exception as e:
                    logger.warning(f"Tool binding error: {e}, tool type: {type(tool)}")
                    continue
            
            # åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨ï¼Œå°†å·¥å…·ä¿¡æ¯ä¼ é€’ç»™API
            from langchain_core.runnables import RunnableLambda
            
            def tool_wrapper(messages):
                return self._generate(messages, tools=openai_tools, **kwargs)
            
            return RunnableLambda(lambda x: tool_wrapper(x))
        else:
            # æ²¡æœ‰å·¥å…·æ—¶ï¼Œç›´æ¥è¿”å›æ¨¡å‹
            return self

def init_qwen_model(
    model: str = None,
    api_key: Optional[str] = None,
    max_tokens: int = 4000,
    temperature: float = 0.7,
    **kwargs
) -> QwenChatModel:
    """
    åˆå§‹åŒ–èŠå¤©æ¨¡å‹ï¼ˆæ”¯æŒOpenAIå…¼å®¹APIï¼‰
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡TEXT2SQL_MODELæˆ–LLM_MODELè¯»å–
        api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°
    
    ç¯å¢ƒå˜é‡ï¼š
        - TEXT2SQL_MODEL: Text2SQLä¸“ç”¨æ¨¡å‹åç§°
        - LLM_MODEL: é€šç”¨æ¨¡å‹åç§°
        - DASHSCOPE_API_KEY: APIå¯†é’¥
        - DASHSCOPE_BASE_URL: APIåŸºç¡€URL
    
    Examples:
        # ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®
        export DASHSCOPE_API_KEY=sk-xxx
        export DASHSCOPE_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
        export TEXT2SQL_MODEL=qwen-plus
        
        # ä½¿ç”¨ç¡…åŸºæµåŠ¨
        export DASHSCOPE_API_KEY=sk-xxx
        export DASHSCOPE_BASE_URL=https://api.siliconflow.cn/v1
        export TEXT2SQL_MODEL=Qwen/Qwen2.5-7B-Instruct
    """
    # æ¨¡å‹åç§°ä¼˜å…ˆçº§ï¼šå‚æ•° > TEXT2SQL_MODEL > LLM_MODEL > é»˜è®¤å€¼
    if model is None:
        model = os.getenv("TEXT2SQL_MODEL") or os.getenv("LLM_MODEL") or "qwen-plus"
        logger.info(f"ğŸ“ æ¨¡å‹åç§°é€‰æ‹©: TEXT2SQL_MODEL={os.getenv('TEXT2SQL_MODEL')}, LLM_MODEL={os.getenv('LLM_MODEL')}, æœ€ç»ˆ={model}")
    
    return QwenChatModel(
        model_name=model,
        api_key=api_key,
        max_tokens=max_tokens,
        temperature=temperature,
        **kwargs
    )

def get_api_key_for_qwen() -> str:
    """è·å–Qwen APIå¯†é’¥"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
    return api_key
