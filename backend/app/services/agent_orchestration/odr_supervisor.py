"""
Open Deep Research ç›‘ç£è€…å­å›¾ - ç®€åŒ–æ¶æ„
åˆå¹¶supervisorå’Œdecision_executorä¸ºå•ä¸€supervisor_plannerèŠ‚ç‚¹ï¼Œæ¶ˆé™¤åµŒå¥—å­å›¾
"""
import asyncio
import logging
import re
from typing import Literal, TypedDict
from pydantic import BaseModel, Field
from typing import List

logger = logging.getLogger(__name__)


class ResearchTopicsResponse(BaseModel):
    """ç ”ç©¶ä¸»é¢˜ç”Ÿæˆå“åº”æ¨¡å‹"""
    analysis: str = Field(description="å¯¹ç ”ç©¶ç®€æŠ¥çš„åˆ†æï¼Œè¯†åˆ«çš„å…³é”®ç»´åº¦")
    research_topics: List[str] = Field(description="ç”Ÿæˆçš„ç ”ç©¶ä¸»é¢˜åˆ—è¡¨")
    reasoning: str = Field(description="ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›ä¸»é¢˜ï¼Œå®ƒä»¬å¦‚ä½•äº’è¡¥")

from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    get_buffer_string,
)
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.types import Command

from .qwen_model import init_qwen_model
from .odr_configuration import Configuration
from .odr_prompts import lead_researcher_prompt, generate_research_topics_prompt
from .odr_state import SupervisorState
from .odr_utils import (
    get_api_key_for_model,
    get_notes_from_tool_calls,
    is_token_limit_exceeded,
)

# LangSmith é›†æˆ
try:
    from ..utils.langsmith_integration import (
        trace_node,
        log_node_execution,
        get_langsmith_config,
        is_langsmith_enabled
    )
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    # æä¾›ç©ºå®ç°
    def trace_node(*args, **kwargs):
        def decorator(func):
            return func
        return decorator

    def log_node_execution(*args, **kwargs):
        pass

    def get_langsmith_config(*args, **kwargs):
        return {}

    def is_langsmith_enabled():
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ§åˆ¶å™¨ç±» - é˜²æ­¢æ— é™ç ”ç©¶
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ResearchQualityController:
    """ç ”ç©¶è´¨é‡æ§åˆ¶å™¨ - é˜²æ­¢ä½æ•ˆé‡å¤ç ”ç©¶"""

    def __init__(self):
        self.quality_history = []
        self.recent_improvements = []

    def should_continue_research(self, current_findings, previous_findings, iteration):
        """ä¸¥æ ¼çš„ç»§ç»­ç ”ç©¶åˆ¤æ–­"""

        # 1. ä¿¡æ¯å¢ç›Šè¯„ä¼°
        information_gain = self.calculate_information_gain(current_findings, previous_findings)
        if information_gain < 0.05:  # æ–°ä¿¡æ¯å°‘äº5%
            return False, "ä¿¡æ¯å¢ç›Šè¿‡ä½"

        # 2. è´¨é‡æ”¹å–„è¶‹åŠ¿
        quality_trend = self.analyze_quality_trend(self.quality_history)
        if quality_trend < 0 and iteration > 2:  # è´¨é‡è¿ç»­ä¸‹é™
            return False, "ç ”ç©¶è´¨é‡å‘ˆä¸‹é™è¶‹åŠ¿"

        # 3. é¥±å’Œåº¦æ£€æµ‹
        saturation_score = self.calculate_saturation_score(current_findings)
        if saturation_score > 0.8:  # ç ”ç©¶é¥±å’Œåº¦è¿‡é«˜
            return False, "ç ”ç©¶é¢†åŸŸå·²åŸºæœ¬é¥±å’Œ"

        # 4. æ•ˆç‡è¯„ä¼°
        efficiency = self.calculate_research_efficiency(current_findings, iteration)
        if efficiency < 0.3 and iteration > 1:  # æ•ˆç‡å¤ªä½
            return False, "ç ”ç©¶æ•ˆç‡è¿‡ä½"

        return True, "ç»§ç»­ç ”ç©¶æœ‰ä»·å€¼"

    def calculate_information_gain(self, current_findings, previous_findings):
        """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
        if not previous_findings:
            return 1.0

        # ç®€åŒ–çš„æ–°ä¿¡æ¯æ¯”ä¾‹è®¡ç®—
        current_text = " ".join(current_findings).lower()
        previous_text = " ".join(previous_findings).lower()

        # è®¡ç®—é‡å¤å†…å®¹çš„æ¯”ä¾‹
        common_words = set(current_text.split()) & set(previous_text.split())
        total_words = set(current_text.split())

        if not total_words:
            return 1.0

        overlap_ratio = len(common_words) / len(total_words)
        return 1.0 - overlap_ratio  # æ–°ä¿¡æ¯è¶Šå¤šï¼Œå¢ç›Šè¶Šå¤§

    def analyze_quality_trend(self, quality_history):
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(quality_history) < 2:
            return 1.0

        # è®¡ç®—æœ€è¿‘çš„è¶‹åŠ¿
        recent_scores = quality_history[-3:]  # æœ€è¿‘3æ¬¡
        if len(recent_scores) < 2:
            return 1.0

        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿
        return (recent_scores[-1] - recent_scores[0]) / len(recent_scores)

    def calculate_saturation_score(self, findings):
        """è®¡ç®—ç ”ç©¶é¥±å’Œåº¦"""
        if not findings:
            return 0.0

        # åŸºäºå‘ç°æ•°é‡å’Œå†…å®¹é‡å¤åº¦çš„é¥±å’Œåº¦è¯„ä¼°
        total_words = sum(len(f.split()) for f in findings)
        unique_words = len(set(" ".join(findings).lower().split()))

        if total_words == 0:
            return 0.0

        # é‡å¤åº¦è¶Šé«˜ï¼Œé¥±å’Œåº¦è¶Šé«˜
        return 1.0 - (unique_words / total_words)

    def calculate_research_efficiency(self, findings, iteration):
        """è®¡ç®—ç ”ç©¶æ•ˆç‡"""
        if not findings or iteration == 0:
            return 1.0

        # åŸºäºå¹³å‡å‘ç°é•¿åº¦å’Œè´¨é‡
        avg_length = sum(len(f) for f in findings) / len(findings)

        # æ•ˆç‡è¯„åˆ†ï¼ˆç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼‰
        if avg_length > 200:  # è¯¦ç»†çš„å‘ç°
            return 0.8
        elif avg_length > 100:
            return 0.6
        else:
            return 0.4


class ProgressiveCompletionStrategy:
    """æ¸è¿›å¼å®Œæˆç­–ç•¥ - åŠ¨æ€è°ƒæ•´å®Œæˆæ ‡å‡†"""

    def get_completion_threshold(self, iteration, max_iterations):
        """æ¸è¿›å¼å®Œæˆé˜ˆå€¼"""

        # æ—©æœŸé˜¶æ®µï¼šé«˜è´¨é‡è¦æ±‚
        if iteration <= max_iterations * 0.3:
            return 0.85  # éœ€è¦85%çš„å®Œæˆåº¦

        # ä¸­æœŸé˜¶æ®µï¼šä¸­ç­‰è´¨é‡è¦æ±‚
        elif iteration <= max_iterations * 0.7:
            return 0.70  # éœ€è¦70%çš„å®Œæˆåº¦

        # åæœŸé˜¶æ®µï¼šè¾ƒä½è´¨é‡è¦æ±‚ï¼ˆé¿å…æ— é™ç ”ç©¶ï¼‰
        else:
            return max(0.50, 0.85 - (iteration / max_iterations) * 0.35)

    def should_force_complete(self, state, config):
        """å¼ºåˆ¶å®Œæˆåˆ¤æ–­"""

        iteration = state.get("research_iterations", 0)
        # å¤„ç†configå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
        if isinstance(config, dict):
            max_iterations = config.get("max_researcher_iterations", 5)
        else:
            max_iterations = config.max_researcher_iterations

        # 1. æ¥è¿‘é™åˆ¶æ—¶å¼ºåˆ¶è€ƒè™‘å®Œæˆ
        if iteration >= max_iterations - 1:
            return True, "æ¥è¿‘è¿­ä»£é™åˆ¶"

        # 2. è¿ç»­ä¸‰è½®æ— æ˜¾è‘—æ”¹å–„ï¼ˆç®€åŒ–å®ç°ï¼‰
        recent_notes_count = len(state.get("notes", []))
        if iteration > 3 and recent_notes_count < iteration * 2:  # å¹³å‡æ¯è½®å°‘äº2ä¸ªå‘ç°
            return True, "ç ”ç©¶å‘ç°å¢é•¿ç¼“æ…¢"

        # 3. èµ„æºæ•ˆç‡è¿‡ä½
        used_units = state.get("used_research_units", 0)
        if used_units > 0 and recent_notes_count / used_units < 0.5:  # æ¯ä¸ªç ”ç©¶å•å…ƒå¹³å‡å°‘äº0.5ä¸ªå‘ç°
            return True, "èµ„æºä½¿ç”¨æ•ˆç‡è¿‡ä½"

        return False, ""


class SmartExitController:
    """æ™ºèƒ½é€€å‡ºæ§åˆ¶å™¨ - å¤šç»´åº¦è¯„ä¼°é€€å‡ºæ—¶æœº"""

    def should_force_complete(self, state, config):
        """å¼ºåˆ¶å®Œæˆåˆ¤æ–­"""

        iteration = state.get("research_iterations", 0)
        # å¤„ç†configå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
        if isinstance(config, dict):
            max_iterations = config.get("max_researcher_iterations", 5)
        else:
            max_iterations = config.max_researcher_iterations

        # 1. æ¥è¿‘é™åˆ¶æ—¶å¼ºåˆ¶è€ƒè™‘å®Œæˆ
        if iteration >= max_iterations - 1:
            return True, "æ¥è¿‘è¿­ä»£é™åˆ¶"

        # 2. ç ”ç©¶å‘ç°å¢é•¿ç¼“æ…¢
        notes = state.get("notes", [])
        if iteration > 2 and len(notes) < iteration:
            return True, "ç ”ç©¶å‘ç°å¢é•¿ç¼“æ…¢"

        return False, ""

    def evaluate_exit_conditions(self, state, config):
        """å¤šç»´åº¦é€€å‡ºæ¡ä»¶è¯„ä¼°"""

        notes = state.get("notes", [])
        research_brief = state.get("research_brief", "")

        # 1. æ£€æŸ¥å‘ç°å……åˆ†æ€§
        findings_sufficiency = self.check_findings_sufficiency(notes, research_brief)

        # 2. æ£€æŸ¥ä¿¡æ¯å¯†åº¦
        information_density = self.check_information_density(notes)

        # 3. è®¡ç®—æ€»ä½“è¯„åˆ†
        conditions = {
            "findings_sufficiency": findings_sufficiency,
            "information_density": information_density
        }

        exit_strength = sum(conditions.values()) / len(conditions)

        # é€€å‡ºå»ºè®®
        if exit_strength > 0.7:
            return "strong_complete", f"å¼ºçƒˆå»ºè®®å®Œæˆ: å‘ç°å……åˆ†({findings_sufficiency:.2f}), ä¿¡æ¯å¯†åº¦({information_density:.2f})"
        elif exit_strength > 0.4:
            return "consider_complete", f"è€ƒè™‘å®Œæˆ: å‘ç°å……åˆ†({findings_sufficiency:.2f}), ä¿¡æ¯å¯†åº¦({information_density:.2f})"
        else:
            return "continue", "å»ºè®®ç»§ç»­ç ”ç©¶"

    def check_findings_sufficiency(self, notes, research_brief):
        """æ£€æŸ¥å‘ç°å……åˆ†æ€§"""
        if not notes:
            return 0.0

        # åŸºäºå‘ç°æ•°é‡å’Œè´¨é‡çš„ç®€å•è¯„ä¼°
        findings_count = len(notes)
        avg_length = sum(len(note) for note in notes) / findings_count if findings_count > 0 else 0

        # æ•°é‡è¯„åˆ† (0-0.5)
        count_score = min(0.5, findings_count / 10)  # 10ä¸ªå‘ç°å¾—æ»¡åˆ†

        # è´¨é‡è¯„åˆ† (0-0.5)
        quality_score = min(0.5, avg_length / 400)  # 400å­—ç¬¦å¾—æ»¡åˆ†

        return count_score + quality_score

    def check_information_density(self, notes):
        """æ£€æŸ¥ä¿¡æ¯å¯†åº¦"""
        if not notes:
            return 0.0

        total_text = " ".join(notes)
        words = total_text.split()
        unique_words = set(word.lower() for word in words)

        if not words:
            return 0.0

        # ä¿¡æ¯å¯†åº¦ï¼šç‹¬ç‰¹è¯æ±‡æ¯”ä¾‹
        density = len(unique_words) / len(words)

        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        return min(1.0, density * 2)


class ResearchStateAnalyzer:
    """ç ”ç©¶çŠ¶æ€åˆ†æå™¨ - æ·±åº¦åˆ†æå¹¶ç»™å‡ºè¡ŒåŠ¨å»ºè®®"""

    async def analyze_research_state(self, state: SupervisorState, config) -> dict:
        """æ·±åº¦åˆ†æç ”ç©¶çŠ¶æ€ï¼Œç»™å‡ºæœ€ä¼˜è¡ŒåŠ¨å»ºè®®"""

        # 1. è´¨é‡è¯„ä¼°
        quality_metrics = self.assess_quality(state)

        # 2. è¦†ç›–åº¦åˆ†æ
        coverage_analysis = self.analyze_coverage(state)

        # 3. èµ„æºçŠ¶æ€
        resource_status = self.check_resource_status(state, config)

        # 4. ç»¼åˆå†³ç­–
        action = self.make_intelligent_decision(
            quality_metrics, coverage_analysis, resource_status
        )

        return {
            "action": action,  # "research", "complete", "refine"
            "research_topics": await self.generate_research_topics(state, action, config),
            "strategy": self.determine_strategy(state, action),
            "confidence": self.calculate_decision_confidence(),
            "reasoning": self.explain_decision(quality_metrics, coverage_analysis),
            "quality_metrics": quality_metrics,
            "coverage_analysis": coverage_analysis,
            "resource_status": resource_status
        }

    def assess_quality(self, state):
        """è¯„ä¼°ç ”ç©¶è´¨é‡"""
        notes = state.get("notes", [])
        if not notes:
            return {"score": 0.0, "breadth": 0.0, "depth": 0.0}

        # è´¨é‡æŒ‡æ ‡
        findings_count = len(notes)
        avg_length = sum(len(note) for note in notes) / findings_count

        breadth_score = min(1.0, findings_count / 5)  # 5ä¸ªå‘ç°ä¸ºæ»¡åˆ†
        depth_score = min(1.0, avg_length / 200)     # 200å­—ç¬¦ä¸ºæ»¡åˆ†

        overall_score = (breadth_score + depth_score) / 2

        return {
            "score": overall_score,
            "breadth": breadth_score,
            "depth": depth_score,
            "count": findings_count,
            "avg_length": avg_length
        }

    def analyze_coverage(self, state):
        """åˆ†æè¦†ç›–åº¦"""
        research_brief = state.get("research_brief", "")
        notes = state.get("notes", [])

        if not research_brief:
            return {"score": 0.0}

        # ç®€åŒ–çš„å…³é”®è¯è¦†ç›–åº¦åˆ†æ
        brief_keywords = set(research_brief.lower().split())
        notes_text = " ".join(notes).lower()

        covered_keywords = sum(1 for keyword in brief_keywords if keyword in notes_text)

        coverage_score = covered_keywords / len(brief_keywords) if brief_keywords else 0.0

        return {
            "score": coverage_score,
            "covered_keywords": covered_keywords,
            "total_keywords": len(brief_keywords)
        }

    def check_resource_status(self, state, config):
        """æ£€æŸ¥èµ„æºçŠ¶æ€"""
        iterations = state.get("research_iterations", 0)
        used_units = state.get("used_research_units", 0)

        # å¤„ç†configå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
        if isinstance(config, dict):
            max_iterations = config.get("max_researcher_iterations", 5)
            max_units = config.get("max_concurrent_research_units", 3)
        else:
            max_iterations = config.max_researcher_iterations
            max_units = config.max_concurrent_research_units

        return {
            "iterations_remaining": max_iterations - iterations,
            "units_remaining": max_units - used_units,
            "iterations_used": iterations,
            "units_used": used_units,
            "progress_ratio": iterations / max_iterations
        }

    def make_intelligent_decision(self, quality, coverage, resources):
        """å¤šå› å­æ™ºèƒ½å†³ç­–"""

        # åŸºç¡€è¯„åˆ†
        quality_score = quality["score"]
        coverage_score = coverage["score"]
        progress_ratio = resources["progress_ratio"]

        # å®Œæˆå€¾å‘è¯„åˆ†
        completion_score = (quality_score + coverage_score) / 2

        # ç ”ç©¶å€¾å‘è¯„åˆ†
        research_score = (1 - completion_score) * (1 - progress_ratio * 0.5)

        # å†³ç­–é€»è¾‘
        if completion_score > 0.75 or progress_ratio > 0.8:
            return "complete"
        elif research_score > 0.4 and resources["iterations_remaining"] > 0:
            return "research"
        else:
            return "complete"  # é»˜è®¤å€¾å‘å®Œæˆ

    async def generate_research_topics(self, state, action, config):
        """ä½¿ç”¨AIæ¨¡å‹æ™ºèƒ½ç”Ÿæˆå¤šä¸ªäº’è¡¥çš„ç ”ç©¶ä¸»é¢˜"""
        if action != "research":
            return []

        research_brief = state.get("research_brief", "")
        notes = state.get("notes", [])
        
        # è·å–é…ç½®
        configurable = Configuration.from_runnable_config(config)
        target_count = min(configurable.max_concurrent_research_units, 5)
        
        # æ„å»ºæç¤ºè¯ - ä¼ é€’æ‰€æœ‰å·²æœ‰å‘ç°ï¼Œé¿å…é‡å¤ç ”ç©¶
        if notes:
            # ä¼ é€’æ‰€æœ‰notesï¼Œä½†é™åˆ¶æ€»é•¿åº¦é¿å…tokenè¶…é™
            all_notes = "\n\n".join(notes)
            if len(all_notes) > 3000:  # å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­ä½†ä¿ç•™å®Œæ•´æ¡ç›®
                # å°½å¯èƒ½å¤šåœ°åŒ…å«å®Œæ•´çš„notes
                existing_notes_text = "\n\n".join(notes[:10]) + "\n\n...(è¿˜æœ‰æ›´å¤šå‘ç°)"
            else:
                existing_notes_text = all_notes
        else:
            existing_notes_text = "æš‚æ— "
        
        prompt_content = generate_research_topics_prompt.format(
            research_brief=research_brief,
            existing_notes=existing_notes_text,
            target_count=target_count
        )
        
        # é…ç½®AIæ¨¡å‹
        model_config = {
            "model": configurable.research_model,
            "max_tokens": 2000,
            "api_key": get_api_key_for_model(configurable.research_model, config),
            "tags": ["langsmith:nostream"]
        }
        
        # è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆä¸»é¢˜
        try:
            from .qwen_model import init_qwen_model
            
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            topic_model = (
                init_qwen_model(
                    model=configurable.research_model,
                    max_tokens=2000
                )
                .with_structured_output(ResearchTopicsResponse)
                .with_config(model_config)
            )
            
            response = await topic_model.ainvoke([HumanMessage(content=prompt_content)])
            
            research_topics = response.research_topics
            
            logger.info(f"[RESEARCH_TOPICS] AIåˆ†æ: {response.analysis}")
            logger.info(f"[RESEARCH_TOPICS] ç”Ÿæˆ {len(research_topics)} ä¸ªç ”ç©¶ä¸»é¢˜")
            for i, topic in enumerate(research_topics, 1):
                logger.info(f"  ä¸»é¢˜{i}: {topic[:100]}...")
            logger.info(f"[RESEARCH_TOPICS] æ¨ç†: {response.reasoning}")
            
            return research_topics[:target_count]
        
        except Exception as e:
            logger.error(f"[RESEARCH_TOPICS] AIç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§ç­–ç•¥ï¼šè¿”å›åŸºç¡€ä¸»é¢˜
            return [research_brief]

    def determine_strategy(self, state, action):
        """ç¡®å®šç ”ç©¶ç­–ç•¥"""
        if action == "research":
            resources = self.check_resource_status(state, Configuration())
            if resources["iterations_remaining"] > 3:
                return "exploratory"
            else:
                return "focused"
        elif action == "complete":
            return "comprehensive"
        else:
            return "refined"

    def calculate_decision_confidence(self):
        """è®¡ç®—å†³ç­–ç½®ä¿¡åº¦"""
        # ç®€åŒ–å®ç°
        return 0.8

    def explain_decision(self, quality, coverage):
        """è§£é‡Šå†³ç­–åŸå› """
        return f"è´¨é‡è¯„åˆ†: {quality['score']:.2f}, è¦†ç›–åº¦: {coverage['score']:.2f}"

# åˆå§‹åŒ–å¯é…ç½®æ¨¡å‹
# æ¨¡å‹åç§°ä»ç¯å¢ƒå˜é‡è¯»å–
import logging
logger = logging.getLogger(__name__)

configurable_model = init_qwen_model(
    model=None,  # ä»ç¯å¢ƒå˜é‡LLM_MODELè¯»å–  
    max_tokens=4000
)

logger.info(f"ğŸ¤– ç›‘ç£è€…æ¨¡å‹åˆå§‹åŒ–: model={configurable_model.model_name}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒèŠ‚ç‚¹ï¼šsupervisor_plannerï¼ˆåˆå¹¶åŸsupervisorå’Œdecision_executorï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@trace_node("supervisor_planner", ["supervisor", "planning", "decision"])
async def supervisor_planner(state: SupervisorState, config: RunnableConfig) -> dict:
    """ç»Ÿä¸€çš„ç›‘ç£è€…è§„åˆ’èŠ‚ç‚¹ - æ•´åˆè¿›åº¦è®¡ç®—ã€LLMå†³ç­–å’Œæ™ºèƒ½æ§åˆ¶
    
    æ­¤èŠ‚ç‚¹åˆå¹¶äº†åŸæ¥çš„supervisorå’Œdecision_executoråŠŸèƒ½ï¼š
    1. è®¡ç®—å½“å‰è¿›åº¦ï¼ˆè¿­ä»£æ¬¡æ•°ã€ç ”ç©¶å•å…ƒä½¿ç”¨ï¼‰
    2. æ„å»ºå¸¦è¿›åº¦ä¿¡æ¯çš„ç³»ç»Ÿæç¤ºè¯
    3. æ‰§è¡Œæ™ºèƒ½å†³ç­–åˆ†æ
    4. è¿”å›å†³ç­–ç»“æœï¼ˆç ”ç©¶ä¸»é¢˜åˆ—è¡¨ æˆ– å®Œæˆä¿¡å·ï¼‰
    
    Args:
        state: å½“å‰ç›‘ç£è€…çŠ¶æ€
        config: è¿è¡Œæ—¶é…ç½®
        
    Returns:
        åŒ…å«å†³ç­–ç»“æœå’Œæ›´æ–°çŠ¶æ€çš„å­—å…¸
    """
    configurable = Configuration.from_runnable_config(config)
    research_iterations = state.get("research_iterations", 0)
    used_research_units = state.get("used_research_units", 0)

    # æ­¥éª¤1ï¼šè®¡ç®—è¿›åº¦å‚æ•°
    current_iteration = research_iterations + 1
    remaining_iterations = configurable.max_researcher_iterations - research_iterations
    remaining_units = configurable.max_concurrent_research_units - used_research_units

    logger.info(f"[SUPERVISOR_PLANNER] ğŸ¯ ç¬¬ {current_iteration} è½®è§„åˆ’å¼€å§‹")
    logger.info(f"[SUPERVISOR_PLANNER] ğŸ“Š è¿›åº¦: {current_iteration}/{configurable.max_researcher_iterations} è¿­ä»£, {used_research_units}/{configurable.max_concurrent_research_units} ç ”ç©¶å•å…ƒ")

    # æ­¥éª¤2ï¼šåˆå§‹åŒ–æ§åˆ¶å™¨
    quality_controller = ResearchQualityController()
    exit_controller = SmartExitController()
    progressive_strategy = ProgressiveCompletionStrategy()
    state_analyzer = ResearchStateAnalyzer()

    current_findings = state.get("notes", [])

    # æ­¥éª¤3ï¼šå¼ºåˆ¶é€€å‡ºæ£€æŸ¥
    force_exit, exit_reason = exit_controller.should_force_complete(state, config)
    if force_exit:
        logger.info(f"[SUPERVISOR_PLANNER] ğŸ›‘ å¼ºåˆ¶å®Œæˆ: {exit_reason}")
        return {
            "decision": {
                "reflection": f"å¼ºåˆ¶å®Œæˆç ”ç©¶: {exit_reason}",
                "should_conduct_research": False,
                "research_topics": [],
                "is_complete": True,
                "reasoning": exit_reason
            },
            "last_action": "complete",
            "completion_reason": f"å¼ºåˆ¶å®Œæˆ: {exit_reason}",
            "exit_type": "forced"
        }

    # æ­¥éª¤4ï¼šè´¨é‡æ§åˆ¶æ£€æŸ¥
    if current_findings:
        previous_findings = state.get("previous_notes", [])
        should_continue, quality_reason = quality_controller.should_continue_research(
            current_findings, previous_findings, research_iterations
        )

        if not should_continue:
            logger.info(f"[SUPERVISOR_PLANNER] ğŸ›‘ è´¨é‡æ§åˆ¶é˜»æ­¢ç»§ç»­ç ”ç©¶: {quality_reason}")
            return {
                "decision": {
                    "reflection": f"è´¨é‡æ§åˆ¶é˜»æ­¢ç»§ç»­ç ”ç©¶: {quality_reason}",
                    "should_conduct_research": False,
                    "research_topics": [],
                    "is_complete": True,
                    "reasoning": quality_reason
                },
                "last_action": "complete",
                "completion_reason": f"è´¨é‡æ§åˆ¶: {quality_reason}",
                "exit_type": "quality_control"
            }

    # æ­¥éª¤5ï¼šæ™ºèƒ½é€€å‡ºè¯„ä¼°
    exit_recommendation, exit_reason = exit_controller.evaluate_exit_conditions(state, config)

    completion_threshold = progressive_strategy.get_completion_threshold(
        research_iterations, configurable.max_researcher_iterations
    )

    # æ­¥éª¤6ï¼šçŠ¶æ€åˆ†æå’Œå†³ç­–
    logger.info(f"[SUPERVISOR_PLANNER] ğŸ¤” å¼€å§‹æ·±åº¦åˆ†æç ”ç©¶çŠ¶æ€...")
    analysis = await state_analyzer.analyze_research_state(state, config)
    logger.info(f"[SUPERVISOR_PLANNER] ğŸ“ˆ åˆ†æç»“æœ: è¡ŒåŠ¨={analysis['action']}, è´¨é‡={analysis['quality_metrics']['score']:.2f}, è¦†ç›–={analysis['coverage_analysis']['score']:.2f}")

    # æ­¥éª¤7ï¼šåŸºäºé€€å‡ºå»ºè®®è°ƒæ•´å†³ç­–
    if exit_recommendation in ["strong_complete", "consider_complete"]:
        completion_score = (analysis["quality_metrics"]["score"] + analysis["coverage_analysis"]["score"]) / 2
        if completion_score >= completion_threshold:
            analysis["action"] = "complete"
            analysis["reasoning"] += f" | é€€å‡ºå»ºè®®: {exit_reason}"
            logger.info(f"[SUPERVISOR_PLANNER] âœ… æ™ºèƒ½é€€å‡ºè¯„ä¼°å»ºè®®å®Œæˆ: {exit_reason}")

    # æ­¥éª¤8ï¼šæ‰§è¡Œæœ€ç»ˆå†³ç­–
    if analysis["action"] == "research":
        # æ‰§è¡Œç ”ç©¶
        research_topics = analysis["research_topics"]
        if not research_topics:
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆç ”ç©¶ä¸»é¢˜ï¼Œå¼ºåˆ¶å®Œæˆ
            logger.warning("[SUPERVISOR_PLANNER] âš ï¸ æœªç”Ÿæˆç ”ç©¶ä¸»é¢˜ï¼Œå¼ºåˆ¶å®Œæˆ")
            return {
                "decision": {
                    "reflection": "æ— æ³•ç”Ÿæˆæœ‰æ•ˆç ”ç©¶ä¸»é¢˜ï¼Œå¼ºåˆ¶å®Œæˆ",
                    "should_conduct_research": False,
                    "research_topics": [],
                    "is_complete": True,
                    "reasoning": "æ— æ³•ç”Ÿæˆæœ‰æ•ˆç ”ç©¶ä¸»é¢˜"
                },
                "last_action": "complete",
                "completion_reason": "æ— æ³•ç”Ÿæˆç ”ç©¶ä¸»é¢˜",
                "exit_type": "no_topics"
            }

        # é™åˆ¶ç ”ç©¶ä¸»é¢˜æ•°é‡
        research_topics = research_topics[:configurable.max_concurrent_research_units]

        logger.info(f"[SUPERVISOR_PLANNER] ğŸ” å†³å®šæ‰§è¡Œç ”ç©¶: {len(research_topics)} ä¸ªä¸»é¢˜")
        for i, topic in enumerate(research_topics, 1):
            logger.info(f"  ä¸»é¢˜{i}: {topic[:80]}...")

        # è¿”å›å†³ç­–ï¼Œè®©è·¯ç”±ç³»ç»Ÿè½¬åˆ° conduct_research
        return {
            "decision": {
                "reflection": f"å‡†å¤‡æ‰§è¡Œç ”ç©¶: {', '.join(t[:30] for t in research_topics[:2])}{'...' if len(research_topics) > 2 else ''}",
                "should_conduct_research": True,
                "research_topics": research_topics,
                "is_complete": False,
                "reasoning": analysis["reasoning"]
            },
            "last_action": "research",
            "exit_recommendation": exit_recommendation
        }

    elif analysis["action"] == "complete":
        logger.info(f"[SUPERVISOR_PLANNER] âœ… å†³å®šå®Œæˆç ”ç©¶: {analysis['reasoning']}")
        return {
            "decision": {
                "reflection": f"å†³å®šå®Œæˆç ”ç©¶: {analysis['reasoning']}",
                "should_conduct_research": False,
                "research_topics": [],
                "is_complete": True,
                "reasoning": analysis["reasoning"]
            },
            "last_action": "complete",
            "completion_reason": analysis["reasoning"],
            "exit_type": "decision",
            "final_quality_score": (analysis["quality_metrics"]["score"] + analysis["coverage_analysis"]["score"]) / 2
        }

    else:
        # é»˜è®¤å®Œæˆ
        logger.warning(f"[SUPERVISOR_PLANNER] âš ï¸ æœªçŸ¥è¡ŒåŠ¨ç±»å‹ï¼Œé»˜è®¤å®Œæˆ: {analysis['action']}")
        return {
            "decision": {
                "reflection": f"æœªçŸ¥è¡ŒåŠ¨ç±»å‹ï¼Œé»˜è®¤å®Œæˆ: {analysis['action']}",
                "should_conduct_research": False,
                "research_topics": [],
                "is_complete": True,
                "reasoning": f"æœªçŸ¥è¡ŒåŠ¨ç±»å‹: {analysis['action']}"
            },
            "last_action": "complete",
            "completion_reason": f"æœªçŸ¥è¡ŒåŠ¨ç±»å‹: {analysis['action']}",
            "exit_type": "fallback"
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç ”ç©¶æ‰§è¡ŒèŠ‚ç‚¹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@trace_node("conduct_research", ["research", "tools", "search"])
async def conduct_research(state: SupervisorState, config: RunnableConfig) -> dict:
    """æ‰§è¡Œç ”ç©¶èŠ‚ç‚¹ - è°ƒç”¨ researcher_subgraph æ‰§è¡Œå®é™…ç ”ç©¶
    
    æ­¤èŠ‚ç‚¹ä¼šï¼š
    1. ä»å†³ç­–ä¸­æå–ç ”ç©¶ä¸»é¢˜
    2. å¹¶è¡Œè°ƒç”¨å¤šä¸ª researcher_subgraph
    3. æ”¶é›†ç ”ç©¶ç»“æœå¹¶èšåˆ
    4. è¿”å›ç ”ç©¶å‘ç°
    
    Args:
        state: å½“å‰ç›‘ç£è€…çŠ¶æ€
        config: è¿è¡Œæ—¶é…ç½®
        
    Returns:
        åŒ…å«ç ”ç©¶ç»“æœçš„å­—å…¸
    """
    logger.info("[CONDUCT_RESEARCH] ğŸ” å¼€å§‹æ‰§è¡Œç ”ç©¶...")
    
    configurable = Configuration.from_runnable_config(config)
    decision = state.get("decision")
    
    if not decision:
        logger.warning("[CONDUCT_RESEARCH] âš ï¸ æ²¡æœ‰å†³ç­–ä¿¡æ¯ï¼Œè·³è¿‡")
        return {}
    
    research_topics = decision.get("research_topics", [])
    if not research_topics:
        logger.warning("[CONDUCT_RESEARCH] âš ï¸ æ²¡æœ‰ç ”ç©¶ä¸»é¢˜ï¼Œè·³è¿‡")
        return {}
    
    # é™åˆ¶å¹¶å‘ç ”ç©¶å•å…ƒæ•°
    research_topics = research_topics[:configurable.max_concurrent_research_units]
    overflow_topics = decision.get("research_topics", [])[configurable.max_concurrent_research_units:]
    
    if overflow_topics:
        logger.warning(f"[CONDUCT_RESEARCH] âš ï¸ è¶…å‡ºå¹¶å‘é™åˆ¶ï¼Œå¿½ç•¥ {len(overflow_topics)} ä¸ªä¸»é¢˜")
    
    logger.info(f"[CONDUCT_RESEARCH] ğŸ“‹ å¹¶è¡Œæ‰§è¡Œ {len(research_topics)} ä¸ªç ”ç©¶ä»»åŠ¡")
    
    try:
        # å¯¼å…¥ researcher_subgraphï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
        from .odr_researcher import researcher_subgraph
        
        # å¹¶è¡Œæ‰§è¡Œç ”ç©¶ä»»åŠ¡
        research_tasks = [
            researcher_subgraph.ainvoke({
                "researcher_messages": [HumanMessage(content=topic)],
                "research_topic": topic,
                "tool_call_iterations": 0,
                "total_searches": 0
            }, config)
            for topic in research_topics
        ]
        
        tool_results = await asyncio.gather(*research_tasks)
        
        # èšåˆç ”ç©¶ç»“æœ
        all_findings = []
        raw_notes_list = []
        
        for i, (result, topic) in enumerate(zip(tool_results, research_topics), 1):
            compressed = result.get("compressed_research", "Error: No research output")
            all_findings.append(f"### Research {i}: {topic[:50]}...\n\n{compressed}")
            
            raw_notes = result.get("raw_notes", [])
            if raw_notes:
                raw_notes_list.extend(raw_notes)
        
        # åˆ›å»ºæ¶ˆæ¯è®°å½•
        findings_message = AIMessage(content="\n\n".join(all_findings))
        
        logger.info(f"[CONDUCT_RESEARCH] âœ… ç ”ç©¶å®Œæˆï¼Œå…± {len(all_findings)} ä¸ªä¸»é¢˜")

        # æ›´æ–°ç ”ç©¶å•å…ƒè®¡æ•°å™¨
        current_used_units = state.get("used_research_units", 0)
        new_used_units = current_used_units + len(research_topics)

        logger.info(f"[CONDUCT_RESEARCH] ğŸ“Š ç ”ç©¶å•å…ƒæ›´æ–°: {current_used_units} + {len(research_topics)} = {new_used_units}")

        return {
            "supervisor_messages": [findings_message],
            "notes": all_findings,
            "raw_notes": raw_notes_list,
            "used_research_units": new_used_units,
            "last_action": "research_completed",
            "research_iterations": state.get("research_iterations", 0) + 1
        }
        
    except Exception as e:
        logger.error(f"[CONDUCT_RESEARCH] âŒ ç ”ç©¶æ‰§è¡Œå¤±è´¥: {e}")
        error_message = AIMessage(content=f"Research execution failed: {str(e)}")
        return {
            "supervisor_messages": [error_message]
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å®ŒæˆèŠ‚ç‚¹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@trace_node("final_complete", ["supervisor", "completion", "summary"])
async def final_complete(state: SupervisorState, config: RunnableConfig) -> dict:
    """æœ€ç»ˆå®ŒæˆèŠ‚ç‚¹ - æ•´ç†å’Œä¼˜åŒ–æ‰€æœ‰å‘ç°"""

    logger.info("[FINAL_COMPLETE] ğŸ“‹ æ•´ç†æœ€ç»ˆç ”ç©¶å‘ç°...")

    # 1. æ”¶é›†æ‰€æœ‰å‘ç°
    all_notes = state.get("notes", [])

    # 2. è´¨é‡ä¼˜åŒ–å’Œå»é‡
    optimized_findings = []
    seen_content = set()

    for note in all_notes:
        # ç®€å•çš„å»é‡é€»è¾‘
        note_hash = hash(note[:100])  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºå“ˆå¸Œ
        if note_hash not in seen_content:
            optimized_findings.append(note)
            seen_content.add(note_hash)

    # 3. ç”Ÿæˆç ”ç©¶æ€»ç»“
    research_summary = f"ç ”ç©¶æ€»ç»“: å…±æ”¶é›† {len(optimized_findings)} æ¡ç ”ç©¶å‘ç°"

    # 4. è¯„ä¼°å®Œæˆè´¨é‡
    completion_quality = {
        "overall_score": min(1.0, len(optimized_findings) / 5),  # 5ä¸ªå‘ç°ä¸ºæ»¡åˆ†
        "findings_count": len(optimized_findings),
        "avg_quality": sum(len(note) for note in optimized_findings) / len(optimized_findings) if optimized_findings else 0
    }

    logger.info(f"[FINAL_COMPLETE] âœ… ç ”ç©¶å®Œæˆï¼Œæ”¶é›† {len(optimized_findings)} æ¡é«˜è´¨é‡å‘ç°")

    return {
        "notes": optimized_findings,
        "research_summary": research_summary,
        "completion_quality": completion_quality,
        "final_statistics": {
            "total_findings": len(optimized_findings),
            "research_iterations": state.get("research_iterations", 0),
            "research_units_used": state.get("used_research_units", 0),
            "quality_score": completion_quality["overall_score"]
        }
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è·¯ç”±å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def route_after_planner(state: SupervisorState) -> Literal["conduct_research", "final_complete", "supervisor_planner"]:
    """åŸºäº supervisor_planner çš„å†³ç­–ç»“æœè¿›è¡Œè·¯ç”±
    
    æµç¨‹ï¼š
    - å¦‚æœå†³å®šå®Œæˆ â†’ "final_complete"
    - å¦‚æœå†³å®šç ”ç©¶ â†’ "conduct_research"
    - å¦‚æœç ”ç©¶å®Œæˆ â†’ "supervisor_planner"ï¼ˆç»§ç»­ä¸‹ä¸€è½®ï¼‰
    
    Args:
        state: å½“å‰ç›‘ç£è€…çŠ¶æ€
        
    Returns:
        ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°
    """
    last_action = state.get("last_action", "")
    decision = state.get("decision", {})

    # 1. ç ”ç©¶å®Œæˆåï¼Œå›åˆ°plannerå¼€å§‹ä¸‹ä¸€è½®
    if last_action == "research_completed":
        logger.info("[ROUTE] ğŸ”„ ç ”ç©¶å®Œæˆï¼Œå›åˆ°supervisor_plannerå¼€å§‹ä¸‹ä¸€è½®")
        return "supervisor_planner"

    # 2. å†³å®šå®Œæˆç ”ç©¶
    if last_action == "complete" or decision.get("is_complete"):
        logger.info("[ROUTE] âœ… å†³å®šå®Œæˆç ”ç©¶ï¼Œè¿›å…¥final_completeèŠ‚ç‚¹")
        return "final_complete"

    # 3. å†³å®šæ‰§è¡Œç ”ç©¶
    elif last_action == "research" and decision.get("should_conduct_research"):
        logger.info("[ROUTE] ğŸ” è½¬åˆ°conduct_researchèŠ‚ç‚¹")
        return "conduct_research"

    # 4. å¼‚å¸¸æƒ…å†µï¼Œç›´æ¥å®Œæˆ
    else:
        logger.warning(f"[ROUTE] âš ï¸ å¼‚å¸¸çŠ¶æ€ï¼Œç›´æ¥å®Œæˆ: last_action={last_action}")
        return "final_complete"


def route_after_complete(state: SupervisorState) -> Literal["supervisor_planner", "__end__"]:
    """å†³å®š final_complete æ‰§è¡Œåæ˜¯ç»§ç»­è¿˜æ˜¯ç»“æŸ
    
    å¦‚æœ decision.is_complete=Trueï¼Œè¯´æ˜åº”è¯¥ç»“æŸ
    å¦åˆ™ï¼Œç»§ç»­å›åˆ° supervisor_planner è¿›è¡Œä¸‹ä¸€è½®
    """
    decision = state.get("decision", {})
    
    if decision and decision.get("is_complete"):
        logger.info("[ROUTE] âœ… ç ”ç©¶å®Œæˆï¼Œç»“æŸ supervisor å­å›¾")
        return END
    
    logger.info("[ROUTE] ğŸ”„ ç»§ç»­ä¸‹ä¸€è½®è§„åˆ’")
    return "supervisor_planner"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ„å»ºç®€åŒ–çš„ supervisor å­å›¾ï¼ˆå•å±‚æ¶æ„ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

supervisor_builder = StateGraph(SupervisorState, config_schema=Configuration)

# æ·»åŠ ä¸‰ä¸ªä¸»è¦èŠ‚ç‚¹
supervisor_builder.add_node("supervisor_planner", supervisor_planner)
supervisor_builder.add_node("conduct_research", conduct_research)
supervisor_builder.add_node("final_complete", final_complete)

# å®šä¹‰æµç¨‹
supervisor_builder.add_edge(START, "supervisor_planner")

# æ¡ä»¶è¾¹ï¼šsupervisor_planner â†’ æ ¹æ®å†³ç­–è·¯ç”±
supervisor_builder.add_conditional_edges(
    "supervisor_planner",
    route_after_planner,
    {
        "conduct_research": "conduct_research",
        "final_complete": "final_complete",
        "supervisor_planner": "supervisor_planner"
    }
)

# conduct_research å®Œæˆåè·¯ç”±
supervisor_builder.add_conditional_edges(
    "conduct_research",
    route_after_planner,
    {
        "supervisor_planner": "supervisor_planner",
        "final_complete": "final_complete",
        "conduct_research": "conduct_research"
    }
)

# final_complete å®Œæˆåç›´æ¥ç»“æŸ
supervisor_builder.add_edge("final_complete", END)

# ç¼–è¯‘ç®€åŒ–çš„å­å›¾
supervisor_subgraph = supervisor_builder.compile()

logger.info("âœ… Simplified supervisor subgraph initialized (3-node architecture)")
