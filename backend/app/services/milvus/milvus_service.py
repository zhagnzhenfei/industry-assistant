"""
Milvusæ ¸å¿ƒæœåŠ¡ç±»
æä¾›ä¸“ä¸šçš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½
"""

import asyncio
import time
import logging
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import numpy as np
from pymilvus import (
    connections, Collection, utility, FieldSchema, CollectionSchema, DataType,
    SearchResult, SearchFuture
)

from .models import (
    DocumentChunk, SearchResult, SearchRequest, SearchResponse,
    CollectionConfig, IndexType, MetricType,
    DEFAULT_COLLECTION_CONFIGS, DEFAULT_SEARCH_PARAMS,
    PERFORMANCE_BASELINES, ERROR_CODES, LOGGING_CONFIG
)

# é…ç½®æ—¥å¿—
logging.basicConfig(**LOGGING_CONFIG)
logger = logging.getLogger(__name__)


class MilvusService:
    """Milvuså‘é‡å­˜å‚¨æ ¸å¿ƒæœåŠ¡"""

    def __init__(self,
                 host: str = "127.0.0.1",
                 port: str = "19530",
                 user: str = "",
                 password: str = "",
                 db_name: str = "default",
                 consistency_level: str = "Strong"):
        """
        åˆå§‹åŒ–MilvusæœåŠ¡

        Args:
            host: MilvusæœåŠ¡å™¨åœ°å€
            port: MilvusæœåŠ¡å™¨ç«¯å£
            user: ç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
            password: å¯†ç ï¼ˆå¯é€‰ï¼‰
            db_name: æ•°æ®åº“åç§°
            consistency_level: ä¸€è‡´æ€§çº§åˆ«
        """
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.db_name = db_name
        self.consistency_level = consistency_level
        self.collections = {}  # ç¼“å­˜é›†åˆå®ä¾‹
        self._connected = False

    async def connect(self) -> bool:
        """è¿æ¥åˆ°MilvusæœåŠ¡å™¨"""
        try:
            logger.info(f"æ­£åœ¨è¿æ¥åˆ°MilvusæœåŠ¡å™¨: {self.host}:{self.port}")

            # æ„å»ºè¿æ¥å‚æ•°
            connect_params = {
                "alias": "default",
                "host": self.host,
                "port": self.port,
                "user": self.user,
                "password": self.password,
                "db_name": self.db_name,
                "consistency_level": self.consistency_level
            }

            # ç§»é™¤ç©ºå€¼å‚æ•°
            connect_params = {k: v for k, v in connect_params.items() if v}

            # å»ºç«‹è¿æ¥
            connections.connect(**connect_params)

            # éªŒè¯è¿æ¥
            server_version = utility.get_server_version()
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°Milvusï¼Œç‰ˆæœ¬: {server_version}")

            self._connected = True
            return True

        except Exception as e:
            logger.error(f"âŒ è¿æ¥Milvuså¤±è´¥: {e}")
            self._connected = False
            return False

    async def disconnect(self) -> bool:
        """æ–­å¼€ä¸MilvusæœåŠ¡å™¨çš„è¿æ¥"""
        try:
            logger.info("æ­£åœ¨æ–­å¼€ä¸Milvusçš„è¿æ¥")
            connections.disconnect("default")
            self._connected = False
            self.collections.clear()
            logger.info("âœ… å·²æ–­å¼€ä¸Milvusçš„è¿æ¥")
            return True
        except Exception as e:
            logger.error(f"âŒ æ–­å¼€è¿æ¥å¤±è´¥: {e}")
            return False

    def is_connected(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿æ¥"""
        return self._connected

    async def create_collection(self,
                              collection_name: str,
                              config: Optional[CollectionConfig] = None) -> bool:
        """åˆ›å»ºé›†åˆ"""
        try:
            logger.info(f"æ­£åœ¨åˆ›å»ºé›†åˆ: {collection_name}")

            # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–è‡ªå®šä¹‰é…ç½®
            if config is None:
                config = DEFAULT_COLLECTION_CONFIGS.get("documents")
                if config is None:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é»˜è®¤é…ç½®ï¼Œåˆ›å»ºåŸºç¡€é…ç½®
                    config = CollectionConfig(collection_name=collection_name)

            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if utility.has_collection(collection_name):
                logger.warning(f"é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œå°†å…ˆåˆ é™¤")
                utility.drop_collection(collection_name)

            # åˆ›å»ºå­—æ®µschema
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=config.vector_dim),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=config.max_length),
                FieldSchema(name="content_ltks", dtype=DataType.VARCHAR, max_length=config.max_length),
                FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="doc_name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="kb_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=200),
                FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="timestamp", dtype=DataType.INT64),
                FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=200),
                FieldSchema(name="keywords", dtype=DataType.VARCHAR, max_length=1000),
                FieldSchema(name="metadata", dtype=DataType.JSON)
            ]

            # åˆ›å»ºé›†åˆschema
            schema = CollectionSchema(
                fields=fields,
                description=config.description,
                enable_dynamic_field=config.enable_dynamic_field
            )

            # åˆ›å»ºé›†åˆ
            collection = Collection(name=collection_name, schema=schema)

            # ç¼“å­˜é›†åˆå®ä¾‹
            self.collections[collection_name] = collection

            logger.info(f"âœ… æˆåŠŸåˆ›å»ºé›†åˆ: {collection_name}")
            logger.info(f"ğŸ“‹ é›†åˆæè¿°: {config.description}")
            logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {config.vector_dim}")
            logger.info(f"ğŸ“Š æ˜¯å¦æ”¯æŒåŠ¨æ€å­—æ®µ: {config.enable_dynamic_field}")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False

    async def create_index(self,
                         collection_name: str,
                         field_name: str = "vector",
                         index_params: Optional[Dict[str, Any]] = None) -> bool:
        """åˆ›å»ºç´¢å¼•"""
        try:
            logger.info(f"æ­£åœ¨åˆ›å»ºç´¢å¼•: {collection_name}.{field_name}")

            # è·å–é›†åˆ
            collection = self._get_collection(collection_name)
            if not collection:
                return False

            # é»˜è®¤ç´¢å¼•å‚æ•°
            if index_params is None:
                if field_name == "vector":
                    index_params = {
                        "index_type": "HNSW",
                        "metric_type": "COSINE",
                        "params": {"M": 16, "efConstruction": 200}
                    }
                else:
                    # éå‘é‡å­—æ®µä½¿ç”¨æ’åºç´¢å¼•
                    index_params = {
                        "index_type": "STL_SORT",
                        "metric_type": "L2"
                    }

            # åˆ›å»ºç´¢å¼•
            start_time = time.time()
            collection.create_index(field_name, index_params)
            build_time = time.time() - start_time

            logger.info(f"âœ… æˆåŠŸåˆ›å»ºç´¢å¼•: {collection_name}.{field_name}")
            logger.info(f"ğŸ”§ ç´¢å¼•ç±»å‹: {index_params.get('index_type', 'unknown')}")
            logger.info(f"ğŸ“ ç›¸ä¼¼åº¦åº¦é‡: {index_params.get('metric_type', 'unknown')}")
            logger.info(f"â±ï¸  æ„å»ºè€—æ—¶: {build_time:.2f}ç§’")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            return False

    async def insert_data(self,
                         collection_name: str,
                         data: List[DocumentChunk],
                         batch_size: int = 1000) -> bool:
        """æ’å…¥æ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨æ’å…¥æ•°æ®åˆ°é›†åˆ: {collection_name} (å…±{len(data)}æ¡)")

            # è·å–é›†åˆ
            collection = self._get_collection(collection_name)
            if not collection:
                return False

            # å‡†å¤‡æ•°æ®
            total_records = len(data)
            start_time = time.time()
            success_count = 0
            failed_count = 0

            # æ‰¹é‡æ’å…¥
            for i in range(0, total_records, batch_size):
                batch_end = min(i + batch_size, total_records)
                batch_data = data[i:batch_end]

                try:
                    # å‡†å¤‡å®ä½“æ•°æ®ï¼ˆæ³¨æ„ï¼šauto_id=Trueçš„å­—æ®µä¸éœ€è¦åœ¨entitiesä¸­æä¾›ï¼‰
                    entities = [
                        [chunk.vector for chunk in batch_data],
                        [chunk.content for chunk in batch_data],
                        [chunk.content_ltks for chunk in batch_data],
                        [chunk.doc_id for chunk in batch_data],
                        [chunk.doc_name for chunk in batch_data],
                        [chunk.kb_id for chunk in batch_data],
                        [chunk.chunk_id for chunk in batch_data],
                        [chunk.category for chunk in batch_data],
                        [chunk.timestamp for chunk in batch_data],
                        [chunk.source for chunk in batch_data],
                        [chunk.keywords for chunk in batch_data],
                        [chunk.metadata for chunk in batch_data]
                    ]

                    # æ’å…¥æ•°æ®
                    collection.insert(entities)
                    success_count += len(batch_data)

                    if (i + batch_size) % 5000 == 0 or batch_end == total_records:
                        logger.info(f"  å·²æ’å…¥ {batch_end}/{total_records} æ¡")

                except Exception as e:
                    logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ (æ‰¹æ¬¡ {i}-{batch_end}): {e}")
                    failed_count += len(batch_data)
                    # å¯ä»¥ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥

            # ä¸æ‰§è¡Œflushæ“ä½œï¼Œé¿å…channelé€šä¿¡é”™è¯¯
            # Milvusä¼šè‡ªåŠ¨åœ¨åå°å¤„ç†æ•°æ®æŒä¹…åŒ–
            # collection.flush()

            total_time = time.time() - start_time
            qps = success_count / total_time if total_time > 0 else 0

            logger.info(f"âœ… æ•°æ®æ’å…¥å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records}")
            logger.info(f"âœ… æˆåŠŸ: {success_count}")
            logger.info(f"âŒ å¤±è´¥: {failed_count}")
            logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"ğŸš€ QPS: {qps:.0f}")
            logger.info("ğŸ’¡ æ•°æ®å·²æ’å…¥æˆåŠŸï¼ŒMilvuså°†åœ¨åå°è‡ªåŠ¨æŒä¹…åŒ–")

            return failed_count == 0  # åªæœ‰åœ¨æ²¡æœ‰å¤±è´¥æ—¶æ‰è¿”å›True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ’å…¥å¤±è´¥: {e}")
            return False

    async def search(self,
                    collection_name: str,
                    query_vector: List[float],
                    top_k: int = 10,
                    filter_expr: Optional[str] = None,
                    search_params: Optional[Dict[str, Any]] = None,
                    output_fields: Optional[List[str]] = None) -> List[SearchResult]:
        """å‘é‡æœç´¢"""
        try:
            logger.info(f"æ­£åœ¨æœç´¢é›†åˆ: {collection_name} (Top-K: {top_k})")

            # è·å–é›†åˆ
            collection = self._get_collection(collection_name)
            if not collection:
                return []

            # ç¡®ä¿é›†åˆå·²åŠ è½½ï¼ˆæ— è®ºé›†åˆæ˜¯å¦ä¸ºç©ºï¼Œæœç´¢å‰éƒ½éœ€è¦åŠ è½½é›†åˆåˆ°å†…å­˜ï¼‰
            try:
                # æ£€æŸ¥é›†åˆåŠ è½½çŠ¶æ€
                from pymilvus import utility
                load_state = utility.load_state(collection_name)
                
                # å¤„ç†æšä¸¾å’Œå­—ç¬¦ä¸²ä¸¤ç§æ ¼å¼
                state_name = load_state.name if hasattr(load_state, 'name') else str(load_state)
                
                # LoadState.Loaded è¡¨ç¤ºå·²åŠ è½½ï¼ŒLoadState.Loading è¡¨ç¤ºæ­£åœ¨åŠ è½½
                # LoadState.NotLoad è¡¨ç¤ºæœªåŠ è½½ï¼ŒLoadState.NotExist è¡¨ç¤ºä¸å­˜åœ¨
                if state_name not in ['Loaded', 'Loading']:
                    logger.info(f"é›†åˆ {collection_name} æœªåŠ è½½ï¼Œæ­£åœ¨åŠ è½½åˆ°å†…å­˜...")
                    collection.load()
                    
                    # ç­‰å¾…åŠ è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
                    max_wait = 5
                    wait_time = 0
                    while wait_time < max_wait:
                        current_state = utility.load_state(collection_name)
                        current_state_name = current_state.name if hasattr(current_state, 'name') else str(current_state)
                        if current_state_name == 'Loaded':
                            logger.info(f"âœ… é›†åˆ {collection_name} åŠ è½½å®Œæˆ")
                            break
                        time.sleep(0.2)
                        wait_time += 0.2
                    
                    if wait_time >= max_wait:
                        logger.warning(f"âš ï¸ é›†åˆ {collection_name} åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å°è¯•æœç´¢")
                else:
                    logger.debug(f"é›†åˆ {collection_name} å·²åŠ è½½ï¼ŒçŠ¶æ€: {state_name}")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥åŠ è½½çŠ¶æ€å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½: {e}")
                try:
                    collection.load()
                    # ç®€çŸ­ç­‰å¾…ç¡®ä¿åŠ è½½å¼€å§‹
                    time.sleep(0.5)
                    logger.info(f"âœ… é›†åˆ {collection_name} å·²è§¦å‘åŠ è½½")
                except Exception as load_error:
                    logger.error(f"âŒ æ— æ³•åŠ è½½é›†åˆ {collection_name}: {load_error}")
                    raise Exception(f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥: {load_error}")

            # é»˜è®¤æœç´¢å‚æ•°
            if search_params is None:
                search_params = {
                    "metric_type": "COSINE",
                    "params": {"ef": 64}
                }

            # é»˜è®¤è¾“å‡ºå­—æ®µ
            if output_fields is None:
                output_fields = ["content", "doc_id", "doc_name", "category", "confidence", "source", "metadata", "chunk_id"]

            # æ‰§è¡Œæœç´¢
            start_time = time.time()

            results = collection.search(
                data=[query_vector],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                expr=filter_expr,
                output_fields=output_fields,
                consistency_level="Strong"
            )

            search_time = time.time() - start_time

            # è½¬æ¢ç»“æœæ ¼å¼
            search_results = []

            if results and len(results) > 0:
                for hit in results[0]:
                    # ä¼˜å…ˆä½¿ç”¨chunk_idä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨Milvuså†…éƒ¨ID
                    chunk_id = hit.entity.get("chunk_id", "")
                    unique_id = chunk_id if chunk_id else str(hit.id)

                    result = SearchResult(
                        id=unique_id,
                        score=hit.score,
                        content=hit.entity.get("content", ""),
                        doc_id=hit.entity.get("doc_id", ""),
                        doc_name=hit.entity.get("doc_name", ""),
                        category=hit.entity.get("category", ""),
                        source=hit.entity.get("source", ""),
                        chunk_id=hit.entity.get("chunk_id", ""),
                        metadata=hit.entity.get("metadata", {})
                    )
                    search_results.append(result)

            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè¿”å› {len(search_results)} æ¡ç»“æœ")
            logger.info(f"â±ï¸  æœç´¢è€—æ—¶: {search_time:.3f}ç§’")

            return search_results

        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            # æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›ç©ºåˆ—è¡¨ï¼Œè¿™æ ·ä¸Šå±‚å¯ä»¥æ­£ç¡®å¤„ç†é”™è¯¯
            raise Exception(f"Milvusæœç´¢å¤±è´¥: {str(e)}")

    async def query(self,
                   collection_name: str,
                   filter_expr: str,
                   output_fields: Optional[List[str]] = None,
                   limit: int = 100) -> List[Dict[str, Any]]:
        """æ¡ä»¶æŸ¥è¯¢"""
        try:
            logger.info(f"æ­£åœ¨æŸ¥è¯¢é›†åˆ: {collection_name} (è¿‡æ»¤: {filter_expr})")

            # è·å–é›†åˆ
            collection = self._get_collection(collection_name)
            if not collection:
                return []

            # ç¡®ä¿é›†åˆå·²åŠ è½½ï¼ˆåªåœ¨æœªåŠ è½½æ—¶æ‰åŠ è½½ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
            if not collection.is_empty:
                try:
                    # æ£€æŸ¥é›†åˆåŠ è½½çŠ¶æ€
                    load_state = utility.load_state(collection_name)
                    if load_state.name not in ['Loaded', 'Loading']:
                        logger.info(f"é›†åˆ {collection_name} æœªåŠ è½½ï¼Œæ­£åœ¨åŠ è½½åˆ°å†…å­˜...")
                        collection.load()
                        logger.info(f"âœ… é›†åˆ {collection_name} åŠ è½½å®Œæˆ")
                    else:
                        logger.debug(f"é›†åˆ {collection_name} å·²åŠ è½½ï¼ŒçŠ¶æ€: {load_state.name}")
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥åŠ è½½çŠ¶æ€å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½: {e}")
                    collection.load()

            # é»˜è®¤è¾“å‡ºå­—æ®µ
            if output_fields is None:
                output_fields = ["id", "content", "doc_id", "doc_name", "category", "confidence", "timestamp"]

            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = time.time()

            results = collection.query(
                expr=filter_expr,
                output_fields=output_fields,
                limit=limit
            )

            query_time = time.time() - start_time

            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
            logger.info(f"â±ï¸  æŸ¥è¯¢è€—æ—¶: {query_time:.3f}ç§’")

            return results

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return []

    async def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection = self._get_collection(collection_name)
            if not collection:
                return {}

            stats = {
                "collection_name": collection_name,
                "num_entities": collection.num_entities,
                "is_empty": collection.is_empty,
                "schema": {
                    "fields": [field.name for field in collection.schema.fields],
                    "description": collection.schema.description,
                    "enable_dynamic_field": collection.schema.enable_dynamic_field
                }
            }

            return stats

        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    async def delete_collection(self, collection_name: str) -> bool:
        """åˆ é™¤é›†åˆ"""
        try:
            logger.info(f"æ­£åœ¨åˆ é™¤é›†åˆ: {collection_name}")

            if utility.has_collection(collection_name):
                utility.drop_collection(collection_name)

                # ä»ç¼“å­˜ä¸­ç§»é™¤
                if collection_name in self.collections:
                    del self.collections[collection_name]

                logger.info(f"âœ… æˆåŠŸåˆ é™¤é›†åˆ: {collection_name}")
                return True
            else:
                logger.warning(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
                return True

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
            return False

    def _get_collection(self, collection_name: str) -> Optional[Collection]:
        """è·å–é›†åˆå®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if collection_name in self.collections:
                return self.collections[collection_name]

            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not utility.has_collection(collection_name):
                logger.error(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
                return None

            # åˆ›å»ºé›†åˆå¹¶ç¼“å­˜
            collection = Collection(name=collection_name)
            self.collections[collection_name] = collection

            return collection

        except Exception as e:
            logger.error(f"è·å–é›†åˆå¤±è´¥: {e}")
            return None

    async def load_collection(self, collection_name: str) -> bool:
        """åŠ è½½é›†åˆåˆ°å†…å­˜"""
        try:
            collection = self._get_collection(collection_name)
            if not collection:
                return False

            if collection.is_empty:
                logger.warning(f"é›†åˆ {collection_name} ä¸ºç©ºï¼Œæ— éœ€åŠ è½½")
                return True

            collection.load()
            logger.info(f"âœ… æˆåŠŸåŠ è½½é›†åˆ: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ åŠ è½½é›†åˆå¤±è´¥: {e}")
            return False

    async def release_collection(self, collection_name: str) -> bool:
        """é‡Šæ”¾é›†åˆå†…å­˜"""
        try:
            collection = self._get_collection(collection_name)
            if not collection:
                return False

            collection.release()
            logger.info(f"âœ… æˆåŠŸé‡Šæ”¾é›†åˆ: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ é‡Šæ”¾é›†åˆå¤±è´¥: {e}")
            return False

    async def get_server_version(self) -> str:
        """è·å–æœåŠ¡å™¨ç‰ˆæœ¬"""
        try:
            return utility.get_server_version()
        except Exception as e:
            logger.error(f"è·å–æœåŠ¡å™¨ç‰ˆæœ¬å¤±è´¥: {e}")
            return "unknown"

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¿æ¥çŠ¶æ€
            if not self.is_connected():
                return {"status": "unhealthy", "error": "Not connected"}

            # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
            server_version = await self.get_server_version()

            # æ£€æŸ¥é›†åˆçŠ¶æ€
            collection_stats = {}
            for collection_name in self.collections.keys():
                stats = await self.get_collection_stats(collection_name)
                collection_stats[collection_name] = stats

            return {
                "status": "healthy",
                "server_version": server_version,
                "connected": True,
                "collections": collection_stats,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # ===== åŒæ­¥æ–¹æ³•åŒ…è£…å™¨ =====
    def create_collection_sync(self, collection_name: str, config: Optional[CollectionConfig] = None) -> bool:
        """åŒæ­¥åˆ›å»ºé›†åˆ - å®Œå…¨åŒæ­¥å®ç°ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥åˆ›å»º/æ£€æŸ¥é›†åˆ: {collection_name}")

            # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–è‡ªå®šä¹‰é…ç½®
            if config is None:
                config = DEFAULT_COLLECTION_CONFIGS.get("documents")
                if config is None:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é»˜è®¤é…ç½®ï¼Œåˆ›å»ºåŸºç¡€é…ç½®
                    config = CollectionConfig(collection_name=collection_name)
                    logger.info(f"ğŸ“ åˆ›å»ºäº†æ–°çš„é…ç½®: {collection_name}")
                else:
                    logger.info(f"ğŸ“ ä½¿ç”¨é»˜è®¤é…ç½®: {collection_name}")

            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œéœ€è¦æ£€æŸ¥schemaå¹¶å¯èƒ½é‡å»ºï¼ˆå› ä¸ºæˆ‘ä»¬è¦ç§»é™¤confidenceå­—æ®µï¼‰
            if utility.has_collection(collection_name):
                logger.warning(f"é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œæ£€æŸ¥schema...")
                collection = Collection(collection_name)
                field_names = [field.name for field in collection.schema.fields]
                logger.info(f"ğŸ” ç°æœ‰é›†åˆå­—æ®µ: {field_names}")
                logger.info(f"ğŸ” ç°æœ‰é›†åˆå­—æ®µæ•°é‡: {len(field_names)}")

                # å¼ºåˆ¶åˆ é™¤ä»»ä½•åŒ…å«confidenceå­—æ®µçš„é›†åˆ
                if "confidence" in field_names:
                    logger.warning(f"ğŸ—‘ï¸ é›†åˆ {collection_name} åŒ…å«å·²åºŸå¼ƒçš„confidenceå­—æ®µï¼Œå°†å¼ºåˆ¶åˆ é™¤é‡å»º")
                    try:
                        utility.drop_collection(collection_name)
                        logger.info(f"âœ… æˆåŠŸåˆ é™¤æ—§é›†åˆ: {collection_name}")
                    except Exception as e:
                        logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
                        return False
                else:
                    # å³ä½¿æ²¡æœ‰confidenceå­—æ®µï¼Œä¹Ÿæ£€æŸ¥å­—æ®µæ•°é‡æ˜¯å¦æ­£ç¡®
                    expected_fields = {"id", "vector", "content", "content_ltks", "doc_id", "doc_name", "kb_id", "chunk_id", "category", "timestamp", "source", "keywords", "metadata"}
                    if set(field_names) != expected_fields:
                        logger.warning(f"ğŸ”§ é›†åˆ {collection_name} å­—æ®µä¸åŒ¹é…ï¼Œé¢„æœŸ: {expected_fields}, å®é™…: {set(field_names)}ï¼Œå°†é‡å»º")
                        try:
                            utility.drop_collection(collection_name)
                            logger.info(f"âœ… æˆåŠŸåˆ é™¤ä¸åŒ¹é…çš„é›†åˆ: {collection_name}")
                        except Exception as e:
                            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
                            return False
                    else:
                        logger.info(f"âœ… é›†åˆ {collection_name} schemaæ­£ç¡®ï¼Œè·³è¿‡åˆ›å»º")
                        return True

            # åˆ›å»ºå­—æ®µschema
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=config.vector_dim),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=config.max_length),
                FieldSchema(name="content_ltks", dtype=DataType.VARCHAR, max_length=config.max_length),
                FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="doc_name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="kb_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=200),
                FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="timestamp", dtype=DataType.INT64),
                FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="keywords", dtype=DataType.VARCHAR, max_length=1000),
                FieldSchema(name="metadata", dtype=DataType.JSON)
            ]

            field_names = [field.name for field in fields]
            logger.info(f"ğŸ” æ–°å»ºé›†åˆschemaå­—æ®µ: {field_names}")
            logger.info(f"ğŸ” æ–°å»ºé›†åˆå­—æ®µæ•°é‡: {len(field_names)}")

            # åˆ›å»ºschema
            schema = CollectionSchema(
                fields=fields,
                description=config.description,
                enable_dynamic_field=config.enable_dynamic_field
            )

            # åˆ›å»ºé›†åˆ
            collection = Collection(name=collection_name, schema=schema)

            logger.info(f"âœ… æˆåŠŸåŒæ­¥åˆ›å»ºé›†åˆ: {collection_name}")
            logger.info(f"ğŸ“‹ é›†åˆæè¿°: {config.description}")
            logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {config.vector_dim}")
            logger.info(f"ğŸ“Š æ˜¯å¦æ”¯æŒåŠ¨æ€å­—æ®µ: {config.enable_dynamic_field}")

            return True

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False

    def create_index_sync(self, collection_name: str, field_name: str = "vector", index_params: Optional[Dict] = None) -> bool:
        """åŒæ­¥åˆ›å»ºç´¢å¼• - å®Œå…¨åŒæ­¥å®ç°ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª"""
        try:
            logger.info(f"æ­£åœ¨åŒæ­¥åˆ›å»ºç´¢å¼•: {collection_name}.{field_name}")

            # è·å–é›†åˆ
            collection = Collection(name=collection_name)

            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            if collection.has_index():
                logger.info(f"é›†åˆ {collection_name} çš„ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True

            # é»˜è®¤ç´¢å¼•å‚æ•°
            if index_params is None:
                if field_name == "vector":
                    index_params = {
                        "index_type": "HNSW",
                        "metric_type": "COSINE",
                        "params": {"M": 16, "efConstruction": 200}
                    }
                else:
                    # éå‘é‡å­—æ®µä½¿ç”¨æ’åºç´¢å¼•
                    index_params = {
                        "index_type": "STL_SORT",
                        "metric_type": "L2"
                    }

            # åˆ›å»ºç´¢å¼•
            start_time = time.time()
            collection.create_index(field_name, index_params)
            build_time = time.time() - start_time

            logger.info(f"âœ… æˆåŠŸåŒæ­¥åˆ›å»ºç´¢å¼•: {collection_name}.{field_name}")
            logger.info(f"ğŸ”§ ç´¢å¼•ç±»å‹: {index_params.get('index_type', 'unknown')}")
            logger.info(f"ğŸ“ ç›¸ä¼¼åº¦åº¦é‡: {index_params.get('metric_type', 'unknown')}")
            logger.info(f"â±ï¸  æ„å»ºè€—æ—¶: {build_time:.2f}ç§’")

            return True

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            return False

    def connect_sync(self) -> bool:
        """åŒæ­¥è¿æ¥åˆ°MilvusæœåŠ¡å™¨"""
        try:
            logger.info(f"æ­£åœ¨åŒæ­¥è¿æ¥åˆ°MilvusæœåŠ¡å™¨: {self.host}:{self.port}")

            # æ„å»ºè¿æ¥å‚æ•°
            connect_params = {
                "alias": "default",
                "host": self.host,
                "port": self.port,
                "user": self.user,
                "password": self.password,
                "db_name": self.db_name,
                "consistency_level": self.consistency_level
            }

            # ç§»é™¤ç©ºå€¼å‚æ•°
            connect_params = {k: v for k, v in connect_params.items() if v}

            # å»ºç«‹è¿æ¥ - åŒæ­¥æ–¹å¼
            connections.connect(**connect_params)

            # éªŒè¯è¿æ¥
            server_version = utility.get_server_version()
            logger.info(f"âœ… æˆåŠŸåŒæ­¥è¿æ¥åˆ°Milvusï¼Œç‰ˆæœ¬: {server_version}")

            self._connected = True
            return True

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥è¿æ¥Milvuså¤±è´¥: {e}")
            self._connected = False
            return False

    def insert_data_sync(self, collection_name: str, data: List[DocumentChunk], batch_size: int = 1000) -> bool:
        """åŒæ­¥æ’å…¥æ•°æ®"""
        try:
            # ç›´æ¥åŒæ­¥æ‰§è¡Œæ’å…¥æ“ä½œï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
            if not self._connected:
                logger.error("Milvusæœªè¿æ¥")
                return False

            if not data:
                logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥")
                return True

            # è·å–é›†åˆ
            collection = Collection(name=collection_name)

            # æ³¨æ„ï¼šæ’å…¥æ•°æ®æ—¶ä¸éœ€è¦åŠ è½½é›†åˆï¼Œload()åªç”¨äºæŸ¥è¯¢/æœç´¢æ“ä½œ
            # æ’å…¥æ“ä½œå¯ä»¥ç›´æ¥åœ¨æœªåŠ è½½çš„é›†åˆä¸Šæ‰§è¡Œ

            # æ‰¹é‡æ’å…¥æ•°æ®
            total_inserted = 0
            for i in range(0, len(data), batch_size):
                batch_end = min(i + batch_size, len(data))
                batch_data = data[i:batch_end]

                try:
                    # æ‰“å°é›†åˆschemaä¿¡æ¯
                    schema_fields = [field.name for field in collection.schema.fields]
                    logger.info(f"ğŸ” é›†åˆschemaå­—æ®µ: {schema_fields}")
                    logger.info(f"ğŸ” é›†åˆschemaå­—æ®µæ•°é‡: {len(schema_fields)}")

                    # å‡†å¤‡å®ä½“æ•°æ®ï¼ˆæ³¨æ„ï¼šauto_id=Trueçš„å­—æ®µä¸éœ€è¦åœ¨entitiesä¸­æä¾›ï¼‰
                    entities = [
                        [chunk.vector for chunk in batch_data],
                        [chunk.content for chunk in batch_data],
                        [chunk.content_ltks for chunk in batch_data],
                        [chunk.doc_id for chunk in batch_data],
                        [chunk.doc_name for chunk in batch_data],
                        [chunk.kb_id for chunk in batch_data],
                        [chunk.chunk_id for chunk in batch_data],
                        [chunk.category for chunk in batch_data],
                        [chunk.timestamp for chunk in batch_data],
                        [chunk.source for chunk in batch_data],
                        [chunk.keywords for chunk in batch_data],
                        [chunk.metadata for chunk in batch_data]
                    ]

                    logger.info(f"ğŸ” æ’å…¥æ•°æ®å­—æ®µæ•°é‡: {len(entities)}")
                    logger.info(f"ğŸ” æ’å…¥æ•°æ®å­—æ®µ: ['vector', 'content', 'content_ltks', 'doc_id', 'doc_name', 'kb_id', 'chunk_id', 'category', 'timestamp', 'source', 'keywords', 'metadata']")

                    # æ’å…¥æ•°æ®
                    collection.insert(entities)
                    total_inserted += len(batch_data)

                    logger.info(f"å·²æ’å…¥ {total_inserted}/{len(data)} æ¡è®°å½•åˆ°é›†åˆ {collection_name}")

                except Exception as batch_error:
                    logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ (æ‰¹æ¬¡ {i//batch_size + 1}): {batch_error}")
                    return False

            # ä¸æ‰§è¡Œflushæ“ä½œï¼Œé¿å…channelé€šä¿¡é”™è¯¯
            # Milvusä¼šè‡ªåŠ¨åœ¨åå°å¤„ç†æ•°æ®æŒä¹…åŒ–ï¼ˆé€šå¸¸åœ¨å‡ ç§’åˆ°å‡ åˆ†é’Ÿå†…å®Œæˆï¼‰
            # æ•°æ®åœ¨æ’å…¥åç«‹å³å¯ç”¨äºæŸ¥è¯¢ï¼Œæ— éœ€ç­‰å¾…flushå®Œæˆ
            logger.info(f"âœ… åŒæ­¥æ’å…¥å®Œæˆï¼Œæ€»å…±æ’å…¥ {total_inserted} æ¡è®°å½•åˆ°é›†åˆ {collection_name}")
            logger.info("ğŸ’¡ æ•°æ®å·²æ’å…¥æˆåŠŸï¼ŒMilvuså°†åœ¨åå°è‡ªåŠ¨æŒä¹…åŒ–")
            return True

        except Exception as e:
            logger.error(f"åŒæ­¥æ’å…¥æ•°æ®å¤±è´¥: {e}")
            return False

    def delete_data_sync(self, collection_name: str, filter_expr: str) -> int:
        """åŒæ­¥åˆ é™¤æ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨åŒæ­¥åˆ é™¤é›†åˆ {collection_name} ä¸­åŒ¹é…æ¡ä»¶çš„æ•°æ®: {filter_expr}")

            if not self._connected:
                logger.error("Milvusæœªè¿æ¥")
                return 0

            # è·å–é›†åˆ
            collection = Collection(name=collection_name)

            # æ‰§è¡Œåˆ é™¤æ“ä½œ
            collection.delete(expr=filter_expr)

            # ä¸æ‰§è¡Œflushæ“ä½œï¼Œé¿å…channelé€šä¿¡é”™è¯¯
            # Milvusä¼šè‡ªåŠ¨åœ¨åå°å¤„ç†æ•°æ®æŒä¹…åŒ–
            logger.info("â³ è·³è¿‡flushæ“ä½œï¼Œå…è®¸Milvusåœ¨åå°è‡ªåŠ¨æŒä¹…åŒ–åˆ é™¤çš„æ•°æ®")

            logger.info(f"âœ… åŒæ­¥åˆ é™¤æ•°æ®å®Œæˆï¼Œé›†åˆ: {collection_name}")
            return 1  # è¿”å›åˆ é™¤è®¡æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

        except Exception as e:
            logger.error(f"åŒæ­¥åˆ é™¤æ•°æ®å¤±è´¥: {e}")
            return 0

    def search_sync(self, collection_name: str, query_vector: List[float], top_k: int = 10,
                   filter_expr: Optional[str] = None, search_params: Optional[Dict] = None,
                   output_fields: Optional[List[str]] = None) -> List[SearchResult]:
        """åŒæ­¥æœç´¢"""
        import asyncio
        loop = None
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(self.search(collection_name, query_vector, top_k, filter_expr, search_params, output_fields))
        except Exception as e:
            logger.error(f"åŒæ­¥æœç´¢å¤±è´¥: {e}")
            return []
        finally:
            if loop is not None:
                loop.close()