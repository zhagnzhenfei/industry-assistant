"""
Milvusä¼˜åŒ–æœåŠ¡
æä¾›æ€§èƒ½ä¼˜åŒ–ã€ç´¢å¼•è°ƒä¼˜ã€æŸ¥è¯¢ä¼˜åŒ–ç­‰åŠŸèƒ½
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np
from dataclasses import dataclass

from .milvus_service import MilvusService
from .models import (
    CollectionConfig, IndexType, MetricType,
    PERFORMANCE_BASELINES, PERFORMANCE_BENCHMARKS
)

logger = logging.getLogger(__name__)


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    collection_name: str
    optimization_type: str
    before_metrics: Dict[str, Any]
    after_metrics: Dict[str, Any]
    improvement_ratio: Dict[str, float]
    recommendations: List[str]
    execution_time: float
    timestamp: datetime


@dataclass
class CollectionStats:
    """é›†åˆç»Ÿè®¡ä¿¡æ¯"""
    collection_name: str
    num_entities: int
    avg_doc_size: float
    index_type: str
    search_latency_p99: float
    insert_throughput: float
    memory_usage_mb: float
    disk_usage_mb: float
    last_updated: datetime


class MilvusOptimizationService:
    """Milvusæ€§èƒ½ä¼˜åŒ–æœåŠ¡"""

    def __init__(self, milvus_service: MilvusService):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æœåŠ¡

        Args:
            milvus_service: MilvusæœåŠ¡å®ä¾‹
        """
        self.milvus_service = milvus_service
        self.optimization_history = []
        self.performance_cache = {}

    async def optimize_collection(self, collection_name: str,
                                optimization_level: str = "balanced") -> OptimizationResult:
        """
        ä¼˜åŒ–é›†åˆæ€§èƒ½

        Args:
            collection_name: é›†åˆåç§°
            optimization_level: ä¼˜åŒ–çº§åˆ« (performance/balanced/memory)

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹ä¼˜åŒ–é›†åˆ: {collection_name}, çº§åˆ«: {optimization_level}")
            start_time = time.time()

            # 1. è·å–å½“å‰æ€§èƒ½åŸºçº¿
            before_metrics = await self._get_collection_metrics(collection_name)
            logger.info(f"ä¼˜åŒ–å‰æ€§èƒ½åŸºçº¿: {before_metrics}")

            # 2. åˆ†æé›†åˆç‰¹å¾
            collection_stats = await self._analyze_collection(collection_name)
            logger.info(f"é›†åˆç»Ÿè®¡åˆ†æ: {collection_stats}")

            # 3. ç´¢å¼•ä¼˜åŒ–
            index_result = await self._optimize_index(collection_name, collection_stats, optimization_level)

            # 4. æœç´¢å‚æ•°ä¼˜åŒ–
            search_result = await self._optimize_search_parameters(collection_name, optimization_level)

            # 5. å†…å­˜ä¼˜åŒ–
            memory_result = await self._optimize_memory_usage(collection_name, optimization_level)

            # 6. è·å–ä¼˜åŒ–åæ€§èƒ½
            after_metrics = await self._get_collection_metrics(collection_name)
            logger.info(f"ä¼˜åŒ–åæ€§èƒ½æŒ‡æ ‡: {after_metrics}")

            # 7. è®¡ç®—æ”¹è¿›æ¯”ä¾‹
            improvement_ratio = self._calculate_improvement_ratio(before_metrics, after_metrics)

            # 8. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = self._generate_recommendations(
                collection_stats, before_metrics, after_metrics, optimization_level
            )

            execution_time = time.time() - start_time

            result = OptimizationResult(
                collection_name=collection_name,
                optimization_type=optimization_level,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_ratio=improvement_ratio,
                recommendations=recommendations,
                execution_time=execution_time,
                timestamp=datetime.now()
            )

            # ä¿å­˜ä¼˜åŒ–å†å²
            self.optimization_history.append(result)

            logger.info(f"âœ… é›†åˆä¼˜åŒ–å®Œæˆ - è€—æ—¶: {execution_time:.2f}s")
            logger.info(f"ğŸ“Š æ€§èƒ½æ”¹è¿›: {improvement_ratio}")

            return result

        except Exception as e:
            logger.error(f"âŒ é›†åˆä¼˜åŒ–å¤±è´¥: {e}")
            raise e

    async def _get_collection_metrics(self, collection_name: str) -> Dict[str, Any]:
        """è·å–é›†åˆæ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = await self.milvus_service.get_collection_stats(collection_name)

            # æ€§èƒ½æµ‹è¯•
            search_latency = await self._measure_search_latency(collection_name)
            insert_throughput = await self._measure_insert_throughput(collection_name)

            # ç³»ç»Ÿèµ„æºä½¿ç”¨
            memory_usage = await self._get_memory_usage(collection_name)
            disk_usage = await self._get_disk_usage(collection_name)

            metrics = {
                "num_entities": stats.get("num_entities", 0),
                "search_latency_p99": search_latency,
                "insert_throughput": insert_throughput,
                "memory_usage_mb": memory_usage,
                "disk_usage_mb": disk_usage,
                "index_type": await self._get_current_index_type(collection_name)
            }

            # ç¼“å­˜æ€§èƒ½æ•°æ®
            self.performance_cache[collection_name] = {
                "metrics": metrics,
                "timestamp": datetime.now()
            }

            return metrics

        except Exception as e:
            logger.error(f"è·å–é›†åˆæŒ‡æ ‡å¤±è´¥: {e}")
            return {}

    async def _analyze_collection(self, collection_name: str) -> CollectionStats:
        """åˆ†æé›†åˆç‰¹å¾"""
        try:
            stats = await self.milvus_service.get_collection_stats(collection_name)
            num_entities = stats.get("num_entities", 0)

            # ä¼°ç®—å¹³å‡æ–‡æ¡£å¤§å°
            avg_doc_size = await self._estimate_avg_document_size(collection_name)

            # è·å–å½“å‰ç´¢å¼•ç±»å‹
            index_type = await self._get_current_index_type(collection_name)

            # æ€§èƒ½æŒ‡æ ‡
            search_latency = await self._measure_search_latency(collection_name)
            insert_throughput = await self._measure_insert_throughput(collection_name)

            # èµ„æºä½¿ç”¨
            memory_usage = await self._get_memory_usage(collection_name)
            disk_usage = await self._get_disk_usage(collection_name)

            collection_stats = CollectionStats(
                collection_name=collection_name,
                num_entities=num_entities,
                avg_doc_size=avg_doc_size,
                index_type=index_type,
                search_latency_p99=search_latency,
                insert_throughput=insert_throughput,
                memory_usage_mb=memory_usage,
                disk_usage_mb=disk_usage,
                last_updated=datetime.now()
            )

            logger.info(f"é›†åˆåˆ†æå®Œæˆ: {collection_stats}")
            return collection_stats

        except Exception as e:
            logger.error(f"é›†åˆåˆ†æå¤±è´¥: {e}")
            raise e

    async def _optimize_index(self, collection_name: str, stats: CollectionStats,
                            optimization_level: str) -> Dict[str, Any]:
        """ä¼˜åŒ–ç´¢å¼•"""
        try:
            logger.info(f"å¼€å§‹ç´¢å¼•ä¼˜åŒ– - é›†åˆ: {collection_name}, æ•°æ®é‡: {stats.num_entities}")

            # æ ¹æ®æ•°æ®é‡å’Œä¼˜åŒ–çº§åˆ«é€‰æ‹©ç´¢å¼•ç±»å‹
            new_index_type = self._select_optimal_index_type(stats.num_entities, optimization_level)

            if new_index_type == stats.index_type:
                logger.info(f"å½“å‰ç´¢å¼•ç±»å‹ {stats.index_type} å·²æ˜¯æœ€ä¼˜é€‰æ‹©")
                return {"status": "already_optimal", "index_type": stats.index_type}

            # æ„å»ºç´¢å¼•å‚æ•°
            index_params = self._build_index_params(new_index_type, stats.num_entities)

            logger.info(f"åˆ›å»ºæ–°ç´¢å¼• - ç±»å‹: {new_index_type}, å‚æ•°: {index_params}")

            # åˆ é™¤æ—§ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                collection = self.milvus_service._get_collection(collection_name)
                if collection:
                    collection.drop_index("vector")
            except Exception as e:
                logger.warning(f"åˆ é™¤æ—§ç´¢å¼•å¤±è´¥: {e}")

            # åˆ›å»ºæ–°ç´¢å¼•
            success = await self.milvus_service.create_index(
                collection_name=collection_name,
                field_name="vector",
                index_params=index_params
            )

            if success:
                logger.info(f"âœ… ç´¢å¼•ä¼˜åŒ–å®Œæˆ - æ–°ç´¢å¼•ç±»å‹: {new_index_type}")
                return {"status": "optimized", "index_type": new_index_type, "params": index_params}
            else:
                logger.error("âŒ ç´¢å¼•ä¼˜åŒ–å¤±è´¥")
                return {"status": "failed", "index_type": stats.index_type}

        except Exception as e:
            logger.error(f"ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    def _select_optimal_index_type(self, num_entities: int, optimization_level: str) -> str:
        """é€‰æ‹©æœ€ä¼˜ç´¢å¼•ç±»å‹"""
        if optimization_level == "performance":
            # æ€§èƒ½ä¼˜å…ˆ
            if num_entities > 1000000:
                return IndexType.IVF_PQ.value
            elif num_entities > 100000:
                return IndexType.IVF_FLAT.value
            else:
                return IndexType.HNSW.value

        elif optimization_level == "memory":
            # å†…å­˜ä¼˜å…ˆ
            if num_entities > 100000:
                return IndexType.IVF_PQ.value
            else:
                return IndexType.IVF_SQ8.value

        else:  # balanced
            # å¹³è¡¡æ¨¡å¼
            if num_entities > 1000000:
                return IndexType.IVF_PQ.value
            elif num_entities > 100000:
                return IndexType.IVF_FLAT.value
            else:
                return IndexType.HNSW.value

    def _build_index_params(self, index_type: str, num_entities: int) -> Dict[str, Any]:
        """æ„å»ºç´¢å¼•å‚æ•°"""
        if index_type == IndexType.HNSW.value:
            return {
                "index_type": IndexType.HNSW.value,
                "metric_type": MetricType.COSINE.value,
                "params": {"M": 16, "efConstruction": 200}
            }
        elif index_type == IndexType.IVF_FLAT.value:
            nlist = min(4096, max(1024, num_entities // 100))
            return {
                "index_type": IndexType.IVF_FLAT.value,
                "metric_type": MetricType.COSINE.value,
                "params": {"nlist": nlist}
            }
        elif index_type == IndexType.IVF_PQ.value:
            nlist = min(4096, max(1024, num_entities // 100))
            return {
                "index_type": IndexType.IVF_PQ.value,
                "metric_type": MetricType.COSINE.value,
                "params": {"nlist": nlist, "m": 16}
            }
        elif index_type == IndexType.IVF_SQ8.value:
            nlist = min(4096, max(1024, num_entities // 100))
            return {
                "index_type": IndexType.IVF_SQ8.value,
                "metric_type": MetricType.COSINE.value,
                "params": {"nlist": nlist}
            }
        else:
            return {
                "index_type": IndexType.HNSW.value,
                "metric_type": MetricType.COSINE.value,
                "params": {"M": 16, "efConstruction": 200}
            }

    async def _optimize_search_parameters(self, collection_name: str,
                                        optimization_level: str) -> Dict[str, Any]:
        """ä¼˜åŒ–æœç´¢å‚æ•°"""
        try:
            logger.info(f"å¼€å§‹æœç´¢å‚æ•°ä¼˜åŒ– - é›†åˆ: {collection_name}")

            # æ ¹æ®ä¼˜åŒ–çº§åˆ«è®¾ç½®æœç´¢å‚æ•°
            if optimization_level == "performance":
                search_params = {
                    "metric_type": MetricType.COSINE.value,
                    "params": {"ef": 128}  # æ›´é«˜çš„efå€¼ï¼Œæ›´å¥½çš„å¬å›ç‡
                }
            elif optimization_level == "memory":
                search_params = {
                    "metric_type": MetricType.COSINE.value,
                    "params": {"ef": 32}  # æ›´ä½çš„efå€¼ï¼Œæ›´å¿«çš„æœç´¢
                }
            else:  # balanced
                search_params = {
                    "metric_type": MetricType.COSINE.value,
                    "params": {"ef": 64}  # å¹³è¡¡è®¾ç½®
                }

            logger.info(f"æœç´¢å‚æ•°ä¼˜åŒ–å®Œæˆ: {search_params}")
            return {"status": "optimized", "search_params": search_params}

        except Exception as e:
            logger.error(f"æœç´¢å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _optimize_memory_usage(self, collection_name: str,
                                   optimization_level: str) -> Dict[str, Any]:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            logger.info(f"å¼€å§‹å†…å­˜ä¼˜åŒ– - é›†åˆ: {collection_name}")

            # æ ¹æ®ä¼˜åŒ–çº§åˆ«å†³å®šæ˜¯å¦åŠ è½½é›†åˆåˆ°å†…å­˜
            if optimization_level == "memory":
                # å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼Œä¸é¢„åŠ è½½é›†åˆ
                await self.milvus_service.release_collection(collection_name)
                logger.info("å·²é‡Šæ”¾é›†åˆå†…å­˜ï¼ˆå†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼‰")
                return {"status": "optimized", "memory_mode": "lazy_loading"}
            else:
                # æ€§èƒ½ä¼˜å…ˆæ¨¡å¼ï¼Œé¢„åŠ è½½é›†åˆ
                await self.milvus_service.load_collection(collection_name)
                logger.info("å·²é¢„åŠ è½½é›†åˆåˆ°å†…å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–æ¨¡å¼ï¼‰")
                return {"status": "optimized", "memory_mode": "preloaded"}

        except Exception as e:
            logger.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _measure_search_latency(self, collection_name: str) -> float:
        """æµ‹é‡æœç´¢å»¶è¿Ÿ"""
        try:
            # ä½¿ç”¨æ¨¡æ‹ŸæŸ¥è¯¢å‘é‡è¿›è¡Œæµ‹è¯•
            test_vector = [0.1] * 1024

            # é¢„çƒ­
            await self.milvus_service.search(
                collection_name=collection_name,
                query_vector=test_vector,
                top_k=10
            )

            # æ­£å¼æµ‹é‡
            latencies = []
            for _ in range(5):
                start_time = time.time()
                await self.milvus_service.search(
                    collection_name=collection_name,
                    query_vector=test_vector,
                    top_k=10
                )
                latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                latencies.append(latency)

            # è¿”å›P99å»¶è¿Ÿï¼ˆè¿™é‡Œç”¨æœ€å¤§å€¼è¿‘ä¼¼ï¼‰
            return max(latencies)

        except Exception as e:
            logger.error(f"æµ‹é‡æœç´¢å»¶è¿Ÿå¤±è´¥: {e}")
            return 100.0  # é»˜è®¤å€¼

    async def _measure_insert_throughput(self, collection_name: str) -> float:
        """æµ‹é‡æ’å…¥ååé‡"""
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            from .models import DocumentChunk
            test_chunks = []
            for i in range(100):
                chunk = DocumentChunk(
                    vector=[0.1] * 1024,
                    content=f"æµ‹è¯•å†…å®¹ {i}",
                    doc_id=f"test_doc_{i}",
                    doc_name=f"æµ‹è¯•æ–‡æ¡£ {i}",
                    kb_id="test_kb",
                    chunk_id=f"test_chunk_{i}",
                    category="test",
                    confidence=0.8,
                    timestamp=int(time.time())
                )
                test_chunks.append(chunk)

            # æµ‹é‡æ’å…¥æ€§èƒ½
            start_time = time.time()
            await self.milvus_service.insert_data(collection_name, test_chunks, batch_size=50)
            insert_time = time.time() - start_time

            # è®¡ç®—ååé‡ (æ–‡æ¡£/ç§’)
            throughput = len(test_chunks) / insert_time if insert_time > 0 else 0

            # æ¸…ç†æµ‹è¯•æ•°æ®
            await self.milvus_service.query(
                collection_name=collection_name,
                filter_expr='doc_id like "test_doc_%"',
                output_fields=["id"]
            )

            return throughput

        except Exception as e:
            logger.error(f"æµ‹é‡æ’å…¥ååé‡å¤±è´¥: {e}")
            return 0.0

    async def _get_memory_usage(self, collection_name: str) -> float:
        """è·å–å†…å­˜ä½¿ç”¨é‡"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç³»ç»ŸAPIè·å–å®é™…å†…å­˜ä½¿ç”¨
            # æš‚æ—¶è¿”å›ä¼°ç®—å€¼
            stats = await self.milvus_service.get_collection_stats(collection_name)
            num_entities = stats.get("num_entities", 0)

            # ç²—ç•¥ä¼°ç®—ï¼šæ¯ä¸ªå®ä½“çº¦å ç”¨1KBå†…å­˜ï¼ˆåŒ…æ‹¬å‘é‡ã€æ–‡æœ¬ã€å…ƒæ•°æ®ï¼‰
            estimated_memory_mb = (num_entities * 1.0) / 1024

            return estimated_memory_mb

        except Exception as e:
            logger.error(f"è·å–å†…å­˜ä½¿ç”¨å¤±è´¥: {e}")
            return 0.0

    async def _get_disk_usage(self, collection_name: str) -> float:
        """è·å–ç£ç›˜ä½¿ç”¨é‡"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç³»ç»ŸAPIè·å–å®é™…ç£ç›˜ä½¿ç”¨
            # æš‚æ—¶è¿”å›ä¼°ç®—å€¼
            stats = await self.milvus_service.get_collection_stats(collection_name)
            num_entities = stats.get("num_entities", 0)

            # ç²—ç•¥ä¼°ç®—ï¼šæ¯ä¸ªå®ä½“çº¦å ç”¨2KBç£ç›˜ç©ºé—´
            estimated_disk_mb = (num_entities * 2.0) / 1024

            return estimated_disk_mb

        except Exception as e:
            logger.error(f"è·å–ç£ç›˜ä½¿ç”¨å¤±è´¥: {e}")
            return 0.0

    async def _estimate_avg_document_size(self, collection_name: str) -> float:
        """ä¼°ç®—å¹³å‡æ–‡æ¡£å¤§å°"""
        try:
            # é‡‡æ ·è·å–æ–‡æ¡£å¤§å°
            samples = await self.milvus_service.query(
                collection_name=collection_name,
                filter_expr="",
                output_fields=["content"],
                limit=10
            )

            if not samples:
                return 0.0

            total_size = sum(len(sample.get("content", "")) for sample in samples)
            return total_size / len(samples)

        except Exception as e:
            logger.error(f"ä¼°ç®—å¹³å‡æ–‡æ¡£å¤§å°å¤±è´¥: {e}")
            return 0.0

    async def _get_current_index_type(self, collection_name: str) -> str:
        """è·å–å½“å‰ç´¢å¼•ç±»å‹"""
        try:
            collection = self.milvus_service._get_collection(collection_name)
            if not collection:
                return "unknown"

            # è·å–ç´¢å¼•ä¿¡æ¯
            indexes = collection.indexes
            if indexes:
                for index in indexes:
                    if index.field_name == "vector":
                        return index.params.get("index_type", "unknown")

            return "none"

        except Exception as e:
            logger.error(f"è·å–å½“å‰ç´¢å¼•ç±»å‹å¤±è´¥: {e}")
            return "unknown"

    def _calculate_improvement_ratio(self, before: Dict[str, Any],
                                   after: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›æ¯”ä¾‹"""
        improvements = {}

        for key in before.keys():
            if key in after and isinstance(before[key], (int, float)) and isinstance(after[key], (int, float)):
                if before[key] > 0:
                    if key in ["search_latency_p99", "memory_usage_mb", "disk_usage_mb"]:
                        # è¿™äº›æŒ‡æ ‡è¶Šå°è¶Šå¥½
                        improvement = (before[key] - after[key]) / before[key] * 100
                    else:
                        # è¿™äº›æŒ‡æ ‡è¶Šå¤§è¶Šå¥½
                        improvement = (after[key] - before[key]) / before[key] * 100

                    improvements[key] = round(improvement, 2)

        return improvements

    def _generate_recommendations(self, stats: CollectionStats,
                                before: Dict[str, Any],
                                after: Dict[str, Any],
                                optimization_level: str) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæ•°æ®é‡çš„å»ºè®®
        if stats.num_entities > 1000000:
            recommendations.append("æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨IVF_PQç´¢å¼•ä»¥èŠ‚çœå†…å­˜")
            recommendations.append("è€ƒè™‘ä½¿ç”¨åˆ†åŒºç­–ç•¥æé«˜æŸ¥è¯¢æ€§èƒ½")

        elif stats.num_entities > 100000:
            recommendations.append("æ•°æ®é‡ä¸­ç­‰ï¼Œå»ºè®®ä½¿ç”¨IVF_FLATç´¢å¼•å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦")

        else:
            recommendations.append("æ•°æ®é‡è¾ƒå°ï¼ŒHNSWç´¢å¼•èƒ½æä¾›æœ€ä½³æ€§èƒ½")

        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        if after.get("search_latency_p99", 0) > 50:
            recommendations.append("æœç´¢å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æœç´¢å‚æ•°æˆ–å¢åŠ ç¡¬ä»¶èµ„æº")

        if after.get("memory_usage_mb", 0) > 2048:
            recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–ç´¢å¼•ç±»å‹æˆ–æ¸…ç†æ— ç”¨æ•°æ®")

        # åŸºäºä¼˜åŒ–çº§åˆ«çš„å»ºè®®
        if optimization_level == "performance":
            recommendations.append("æ€§èƒ½ä¼˜åŒ–æ¨¡å¼ï¼šå·²å¯ç”¨é¢„åŠ è½½å’Œé«˜çº§ç´¢å¼•å‚æ•°")
        elif optimization_level == "memory":
            recommendations.append("å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼šå·²å¯ç”¨å»¶è¿ŸåŠ è½½å’Œå‹ç¼©ç´¢å¼•")
        else:
            recommendations.append("å¹³è¡¡æ¨¡å¼ï¼šåœ¨æ€§èƒ½å’Œèµ„æºä½¿ç”¨ä¹‹é—´å–å¾—å¹³è¡¡")

        return recommendations

    async def get_optimization_history(self, collection_name: Optional[str] = None) -> List[OptimizationResult]:
        """è·å–ä¼˜åŒ–å†å²"""
        if collection_name:
            return [result for result in self.optimization_history
                   if result.collection_name == collection_name]
        return self.optimization_history

    async def get_performance_trends(self, collection_name: str,
                                   days: int = 7) -> Dict[str, List[float]]:
        """è·å–æ€§èƒ½è¶‹åŠ¿"""
        try:
            trends = {
                "search_latency": [],
                "insert_throughput": [],
                "memory_usage": [],
                "timestamps": []
            }

            # ä»å†å²æ•°æ®ä¸­æå–è¶‹åŠ¿
            cutoff_date = datetime.now() - timedelta(days=days)
            relevant_history = [
                result for result in self.optimization_history
                if result.collection_name == collection_name
                and result.timestamp >= cutoff_date
            ]

            for result in relevant_history:
                trends["search_latency"].append(result.after_metrics.get("search_latency_p99", 0))
                trends["insert_throughput"].append(result.after_metrics.get("insert_throughput", 0))
                trends["memory_usage"].append(result.after_metrics.get("memory_usage_mb", 0))
                trends["timestamps"].append(result.timestamp.isoformat())

            return trends

        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½è¶‹åŠ¿å¤±è´¥: {e}")
            return {}

    async def benchmark_collection(self, collection_name: str) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•é›†åˆæ€§èƒ½"""
        try:
            logger.info(f"å¼€å§‹åŸºå‡†æµ‹è¯• - é›†åˆ: {collection_name}")

            # è·å–é›†åˆç»Ÿè®¡
            stats = await self.milvus_service.get_collection_stats(collection_name)
            num_entities = stats.get("num_entities", 0)

            # ç¡®å®šæ•°æ®é›†è§„æ¨¡ç±»åˆ«
            dataset_size = self._classify_dataset_size(num_entities)

            # è·å–åŸºå‡†é…ç½®
            benchmark_config = PERFORMANCE_BENCHMARKS.get(dataset_size, {})

            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            actual_metrics = await self._get_collection_metrics(collection_name)

            # å¯¹æ¯”åŸºå‡†
            comparison = {}
            for metric, expected in benchmark_config.items():
                actual = actual_metrics.get(metric, 0)
                if isinstance(expected, str) and expected.startswith("<"):
                    # å°äºæŸä¸ªå€¼
                    threshold = float(expected[1:])
                    meets_benchmark = actual < threshold
                elif isinstance(expected, str) and expected.startswith(">"):
                    # å¤§äºæŸä¸ªå€¼
                    threshold = float(expected[1:])
                    meets_benchmark = actual > threshold
                else:
                    meets_benchmark = False

                comparison[metric] = {
                    "expected": expected,
                    "actual": actual,
                    "meets_benchmark": meets_benchmark
                }

            result = {
                "dataset_size": dataset_size,
                "num_entities": num_entities,
                "benchmark_config": benchmark_config,
                "actual_metrics": actual_metrics,
                "comparison": comparison,
                "overall_score": self._calculate_overall_score(comparison)
            }

            logger.info(f"åŸºå‡†æµ‹è¯•å®Œæˆ - æ€»ä½“è¯„åˆ†: {result['overall_score']}")
            return result

        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}

    def _classify_dataset_size(self, num_entities: int) -> str:
        """åˆ†ç±»æ•°æ®é›†è§„æ¨¡"""
        if num_entities < 10000:
            return "small_dataset"
        elif num_entities < 1000000:
            return "medium_dataset"
        elif num_entities < 10000000:
            return "large_dataset"
        else:
            return "xlarge_dataset"

    def _calculate_overall_score(self, comparison: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        total_metrics = len(comparison)
        if total_metrics == 0:
            return 0.0

        passed_metrics = sum(1 for metric in comparison.values()
                           if metric.get("meets_benchmark", False))

        return (passed_metrics / total_metrics) * 100

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†ä¼˜åŒ–æœåŠ¡èµ„æº")
            self.performance_cache.clear()
            logger.info("âœ… ä¼˜åŒ–æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")