"""
ESåˆ°Milvusæ•°æ®è¿ç§»æœåŠ¡
å®ç°ä»ç°æœ‰Elasticsearchåˆ°Milvusçš„å¹³æ»‘æ•°æ®è¿ç§»
"""

import asyncio
import time
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from elasticsearch import AsyncElasticsearch
from pymilvus import Collection, utility

from .models import (
    DocumentChunk, MigrationResult, ES_TO_MILVUS_MAPPING,
    MIGRATION_CONFIG, PERFORMANCE_BASELINES
)
from .milvus_service import MilvusService

logger = logging.getLogger(__name__)


class DataMigrationService:
    """ESåˆ°Milvusæ•°æ®è¿ç§»æœåŠ¡"""

    def __init__(self,
                 es_client: AsyncElasticsearch,
                 milvus_service: MilvusService,
                 batch_size: int = 1000,
                 max_workers: int = 4,
                 validation_sample_rate: float = 0.01):
        """
        åˆå§‹åŒ–è¿ç§»æœåŠ¡

        Args:
            es_client: Elasticsearchå®¢æˆ·ç«¯
            milvus_service: MilvusæœåŠ¡å®ä¾‹
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            validation_sample_rate: éªŒè¯é‡‡æ ·ç‡
        """
        self.es_client = es_client
        self.milvus_service = milvus_service
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.validation_sample_rate = validation_sample_rate
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    async def migrate_user_data(self, user_id: int) -> MigrationResult:
        """è¿ç§»å•ä¸ªç”¨æˆ·çš„æ‰€æœ‰æ•°æ®"""
        try:
            logger.info(f"å¼€å§‹è¿ç§»ç”¨æˆ· {user_id} çš„æ•°æ®")

            start_time = datetime.now()
            total_migrated = 0
            success_count = 0
            failed_count = 0
            errors = []

            # 1. è·å–ç”¨æˆ·æ•°æ®ç»Ÿè®¡
            es_stats = await self.get_es_statistics(user_id)
            logger.info(f"ESæ•°æ®ç»Ÿè®¡ - æ€»è®°å½•æ•°: {es_stats['total_documents']}")

            # 2. åˆ›å»ºMilvusé›†åˆ
            collection_name = f"user_{user_id}_documents"
            if not await self._create_user_collection(collection_name):
                error_msg = f"åˆ›å»ºç”¨æˆ·é›†åˆå¤±è´¥: {collection_name}"
                logger.error(error_msg)
                return MigrationResult(
                    user_id=str(user_id),
                    total_migrated=0,
                    success_count=0,
                    failed_count=0,
                    validation_passed=False,
                    migration_time=0,
                    start_time=start_time,
                    end_time=datetime.now(),
                    errors=[error_msg]
                )

            # 3. æ‰§è¡Œæ•°æ®è¿ç§»
            migration_result = await self._migrate_data_in_batches(
                user_id, collection_name, es_stats['total_documents']
            )

            total_migrated = migration_result['total_processed']
            success_count = migration_result['success_count']
            failed_count = migration_result['failed_count']
            errors.extend(migration_result['errors'])

            # 4. æ•°æ®éªŒè¯
            validation_passed = await self._validate_migration(
                user_id, collection_name, total_migrated
            )

            # 5. æ€§èƒ½å¯¹æ¯”
            performance_comparison = await self._compare_performance(
                user_id, collection_name, es_stats
            )

            end_time = datetime.now()
            migration_time = (end_time - start_time).total_seconds()

            # 6. ç”Ÿæˆè¿ç§»æŠ¥å‘Š
            migration_result = MigrationResult(
                user_id=str(user_id),
                total_migrated=total_migrated,
                success_count=success_count,
                failed_count=failed_count,
                validation_passed=validation_passed,
                migration_time=migration_time,
                start_time=start_time,
                end_time=end_time,
                errors=errors
            )

            # 7. è®°å½•è¿ç§»ç»“æœ
            await self._log_migration_result(migration_result, performance_comparison)

            logger.info(f"âœ… ç”¨æˆ· {user_id} æ•°æ®è¿ç§»å®Œæˆ")
            logger.info(f"ğŸ“Š æ€»è®¡: {total_migrated}, æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
            logger.info(f"âœ… éªŒè¯é€šè¿‡: {validation_passed}")
            logger.info(f"â±ï¸  è€—æ—¶: {migration_time:.2f}ç§’")

            return migration_result

        except Exception as e:
            logger.error(f"âŒ è¿ç§»ç”¨æˆ· {user_id} æ•°æ®å¤±è´¥: {e}")
            end_time = datetime.now()
            migration_time = (end_time - start_time).total_seconds()

            return MigrationResult(
                user_id=str(user_id),
                total_migrated=0,
                success_count=0,
                failed_count=0,
                validation_passed=False,
                migration_time=migration_time,
                start_time=start_time,
                end_time=end_time,
                errors=[str(e)]
            )

    async def get_es_statistics(self, user_id: int) -> Dict[str, Any]:
        """è·å–ESæ•°æ®ç»Ÿè®¡"""
        try:
            logger.info(f"æ­£åœ¨è·å–ç”¨æˆ· {user_id} çš„ESæ•°æ®ç»Ÿè®¡")

            # è·å–æ€»æ–‡æ¡£æ•°
            count_query = {
                "query": {"match_all": {}},
                "size": 0
            }

            count_response = await self.es_client.search(
                index=str(user_id),
                body=count_query
            )

            total_documents = count_response['hits']['total']['value']

            # è·å–ç´¢å¼•ä¿¡æ¯
            index_info = await self.es_client.indices.get(index=str(user_id))

            # è·å–æ–‡æ¡£å¤§å°ç»Ÿè®¡
            size_query = {
                "query": {"match_all": {}},
                "size": 100,  # é‡‡æ ·100ä¸ªæ–‡æ¡£
                "_source": ["content_with_weight"]
            }

            size_response = await self.es_client.search(
                index=str(user_id),
                body=size_query
            )

            # è®¡ç®—å¹³å‡æ–‡æ¡£å¤§å°
            total_size = 0
            doc_count = 0
            for hit in size_response['hits']['hits']:
                content = hit['_source'].get('content_with_weight', '')
                total_size += len(content)
                doc_count += 1

            avg_doc_size = total_size / doc_count if doc_count > 0 else 0

            stats = {
                "total_documents": total_documents,
                "index_name": str(user_id),
                "avg_doc_size": avg_doc_size,
                "estimated_total_size": total_documents * avg_doc_size
            }

            logger.info(f"ğŸ“Š ESæ•°æ®ç»Ÿè®¡ - æ€»è®°å½•æ•°: {total_documents}, å¹³å‡æ–‡æ¡£å¤§å°: {avg_doc_size:.0f}å­—ç¬¦")

            return stats

        except Exception as e:
            logger.error(f"âŒ è·å–ESæ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total_documents": 0, "avg_doc_size": 0}

    async def _create_user_collection(self, collection_name: str) -> bool:
        """åˆ›å»ºç”¨æˆ·ä¸“ç”¨é›†åˆ"""
        try:
            logger.info(f"æ­£åœ¨åˆ›å»ºç”¨æˆ·é›†åˆ: {collection_name}")

            # åˆ›å»ºé›†åˆé…ç½®
            config = CollectionConfig(
                collection_name=collection_name,
                description=f"ç”¨æˆ·æ–‡æ¡£å‘é‡å­˜å‚¨ - {collection_name}",
                vector_dim=1024,  # ä¿æŒä¸ESç›¸åŒçš„ç»´åº¦
                metric_type=MetricType.COSINE,  # ä¿æŒä¸ESç›¸åŒçš„åº¦é‡æ–¹å¼
                index_type=IndexType.HNSW,  # é«˜æ€§èƒ½ç´¢å¼•
                enable_dynamic_field=True
            )

            # åˆ›å»ºé›†åˆ
            success = await self.milvus_service.create_collection(collection_name, config)
            if not success:
                return False

            # åˆ›å»ºç´¢å¼•
            index_success = await self.milvus_service.create_index(collection_name)
            if not index_success:
                return False

            logger.info(f"âœ… æˆåŠŸåˆ›å»ºç”¨æˆ·é›†åˆ: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç”¨æˆ·é›†åˆå¤±è´¥: {e}")
            return False

    async def _migrate_data_in_batches(self, user_id: int, collection_name: str, total_count: int) -> Dict[str, Any]:
        """æ‰¹é‡è¿ç§»æ•°æ®"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡è¿ç§»æ•°æ® - æ€»æ•°: {total_count}")

            total_processed = 0
            success_count = 0
            failed_count = 0
            errors = []

            # ä½¿ç”¨scroll APIæ‰¹é‡è¯»å–æ•°æ®
            scroll_time = "5m"
            batch_size = self.batch_size

            # å¼€å§‹scroll
            initial_query = {
                "query": {"match_all": {}},
                "size": batch_size,
                "_source": [
                    "_id", "content_with_weight", "content_ltks", "doc_id", "docnm_kwd",
                    "q_1024_vec", "create_time", "create_timestamp_flt", "kb_id"
                ],
                "sort": ["_doc"]
            }

            scroll_response = await self.es_client.search(
                index=str(user_id),
                body=initial_query,
                scroll=scroll_time
            )

            scroll_id = scroll_response.get('_scroll_id')
            hits = scroll_response['hits']['hits']

            while hits:
                logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {total_processed}-{min(total_processed + batch_size, total_count)}")

                # è½¬æ¢å’Œå¤„ç†æ•°æ®
                batch_result = await self._process_batch(hits, collection_name)

                total_processed += batch_result['processed_count']
                success_count += batch_result['success_count']
                failed_count += batch_result['failed_count']
                errors.extend(batch_result['errors'])

                # è·å–ä¸‹ä¸€æ‰¹æ•°æ®
                scroll_response = await self.es_client.scroll(
                    scroll_id=scroll_id,
                    scroll=scroll_time
                )

                hits = scroll_response['hits']['hits']

                # å®šæœŸæŠ¥å‘Šè¿›åº¦
                if total_processed % 10000 == 0:
                    progress = (total_processed / total_count) * 100
                    logger.info(f"ğŸ“ˆ è¿ç§»è¿›åº¦: {progress:.1f}% ({total_processed}/{total_count})")

            return {
                "total_processed": total_processed,
                "success_count": success_count,
                "failed_count": failed_count,
                "errors": errors
            }

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è¿ç§»æ•°æ®å¤±è´¥: {e}")
            return {
                "total_processed": total_processed,
                "success_count": success_count,
                "failed_count": failed_count,
                "errors": [str(e)]
            }

    async def _process_batch(self, hits: List[Dict[str, Any]], collection_name: str) -> Dict[str, Any]:
        """å¤„ç†ä¸€æ‰¹æ•°æ®"""
        try:
            processed_count = 0
            success_count = 0
            failed_count = 0
            errors = []
            milvus_data = []

            for hit in hits:
                try:
                    # è½¬æ¢ESæ•°æ®åˆ°Milvusæ ¼å¼
                    milvus_record = self._convert_es_to_milvus(hit)
                    milvus_data.append(milvus_record)
                    processed_count += 1

                except Exception as e:
                    logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥ (ID: {hit.get('_id', 'unknown')}): {e}")
                    failed_count += 1
                    errors.append(f"ID {hit.get('_id', 'unknown')}: {str(e)}")

            # æ‰¹é‡æ’å…¥Milvus
            if milvus_data:
                try:
                    # åˆ›å»ºDocumentChunkå¯¹è±¡
                    chunks = []
                    for record in milvus_data:
                        chunk = DocumentChunk(**record)
                        chunks.append(chunk)

                    # æ’å…¥æ•°æ®
                    insert_success = await self.milvus_service.insert_data(
                        collection_name, chunks, batch_size=100
                    )

                    if insert_success:
                        success_count += len(milvus_data)
                    else:
                        failed_count += len(milvus_data)
                        errors.append("Milvusæ’å…¥å¤±è´¥")

                except Exception as e:
                    logger.error(f"Milvusæ’å…¥å¤±è´¥: {e}")
                    failed_count += len(milvus_data)
                    errors.append(f"Milvusæ’å…¥: {str(e)}")

            return {
                "processed_count": processed_count,
                "success_count": success_count,
                "failed_count": failed_count,
                "errors": errors
            }

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}")
            return {
                "processed_count": 0,
                "success_count": 0,
                "failed_count": len(hits),
                "errors": [str(e)]
            }

    def _convert_es_to_milvus(self, es_hit: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢ESæ•°æ®åˆ°Milvusæ ¼å¼"""
        try:
            source = es_hit['_source']
            es_id = es_hit['_id']
            es_index = es_hit['_index']

            # åŸºç¡€æ•°æ®è½¬æ¢
            milvus_record = {
                "vector": source.get('q_1024_vec', []),
                "content": source.get('content_with_weight', ''),
                "content_ltks": source.get('content_ltks', ''),
                "doc_id": source.get('doc_id', ''),
                "doc_name": source.get('docnm_kwd', ''),
                "kb_id": es_index,  # ESç´¢å¼•åä½œä¸ºkb_id
                "chunk_id": es_id,  # ESæ–‡æ¡£IDä½œä¸ºchunk_id
                "category": "general",  # é»˜è®¤åˆ†ç±»
                "confidence": 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                "timestamp": int(source.get('create_timestamp_flt', time.time())),
                "source": "migration",
                "keywords": "",  # å¯ä»¥ä»å†…å®¹ä¸­æå–
                "metadata": {
                    "original_id": es_id,
                    "original_index": es_index,
                    "migration_time": datetime.now().isoformat(),
                    "es_create_time": source.get('create_time', ''),
                    # å­˜å‚¨ESä¸­çš„å…¶ä»–åŠ¨æ€å­—æ®µ
                    "es_dynamic_fields": {
                        key: value for key, value in source.items()
                        if key not in ['q_1024_vec', 'content_with_weight', 'content_ltks', 'doc_id', 'docnm_kwd', 'create_time', 'create_timestamp_flt']
                    }
                }
            }

            return milvus_record

        except Exception as e:
            logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            raise e

    async def _validate_migration(self, user_id: int, collection_name: str, expected_count: int) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        try:
            logger.info(f"æ­£åœ¨éªŒè¯è¿ç§»ç»“æœ - ç”¨æˆ·: {user_id}, é›†åˆ: {collection_name}")

            # è·å–Milvusä¸­çš„æ•°æ®æ•°é‡
            milvus_stats = await self.milvus_service.get_collection_stats(collection_name)
            milvus_count = milvus_stats.get('num_entities', 0)

            # è·å–ESä¸­çš„æ•°æ®æ•°é‡ï¼ˆå†æ¬¡ç¡®è®¤ï¼‰
            es_stats = await self.get_es_statistics(user_id)
            es_count = es_stats.get('total_documents', 0)

            # æ•°æ®æ•°é‡éªŒè¯
            count_match = abs(milvus_count - expected_count) <= 10  # å…è®¸10æ¡ä»¥å†…çš„å·®å¼‚

            # é‡‡æ ·éªŒè¯
            sample_validation = await self._sample_validation(user_id, collection_name)

            validation_passed = count_match and sample_validation

            logger.info(f"âœ… éªŒè¯ç»“æœ - æ•°é‡åŒ¹é…: {count_match}, é‡‡æ ·éªŒè¯: {sample_validation}")
            logger.info(f"ğŸ“Š Milvusæ•°é‡: {milvus_count}, ESæ•°é‡: {es_count}, æœŸæœ›æ•°é‡: {expected_count}")

            return validation_passed

        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¿ç§»ç»“æœå¤±è´¥: {e}")
            return False

    async def _sample_validation(self, user_id: int, collection_name: str) -> bool:
        """é‡‡æ ·éªŒè¯"""
        try:
            logger.info(f"æ­£åœ¨æ‰§è¡Œé‡‡æ ·éªŒè¯ - é‡‡æ ·ç‡: {self.validation_sample_rate}")

            # ä»ESè·å–é‡‡æ ·æ•°æ®
            sample_query = {
                "query": {"function_score": {
                    "functions": [{"random_score": {}}],
                    "random_score": {}
                }},
                "size": 100,  # é‡‡æ ·100æ¡
                "_source": [
                    "_id", "content_with_weight", "q_1024_vec", "doc_id", "docnm_kwd"
                ]
            }

            sample_response = await self.es_client.search(
                index=str(user_id),
                body=sample_query
            )

            samples = sample_response['hits']['hits']
            validation_passed = True

            for i, hit in enumerate(samples):
                try:
                    es_data = hit['_source']
                    es_id = hit['_id']

                    # åœ¨Milvusä¸­æŸ¥æ‰¾å¯¹åº”è®°å½•
                    milvus_results = await self.milvus_service.query(
                        collection_name=collection_name,
                        filter_expr=f'chunk_id == "{es_id}"',
                        output_fields=["content", "vector", "doc_id", "doc_name"],
                        limit=1
                    )

                    if len(milvus_results) == 0:
                        logger.warning(f"é‡‡æ ·éªŒè¯å¤±è´¥ - æœªæ‰¾åˆ°å¯¹åº”è®°å½•: {es_id}")
                        validation_passed = False
                        continue

                    milvus_data = milvus_results[0]

                    # éªŒè¯å…³é”®å­—æ®µ
                    content_match = es_data.get('content_with_weight', '') == milvus_data.get('content', '')
                    doc_id_match = es_data.get('doc_id', '') == milvus_data.get('doc_id', '')
                    doc_name_match = es_data.get('docnm_kwd', '') == milvus_data.get('doc_name', '')
                    vector_match = len(es_data.get('q_1024_vec', [])) == len(milvus_data.get('vector', []))

                    if not (content_match and doc_id_match and doc_name_match and vector_match):
                        logger.warning(f"é‡‡æ ·éªŒè¯å¤±è´¥ - å­—æ®µä¸åŒ¹é…: {es_id}")
                        validation_passed = False

                except Exception as e:
                    logger.error(f"é‡‡æ ·éªŒè¯é”™è¯¯ (ID: {es_id}): {e}")
                    validation_passed = False

            logger.info(f"âœ… é‡‡æ ·éªŒè¯å®Œæˆ - é€šè¿‡ç‡: {validation_passed}")
            return validation_passed

        except Exception as e:
            logger.error(f"âŒ é‡‡æ ·éªŒè¯å¤±è´¥: {e}")
            return False

    async def _compare_performance(self, user_id: int, collection_name: str, es_stats: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½å¯¹æ¯”"""
        try:
            logger.info(f"æ­£åœ¨å¯¹æ¯”æ€§èƒ½ - ç”¨æˆ·: {user_id}, é›†åˆ: {collection_name}")

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_query = {
                "query": {"match_all": {}},
                "size": 10,
                "_source": ["q_1024_vec"]
            }

            test_response = await self.es_client.search(
                index=str(user_id),
                body=test_query
            )

            if len(test_response['hits']['hits']) == 0:
                return {"error": "No test data available"}

            test_vector = test_response['hits']['hits'][0]['_source']['q_1024_vec']

            # ESæ€§èƒ½æµ‹è¯•
            es_start = time.time()
            es_results = await self.es_client.search(
                index=str(user_id),
                body={
                    "query": {
                        "script_score": {
                            "query": {"match_all": {}},
                            "script": {
                                "source": "cosineSimilarity(params.query_vector, 'q_1024_vec') + 1.0",
                                "params": {"query_vector": test_vector}
                            }
                        }
                    },
                    "size": 10
                }
            )
            es_time = time.time() - es_start

            # Milvusæ€§èƒ½æµ‹è¯•
            milvus_start = time.time()
            milvus_results = await self.milvus_service.search(
                collection_name=collection_name,
                query_vector=test_vector,
                top_k=10
            )
            milvus_time = time.time() - milvus_start

            comparison = {
                "es_search_time": es_time,
                "milvus_search_time": milvus_time,
                "speedup_ratio": es_time / milvus_time if milvus_time > 0 else 0,
                "es_result_count": len(es_results['hits']['hits']),
                "milvus_result_count": len(milvus_results)
            }

            logger.info(f"ğŸ“Š æ€§èƒ½å¯¹æ¯” - ES: {es_time:.3f}s, Milvus: {milvus_time:.3f}s, åŠ é€Ÿæ¯”: {comparison['speedup_ratio']:.2f}x")

            return comparison

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å¯¹æ¯”å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _log_migration_result(self, result: MigrationResult, performance_comparison: Dict[str, Any]) -> None:
        """è®°å½•è¿ç§»ç»“æœ"""
        try:
            migration_log = {
                "user_id": result.user_id,
                "timestamp": datetime.now().isoformat(),
                "total_migrated": result.total_migrated,
                "success_rate": result.success_count / result.total_migrated if result.total_migrated > 0 else 0,
                "validation_passed": result.validation_passed,
                "migration_time": result.migration_time,
                "performance_comparison": performance_comparison,
                "errors_count": len(result.errors)
            }

            logger.info(f"ğŸ“‹ è¿ç§»ç»“æœè®°å½•: {json.dumps(migration_log, indent=2)}")

        except Exception as e:
            logger.error(f"è®°å½•è¿ç§»ç»“æœå¤±è´¥: {e}")

    async def incremental_sync(self, user_id: int, last_sync_time: datetime) -> bool:
        """å¢é‡æ•°æ®åŒæ­¥"""
        try:
            logger.info(f"å¼€å§‹å¢é‡æ•°æ®åŒæ­¥ - ç”¨æˆ·: {user_id}, ä¸Šæ¬¡åŒæ­¥: {last_sync_time}")

            # è·å–æ–°å¢æ•°æ®
            new_data_query = {
                "query": {
                    "range": {
                        "create_timestamp_flt": {
                            "gt": last_sync_time.timestamp()
                        }
                    }
                },
                "size": 1000,
                "sort": [{"create_timestamp_flt": "asc"}]
            }

            new_data_response = await self.es_client.search(
                index=str(user_id),
                body=new_data_query,
                scroll="2m"
            )

            new_data_count = 0
            scroll_id = new_data_response.get('_scroll_id')
            hits = new_data_response['hits']['hits']

            while hits:
                # å¤„ç†æ–°å¢æ•°æ®
                for hit in hits:
                    await self._process_new_document(hit, user_id)
                    new_data_count += 1

                # è·å–ä¸‹ä¸€æ‰¹
                scroll_response = await self.es_client.scroll(
                    scroll_id=scroll_id,
                    scroll="2m"
                )
                hits = scroll_response['hits']['hits']

            # è·å–æ›´æ–°æ•°æ®
            updated_data_query = {
                "query": {
                    "bool": {
                        "must": [
                            {"range": {"create_timestamp_flt": {"lte": last_sync_time.timestamp()}}},
                            {"range": {"update_timestamp_flt": {"gt": last_sync_time.timestamp()}}}
                        ]
                    }
                },
                "size": 1000
            }

            # å¤„ç†æ›´æ–°æ•°æ®
            updated_data_response = await self.es_client.search(
                index=str(user_id),
                body=updated_data_query
            )

            updated_data_count = 0
            for hit in updated_data_response['hits']['hits']:
                await self._process_updated_document(hit, user_id)
                updated_data_count += 1

            logger.info(f"âœ… å¢é‡åŒæ­¥å®Œæˆ - æ–°å¢: {new_data_count}, æ›´æ–°: {updated_data_count}")
            return True

        except Exception as e:
            logger.error(f"âŒ å¢é‡åŒæ­¥å¤±è´¥: {e}")
            return False

    async def _process_new_document(self, hit: Dict[str, Any], user_id: int) -> None:
        """å¤„ç†æ–°å¢æ–‡æ¡£"""
        try:
            # è½¬æ¢æ•°æ®æ ¼å¼
            milvus_record = self._convert_es_to_milvus(hit)

            # æ’å…¥åˆ°Milvus
            collection_name = f"user_{user_id}_documents"
            chunk = DocumentChunk(**milvus_record)
            await self.milvus_service.insert_data(collection_name, [chunk])

        except Exception as e:
            logger.error(f"å¤„ç†æ–°å¢æ–‡æ¡£å¤±è´¥: {e}")

    async def _process_updated_document(self, hit: Dict[str, Any], user_id: int) -> None:
        """å¤„ç†æ›´æ–°æ–‡æ¡£"""
        try:
            es_id = hit['_id']
            collection_name = f"user_{user_id}_documents"

            # åˆ é™¤æ—§è®°å½•
            await self.milvus_service.query(
                collection_name=collection_name,
                filter_expr=f'chunk_id == "{es_id}"',
                output_fields=["id"],
                limit=1
            )

            # æ’å…¥æ›´æ–°åçš„è®°å½•
            milvus_record = self._convert_es_to_milvus(hit)
            chunk = DocumentChunk(**milvus_record)
            await self.milvus_service.insert_data(collection_name, [chunk])

        except Exception as e:
            logger.error(f"å¤„ç†æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}")

    async def rollback_migration(self, user_id: int) -> bool:
        """å›æ»šè¿ç§»"""
        try:
            logger.warning(f"æ­£åœ¨å›æ»šç”¨æˆ· {user_id} çš„è¿ç§»")

            collection_name = f"user_{user_id}_documents"

            # åˆ é™¤Milvusä¸­çš„æ•°æ®
            await self.milvus_service.delete_collection(collection_name)

            logger.info(f"âœ… å›æ»šå®Œæˆ - å·²åˆ é™¤é›†åˆ: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ å›æ»šå¤±è´¥: {e}")
            return False

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†è¿ç§»æœåŠ¡èµ„æº")
            self.executor.shutdown(wait=True)
            logger.info("âœ… è¿ç§»æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")

    def get_migration_status(self, user_id: int) -> Dict[str, Any]:
        """è·å–è¿ç§»çŠ¶æ€"""
        try:
            collection_name = f"user_{user_id}_documents"

            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not utility.has_collection(collection_name):
                return {
                    "status": "not_started",
                    "milvus_count": 0,
                    "es_count": 0,
                    "progress": 0
                }

            # è·å–Milvusæ•°æ®æ•°é‡
            milvus_stats = self.milvus_service.get_collection_stats(collection_name)
            milvus_count = milvus_stats.get('num_entities', 0)

            # è·å–ESæ•°æ®æ•°é‡
            # è¿™é‡Œéœ€è¦å¼‚æ­¥è°ƒç”¨ï¼Œä½†åœ¨åŒæ­¥æ–¹æ³•ä¸­æ— æ³•ä½¿ç”¨await
            # è¿”å›å¼‚æ­¥æ–¹æ³•è°ƒç”¨æ‰€éœ€çš„ä¿¡æ¯
            return {
                "status": "completed" if milvus_count > 0 else "in_progress",
                "milvus_count": milvus_count,
                "collection_name": collection_name,
                "user_id": user_id
            }

        except Exception as e:
            logger.error(f"è·å–è¿ç§»çŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}
