"""
ç ”ç©¶æœåŠ¡å±‚
å¤„ç†ç ”ç©¶ä»»åŠ¡çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""
import asyncio
import logging
import os
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator, List

from services.agent_orchestration.odr_orchestrator import ODRResearchOrchestrator, ResearchResult
from services.agent_orchestration.odr_configuration import Configuration

# LangSmith é›†æˆ
try:
    from utils.langsmith_integration import (
        get_langsmith_integration,
        trace_research_step,
        log_research_start,
        log_research_complete,
        is_langsmith_enabled,
        get_langsmith_config
    )
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    # æä¾›ç©ºå®ç°
    def trace_research_step(*args, **kwargs):
        def decorator(func):
            return func
        return decorator

    def log_research_start(*args, **kwargs):
        pass

    def log_research_complete(*args, **kwargs):
        pass

    def is_langsmith_enabled():
        return False

    def get_langsmith_config(*args, **kwargs):
        return {}

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenv æœªå®‰è£…ï¼Œè·³è¿‡

logger = logging.getLogger(__name__)

# é…ç½®æ—¥å¿—
if not logger.handlers:
    # é˜²æ­¢æ—¥å¿—é‡å¤
    logger.propagate = False
    
    # æ ¹æ®ç¯å¢ƒå˜é‡è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    logger.setLevel(getattr(logging, log_level, logging.INFO))
    
    # æ·»åŠ æ§åˆ¶å° handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, log_level, logging.INFO))
    
    # è®¾ç½®æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(name)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)

# å…¨å±€ç¼–æ’å™¨å®ä¾‹
orchestrator: Optional[ODRResearchOrchestrator] = None

# ç ”ç©¶ä»»åŠ¡çŠ¶æ€ç®¡ç†
research_tasks: Dict[str, ResearchResult] = {}


async def get_orchestrator(research_depth: str = "comprehensive") -> ODRResearchOrchestrator:
    """è·å–ç¼–æ’å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global orchestrator

    if orchestrator is None:
        # ä½¿ç”¨ Configuration çš„é»˜è®¤å€¼ï¼Œä¾¿äºç»Ÿä¸€ç®¡ç†
        # é»˜è®¤å€¼åœ¨ odr_configuration.py ä¸­å®šä¹‰
        config = Configuration(
            # max_researcher_iterations=3  # ä½¿ç”¨é»˜è®¤å€¼
            # max_concurrent_research_units=5  # ä½¿ç”¨é»˜è®¤å€¼
            # max_react_tool_calls=10  # ä½¿ç”¨é»˜è®¤å€¼
            allow_clarification=True,
            search_api="serper"
        )
        orchestrator = ODRResearchOrchestrator(config)
        # æ·»åŠ è¶…æ—¶ä¿æŠ¤
        try:
            await asyncio.wait_for(orchestrator.initialize(), timeout=10.0)
            logger.info(f"Open Deep Research ç¼–æ’å™¨å·²åˆ›å»ºå¹¶åˆå§‹åŒ–")
        except asyncio.TimeoutError:
            logger.error("ç¼–æ’å™¨åˆå§‹åŒ–è¶…æ—¶")
            raise Exception("ç¼–æ’å™¨åˆå§‹åŒ–è¶…æ—¶")

    return orchestrator


@trace_research_step("execute_research_task", ["research", "execution"])
async def execute_research_task(
    research_id: str,
    question: str,
    user_context: Optional[Dict[str, Any]] = None,
    allow_clarification: bool = False,
    research_depth: str = "comprehensive"
):
    """æ‰§è¡Œç ”ç©¶ä»»åŠ¡çš„åå°å‡½æ•°"""
    # LangSmith è¿½è¸ªå¼€å§‹
    user_id = user_context.get("user_id") if user_context else None
    log_research_start(question, user_id)

    logger.info(f"ğŸš€ [TASK_START] å¼€å§‹æ‰§è¡Œç ”ç©¶ä»»åŠ¡ {research_id}")
    logger.info(f"ğŸ“ [TASK_INFO] é—®é¢˜: {question}")
    logger.info(f"âš™ï¸ [TASK_CONFIG] æ¾„æ¸…={allow_clarification}, æ·±åº¦={research_depth}")
    
    # ç«‹å³æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œè¡¨æ˜ä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œ
    if research_id in research_tasks:
        research_tasks[research_id].status = "starting"
        research_tasks[research_id].progress = 5.0
        research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
        logger.info(f"ğŸ”„ [STATUS_UPDATE] ä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸º: starting (5%)")
    
    try:
        # æ­¥éª¤1: è·å–ç¼–æ’å™¨
        logger.info(f"ğŸ”§ [STEP_1] æ­£åœ¨è·å–ç¼–æ’å™¨å®ä¾‹...")
        enh_orchestrator = await get_orchestrator(research_depth)
        logger.info(f"âœ… [STEP_1] ç¼–æ’å™¨è·å–æˆåŠŸ")

        # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(state):
            logger.info(f"ğŸ“Š [PROGRESS] ä»»åŠ¡ {research_id} è¿›åº¦æ›´æ–°: {state.status} ({state.progress:.1f}%)")
            if research_id in research_tasks:
                # æ›´æ–°ç ”ç©¶ä»»åŠ¡çš„çŠ¶æ€ï¼Œä»ResearchStateè½¬æ¢ä¸ºResearchResult
                result = research_tasks[research_id]
                old_status = result.status
                old_progress = result.progress
                result.status = state.status.value if hasattr(state.status, 'value') else str(state.status)
                result.progress = state.progress
                result.metadata["updated_at"] = datetime.now().isoformat()
                
                # è¯¦ç»†çš„çŠ¶æ€å˜åŒ–æ—¥å¿—
                if old_status != result.status or abs(old_progress - result.progress) >= 5:
                    logger.info(f"ğŸ”„ [STATUS_CHANGE] {research_id}: {old_status}({old_progress:.1f}%) â†’ {result.status}({result.progress:.1f}%)")
            else:
                logger.error(f"âŒ [ERROR] ç ”ç©¶ä»»åŠ¡ {research_id} ä¸å­˜åœ¨äº research_tasks ä¸­")

        # æ­¥éª¤2: æ›´æ–°çŠ¶æ€ä¸ºç ”ç©¶ä¸­
        logger.info(f"ğŸ” [STEP_2] å¼€å§‹æ‰§è¡Œç ”ç©¶æµç¨‹...")
        research_tasks[research_id].status = "researching"
        research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()

        # æ­¥éª¤3: æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹
        logger.info(f"âš¡ [STEP_3] è°ƒç”¨ç¼–æ’å™¨å¤„ç†ç ”ç©¶è¯·æ±‚...")
        start_time = datetime.now()
        result = await enh_orchestrator.process_research_request(
            question=question,
            user_context=user_context,
            allow_clarification=allow_clarification,
            progress_callback=progress_callback
        )
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # LangSmith è¿½è¸ªå®Œæˆ
        log_research_complete(question, duration, len(result.key_findings))

        logger.info(f"âœ… [STEP_3] ç ”ç©¶æµç¨‹æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        logger.info(f"ğŸ“‹ [RESULT] æœ€ç»ˆçŠ¶æ€: {result.status}, è¿›åº¦: {result.progress:.1f}%")

        # æ­¥éª¤4: ä¿å­˜æœ€ç»ˆç»“æœ
        logger.info(f"ğŸ’¾ [STEP_4] ä¿å­˜ç ”ç©¶ç»“æœ...")
        result.metadata["updated_at"] = datetime.now().isoformat()
        result.metadata["execution_duration"] = duration
        research_tasks[research_id] = result

        logger.info(f"ğŸ‰ [TASK_COMPLETE] ç ”ç©¶ä»»åŠ¡ {research_id} æˆåŠŸå®Œæˆï¼")
        logger.info(f"ğŸ“Š [FINAL_STATS] çŠ¶æ€: {result.status}, å…³é”®å‘ç°: {len(result.key_findings)}ä¸ª, æ—¶é•¿: {duration:.2f}ç§’")

    except Exception as e:
        logger.error(f"ğŸ’¥ [TASK_FAILED] ç ”ç©¶ä»»åŠ¡ {research_id} æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(f"ğŸ” [ERROR_DETAILS] å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
        
        # æ›´æ–°å¤±è´¥çŠ¶æ€
        if research_id in research_tasks:
            research_tasks[research_id].status = "failed"
            research_tasks[research_id].metadata["error"] = str(e)
            research_tasks[research_id].metadata["error_traceback"] = traceback.format_exc()
            research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
            logger.error(f"âŒ [STATUS_UPDATE] å·²æ›´æ–°å¤±è´¥çŠ¶æ€ï¼Œresearch_id={research_id}")
        else:
            logger.error(f"âŒ [CRITICAL] ä»»åŠ¡ {research_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°å¤±è´¥çŠ¶æ€")


def get_research_task(research_id: str) -> Optional[ResearchResult]:
    """è·å–ç ”ç©¶ä»»åŠ¡çŠ¶æ€"""
    return research_tasks.get(research_id)


def get_all_research_tasks() -> Dict[str, ResearchResult]:
    """è·å–æ‰€æœ‰ç ”ç©¶ä»»åŠ¡çŠ¶æ€"""
    return research_tasks.copy()


def get_active_research_tasks() -> Dict[str, ResearchResult]:
    """è·å–æ´»è·ƒçš„ç ”ç©¶ä»»åŠ¡ï¼ˆéå®ŒæˆçŠ¶æ€ï¼‰"""
    return {
        task_id: task 
        for task_id, task in research_tasks.items() 
        if task.status not in ["completed", "failed"]
    }


async def execute_research_task_sync(
    research_id: str,
    question: str,
    user_context: Optional[Dict[str, Any]] = None,
    allow_clarification: bool = False,
    research_depth: str = "comprehensive"
) -> ResearchResult:
    """åŒæ­¥æ‰§è¡Œç ”ç©¶ä»»åŠ¡ï¼ˆé˜»å¡å¼ï¼‰"""
    logger.info(f"ğŸš€ [SYNC_START] å¼€å§‹åŒæ­¥æ‰§è¡Œç ”ç©¶ä»»åŠ¡ {research_id}")
    logger.info(f"ğŸ“ [SYNC_INFO] é—®é¢˜: {question}")
    logger.info(f"âš™ï¸ [SYNC_CONFIG] æ¾„æ¸…={allow_clarification}, æ·±åº¦={research_depth}")
    
    # åˆ›å»ºä»»åŠ¡è®°å½•
    initial_result = ResearchResult(
        question=question,
        status="starting",
        progress=0.0,
        metadata={
            "research_id": research_id,
            "user_id": "test_user",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "request_clarification": allow_clarification,
            "research_depth": research_depth
        }
    )
    research_tasks[research_id] = initial_result
    logger.info(f"ğŸ“ [TASK_CREATED] åˆ›å»ºç ”ç©¶ä»»åŠ¡ {research_id}")
    
    try:
        # æ­¥éª¤1: è·å–ç¼–æ’å™¨
        logger.info(f"ğŸ”§ [STEP_1] æ­£åœ¨è·å–ç¼–æ’å™¨å®ä¾‹...")
        enh_orchestrator = await get_orchestrator(research_depth)
        logger.info(f"âœ… [STEP_1] ç¼–æ’å™¨è·å–æˆåŠŸ")
        
        # æ›´æ–°è¿›åº¦
        research_tasks[research_id].status = "initializing"
        research_tasks[research_id].progress = 10.0
        research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
        logger.info(f"ğŸ”„ [PROGRESS] åˆå§‹åŒ–å®Œæˆ (10%)")

        # æ­¥éª¤2: å¼€å§‹ç ”ç©¶æµç¨‹
        logger.info(f"ğŸ” [STEP_2] å¼€å§‹æ‰§è¡Œç ”ç©¶æµç¨‹...")
        research_tasks[research_id].status = "researching"
        research_tasks[research_id].progress = 20.0
        research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()

        # åˆ›å»ºç®€å•çš„è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå¤„ç†æµ®ç‚¹æ•°è¿›åº¦ï¼‰
        def simple_progress_callback(progress_value):
            if isinstance(progress_value, (int, float)):
                logger.info(f"ğŸ“Š [PROGRESS] ä»»åŠ¡ {research_id} è¿›åº¦æ›´æ–°: {progress_value:.1f}%")
                if research_id in research_tasks:
                    research_tasks[research_id].progress = float(progress_value)
                    research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
            else:
                logger.info(f"ğŸ“Š [PROGRESS] ä»»åŠ¡ {research_id} çŠ¶æ€æ›´æ–°: {progress_value}")

        # æ­¥éª¤3: æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹
        logger.info(f"âš¡ [STEP_3] è°ƒç”¨ç¼–æ’å™¨å¤„ç†ç ”ç©¶è¯·æ±‚...")
        start_time = datetime.now()
        result = await enh_orchestrator.process_research_request(
            question=question,
            user_context=user_context,
            allow_clarification=allow_clarification,
            progress_callback=simple_progress_callback
        )
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info(f"âœ… [STEP_3] ç ”ç©¶æµç¨‹æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        logger.info(f"ğŸ“‹ [RESULT] æœ€ç»ˆçŠ¶æ€: {result.status}, è¿›åº¦: {result.progress:.1f}%")

        # æ­¥éª¤4: ä¿å­˜æœ€ç»ˆç»“æœ
        logger.info(f"ğŸ’¾ [STEP_4] ä¿å­˜ç ”ç©¶ç»“æœ...")
        result.metadata["updated_at"] = datetime.now().isoformat()
        result.metadata["execution_duration"] = duration
        research_tasks[research_id] = result

        logger.info(f"ğŸ‰ [SYNC_COMPLETE] ç ”ç©¶ä»»åŠ¡ {research_id} æˆåŠŸå®Œæˆï¼")
        logger.info(f"ğŸ“Š [FINAL_STATS] çŠ¶æ€: {result.status}, å…³é”®å‘ç°: {len(result.key_findings)}ä¸ª, æ—¶é•¿: {duration:.2f}ç§’")
        
        return result

    except Exception as e:
        logger.error(f"ğŸ’¥ [SYNC_FAILED] ç ”ç©¶ä»»åŠ¡ {research_id} æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(f"ğŸ” [ERROR_DETAILS] å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
        
        # æ›´æ–°å¤±è´¥çŠ¶æ€
        if research_id in research_tasks:
            research_tasks[research_id].status = "failed"
            research_tasks[research_id].metadata["error"] = str(e)
            research_tasks[research_id].metadata["error_traceback"] = traceback.format_exc()
            research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
            return research_tasks[research_id]
        else:
            # åˆ›å»ºä¸€ä¸ªå¤±è´¥çš„ç»“æœ
            return ResearchResult(
                question=question,
                status="failed",
                progress=0.0,
                metadata={
                    "research_id": research_id,
                    "error": str(e),
                    "error_traceback": traceback.format_exc(),
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat()
                }
            )


def build_memory_prompt(question: str, memories: List[Dict[str, Any]]) -> str:
    """
    å°†ç”¨æˆ·è®°å¿†è½¬æ¢ä¸ºç ”ç©¶æç¤º
    
    Args:
        question: ç ”ç©¶é—®é¢˜
        memories: ç”¨æˆ·è®°å¿†åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„è®°å¿†æç¤ºå­—ç¬¦ä¸²
    """
    if not memories:
        return ""

    prompt_parts = ["=== ç›¸å…³å†å²ç ”ç©¶è®°å¿† ==="]

    for i, memory in enumerate(memories[:5], 1):  # é™åˆ¶5æ¡æœ€ç›¸å…³çš„
        content = memory.get("memory", "")
        if isinstance(content, dict):
            content = content.get("content", str(content))

        # æå–å…³é”®ä¿¡æ¯
        metadata = memory.get("metadata", {})
        if metadata.get("type") == "research_result":
            # è¿™æ˜¯ç ”ç©¶ç±»å‹çš„è®°å¿†
            prompt_parts.append(f"{i}. ç ”ç©¶ä¸»é¢˜: {metadata.get('question', 'æœªçŸ¥ä¸»é¢˜')}")
            if metadata.get("key_findings_count", 0) > 0:
                prompt_parts.append(f"   å…³é”®å‘ç°æ•°: {metadata['key_findings_count']}")
            if metadata.get("quality_score"):
                prompt_parts.append(f"   ç ”ç©¶è´¨é‡: {metadata['quality_score']:.1f}/10")
        else:
            # æ™®é€šè®°å¿†
            prompt_parts.append(f"{i}. {content[:200]}...")  # é™åˆ¶é•¿åº¦

    prompt_parts.append("=== è¯·åŸºäºä»¥ä¸Šå†å²ç ”ç©¶ï¼Œé¿å…é‡å¤å†…å®¹ï¼Œæä¾›æ–°çš„è§è§£ ===")

    return "\n".join(prompt_parts)


async def save_research_memory(
    user_id: str,
    research_id: str,
    question: str,
    result: ResearchResult,
    memory_service
) -> bool:
    """
    ä¿å­˜ç ”ç©¶è®°å¿†åˆ° Mem0 ç³»ç»Ÿ
    
    Args:
        user_id: ç”¨æˆ·IDï¼ˆå·²è®¤è¯ç”¨æˆ·ï¼‰
        research_id: ç ”ç©¶ä»»åŠ¡ID
        question: ç ”ç©¶é—®é¢˜
        result: ç ”ç©¶ç»“æœå¯¹è±¡
        memory_service: Mem0è®°å¿†æœåŠ¡å®ä¾‹
        
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        logger.info(f"ğŸ’¾ [MEMORY_SAVE] å¼€å§‹ä¿å­˜ç ”ç©¶è®°å¿†: {research_id}")

        # æ„å»ºè®°å¿†å†…å®¹
        content = f"""ç ”ç©¶ä¸»é¢˜: {question}

ç ”ç©¶æŠ¥å‘Š:
{result.final_report[:2000] if result.final_report else 'æŠ¥å‘Šä¸ºç©º'}...

å…³é”®å‘ç°:
{chr(10).join(f"- {finding}" for finding in result.key_findings[:10])}

ç ”ç©¶è´¨é‡: {result.metadata.get('quality_score', 0):.1f}/10
ç ”ç©¶æ—¶é•¿: {result.metadata.get('duration', 0):.1f}ç§’
å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "research_id": research_id,
            "question": question,
            "status": result.status,
            "key_findings_count": len(result.key_findings),
            "quality_score": result.metadata.get("quality_score"),
            "duration": result.metadata.get("duration"),
            "created_at": datetime.now().isoformat(),
            "type": "research_result",
            "word_count": len(result.final_report.split()) if result.final_report else 0,
            "finding_count": len(result.key_findings)
        }

        # ä½¿ç”¨LLMæ™ºèƒ½æŠ½å–ä¿å­˜
        save_result = await memory_service.add_memory(
            user_id=user_id,
            content=content,
            metadata=metadata,
            infer=True  # å¯ç”¨æ™ºèƒ½æŠ½å–
        )

        if save_result.get("success"):
            logger.info(f"âœ… [MEMORY_SAVE] ç ”ç©¶è®°å¿†ä¿å­˜æˆåŠŸ: {research_id}")
            return True
        else:
            logger.error(f"âŒ [MEMORY_SAVE] ç ”ç©¶è®°å¿†ä¿å­˜å¤±è´¥: {save_result.get('error')}")
            return False

    except Exception as e:
        logger.error(f"ğŸ’¥ [MEMORY_SAVE] ä¿å­˜ç ”ç©¶è®°å¿†å¼‚å¸¸: {e}")
        import traceback
        logger.error(f"ğŸ” [MEMORY_SAVE_ERROR] å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
        return False


async def execute_research_task_stream(
    research_id: str,
    question: str,
    user_context: Optional[Dict[str, Any]] = None,
    allow_clarification: bool = False,
    research_depth: str = "comprehensive",
    memory_mode: str = "smart",
    memory_service=None
) -> AsyncGenerator[Dict[str, Any], None]:
    """æµå¼æ‰§è¡Œç ”ç©¶ä»»åŠ¡ï¼Œæ”¯æŒè®°å¿†åŠŸèƒ½"""
    logger.info(f"ğŸš€ [STREAM_START] å¼€å§‹æµå¼æ‰§è¡Œç ”ç©¶ä»»åŠ¡ {research_id}, è®°å¿†æ¨¡å¼: {memory_mode}")

    # å¤„ç†è®°å¿†å¢å¼ºçš„ä¸Šä¸‹æ–‡
    enhanced_context = user_context or {}

    # å¦‚æœæœ‰ç”¨æˆ·è®°å¿†ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­å¹¶æ„å»ºè®°å¿†æç¤º
    if memory_service and memory_mode != "none" and enhanced_context.get("user_memories"):
        memories = enhanced_context["user_memories"]
        # ç¡®ä¿ memories æ˜¯åˆ—è¡¨ç±»å‹
        if isinstance(memories, list) and memories:
            # å°†è®°å¿†è½¬æ¢ä¸ºç ”ç©¶æç¤ºï¼ˆbuild_memory_promptå‡½æ•°åœ¨ä¸Šé¢å®šä¹‰ï¼‰
            memory_prompt = build_memory_prompt(question, memories)
            enhanced_context["memory_prompt"] = memory_prompt
            enhanced_context["has_memories"] = True
            logger.info(f"ğŸ§  [MEMORY] å·²æ·»åŠ è®°å¿†æç¤ºï¼Œé•¿åº¦: {len(memory_prompt)} å­—ç¬¦")
        else:
            enhanced_context["has_memories"] = False
            logger.warning(f"ğŸ§  [MEMORY] è®°å¿†æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {type(memories)}")
    
    try:
        # æ­¥éª¤1: å‘é€åˆå§‹åŒ–ä¿¡æ¯
        init_data = {
            'type': 'progress',
            'stage': 'initializing',
            'progress': 5.0,
            'message': 'ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç ”ç©¶ç³»ç»Ÿ...',
            'details': 'è·å–ç¼–æ’å™¨å®ä¾‹ï¼Œé…ç½®ç ”ç©¶å‚æ•°'
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: åˆå§‹åŒ–é˜¶æ®µ (5%)")
        yield init_data
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        initial_result = ResearchResult(
            question=question,
            status="initializing",
            progress=5.0,
            metadata={
                "research_id": research_id,
                "user_id": "test_user",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "request_clarification": allow_clarification,
                "research_depth": research_depth
            }
        )
        research_tasks[research_id] = initial_result
        
        # æ­¥éª¤2: è·å–ç¼–æ’å™¨
        setup_data = {
            'type': 'progress',
            'stage': 'setup',
            'progress': 10.0,
            'message': 'âš™ï¸ æ­£åœ¨é…ç½®ç ”ç©¶ç¯å¢ƒ...',
            'details': 'åˆå§‹åŒ–Open Deep Researchç¼–æ’å™¨'
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: é…ç½®é˜¶æ®µ (10%)")
        yield setup_data
        
        logger.info(f"ğŸ”§ [STREAM_STEP] {research_id}: æ­£åœ¨è·å–ç¼–æ’å™¨å®ä¾‹...")
        enh_orchestrator = await get_orchestrator(research_depth)
        logger.info(f"âœ… [STREAM_STEP] {research_id}: ç¼–æ’å™¨è·å–æˆåŠŸ")
        
        analyze_data = {
            'type': 'progress',
            'stage': 'analyzing',
            'progress': 15.0,
            'message': 'ğŸ” æ­£åœ¨åˆ†æç ”ç©¶é—®é¢˜...',
            'details': f'é—®é¢˜: {question[:50]}...'
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: åˆ†æé˜¶æ®µ (15%)")
        yield analyze_data
        
        # æ­¥éª¤3: å¼€å§‹ç ”ç©¶æµç¨‹ (è¿›å…¥ LangGraph æµå¼æ‰§è¡Œ)
        research_data = {
            'type': 'progress',
            'stage': 'researching',
            'progress': 20.0,
            'message': 'ğŸš€ å¼€å§‹æ‰§è¡Œç ”ç©¶æµç¨‹',
            'details': 'è¿›å…¥LangGraphå·¥ä½œæµï¼Œå®æ—¶è¾“å‡ºæ‰§è¡Œè¿›åº¦'
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: ç ”ç©¶é˜¶æ®µå¼€å§‹ (20%)")
        yield research_data
        
        logger.info(f"âš¡ [STREAM_STEP] {research_id}: å¼€å§‹è°ƒç”¨ç¼–æ’å™¨æµå¼å¤„ç†...")
        start_time = datetime.now()
        
        final_result_obj = None  # ç”¨äºä¿å­˜æœ€ç»ˆç»“æœå¯¹è±¡
        
        # æµå¼æ¥æ”¶ LangGraph çš„æ‰§è¡Œè¿›åº¦
        # æ³¨æ„ï¼šä½¿ç”¨enhanced_contextï¼ŒåŒ…å«memory_prompt
        async for progress_data in enh_orchestrator.process_research_request_stream(
            question=question,
            user_context=enhanced_context,  # åŒ…å«memory_promptå’Œhas_memories
            allow_clarification=allow_clarification
        ):
            # å°†å†…éƒ¨è¿›åº¦ï¼ˆ0-100ï¼‰æ˜ å°„åˆ°å¤–éƒ¨è¿›åº¦ï¼ˆ20-95ï¼‰
            if progress_data.get('type') == 'progress':
                internal_progress = progress_data.get('progress', 0)
                # æ˜ å°„åˆ° 20-95% åŒºé—´ï¼ˆç•™5%ç»™æœ€åçš„å®Œæˆä¿¡æ¯ï¼‰
                mapped_progress = 20 + (internal_progress / 100.0) * 75
                progress_data['progress'] = mapped_progress
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                if research_id in research_tasks:
                    research_tasks[research_id].progress = mapped_progress
                    research_tasks[research_id].metadata["updated_at"] = datetime.now().isoformat()
                
                # è®°å½•æ—¥å¿—ï¼ˆåªè®°å½•å…³é”®ä¿¡æ¯ï¼Œè¯¦ç»†ä¿¡æ¯åœ¨ DEBUG æ¨¡å¼ï¼‰
                message = progress_data.get('message', '')
                stage = progress_data.get('stage', '')
                
                # INFO çº§åˆ«ï¼šç®€æ´ä¿¡æ¯
                logger.info(f"[{research_id[:20]}...] {mapped_progress:5.1f}% | {message}")
                
                # DEBUG çº§åˆ«ï¼šè¯¦ç»†ä¿¡æ¯
                if logger.isEnabledFor(logging.DEBUG):
                    pass  # âœ… è¯¦ç»†æ—¥å¿—å·²æ³¨é‡Šï¼ŒåŠŸèƒ½å·²éªŒè¯æ­£å¸¸
                    # logger.debug(f"[PROGRESS_DETAIL] Stage: {stage}")
                    # logger.debug(f"[PROGRESS_DETAIL] Data: {progress_data}")
                
                # è½¬å‘ç»™å‰ç«¯
                yield progress_data
            
            elif progress_data.get('type') == 'result':
                # ä¿å­˜æœ€ç»ˆç»“æœå¯¹è±¡ï¼Œç”¨äºåç»­è®°å¿†ä¿å­˜
                result_data = progress_data.get('result')
                if result_data:
                    # ç¡®ä¿result_dataæ˜¯ResearchResultå¯¹è±¡
                    if isinstance(result_data, ResearchResult):
                        final_result_obj = result_data
                    else:
                        # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºResearchResult
                        final_result_obj = ResearchResult(
                            question=result_data.get('question', question),
                            final_report=result_data.get('final_report', ''),
                            status=result_data.get('status', 'completed'),
                            key_findings=result_data.get('key_findings', []),
                            raw_notes=result_data.get('raw_notes', []),
                            metadata=result_data.get('metadata', {}),
                            progress=result_data.get('progress', 100.0)
                        )
                # å°†æœ€ç»ˆç»“æœä¹ŸåŠ å…¥åˆ°è¿›åº¦æ•°æ®ä¸­ï¼Œæ–¹ä¾¿è·¯ç”±å±‚è·å–
                progress_data['final_result'] = final_result_obj
                logger.info(f"ğŸ“‹ [STREAM_RESULT] {research_id}: ç ”ç©¶å®Œæˆ")
            
            elif progress_data.get('type') == 'error':
                # é”™è¯¯ç›´æ¥è½¬å‘
                logger.error(f"ğŸ’¥ [STREAM_ERROR] {research_id}: {progress_data.get('message', '')}")
                yield progress_data
                return  # é”™è¯¯æ—¶ç›´æ¥è¿”å›
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"âœ… [STREAM_STEP] {research_id}: ç¼–æ’å™¨æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {duration:.1f}ç§’")
        
        # å¦‚æœæ²¡æœ‰è·å–åˆ°æœ€ç»ˆç»“æœï¼Œåˆ›å»ºä¸€ä¸ªé”™è¯¯ç»“æœ
        if not final_result_obj:
            logger.warning(f"âš ï¸ [STREAM_WARN] {research_id}: æœªè·å–åˆ°æœ€ç»ˆç»“æœ")
            yield {
                'type': 'error',
                'stage': 'failed',
                'message': 'âŒ ç ”ç©¶æœªå®Œæˆï¼šæœªè·å–åˆ°æœ€ç»ˆç»“æœ',
                'error': 'No final result received'
            }
            return
        
        # ä½¿ç”¨å·²ä¿å­˜çš„æœ€ç»ˆç»“æœå¯¹è±¡
        result = final_result_obj
        
        # æ­¥éª¤4: å‘é€å®Œæˆä¿¡æ¯
        complete_data = {
            'type': 'progress',
            'stage': 'completed',
            'progress': 95.0,
            'message': 'ğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...',
            'details': f'ç ”ç©¶è€—æ—¶: {duration:.1f}ç§’'
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ (95%)")
        yield complete_data
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        result.metadata["updated_at"] = datetime.now().isoformat()
        result.metadata["execution_duration"] = duration
        research_tasks[research_id] = result
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = min(100.0, (len(result.key_findings) * 5 + len(result.final_report or "") / 100))
        logger.info(f"ğŸ“Š [STREAM_STATS] {research_id}: è´¨é‡è¯„åˆ† {quality_score:.1f}åˆ†ï¼Œå…³é”®å‘ç° {len(result.key_findings)}ä¸ª")
        
        # å‘é€æœ€ç»ˆç»“æœ
        result_data = {
            'type': 'result',
            'stage': 'completed',
            'progress': 100.0,
            'message': 'âœ… ç ”ç©¶ä»»åŠ¡å®Œæˆï¼',
            'details': f'è´¨é‡è¯„åˆ†: {quality_score:.1f}åˆ†ï¼Œå…³é”®å‘ç°: {len(result.key_findings)}ä¸ª',
            'research_id': research_id,
            'question': question,
            'status': result.status,
            'final_report': result.final_report or "ç ”ç©¶æœªå®Œæˆ",
            'key_findings': result.key_findings,
            'metadata': result.metadata,
            'quality_score': quality_score,
            'duration': duration,
            'created_at': result.metadata.get("created_at", datetime.now().isoformat()),
            'result': result,  # æ·»åŠ ResearchResultå¯¹è±¡
            'final_result': result  # åŒæ—¶æ·»åŠ final_resultå­—æ®µï¼Œæ–¹ä¾¿è·¯ç”±å±‚è·å–
        }
        logger.info(f"ğŸ“¤ [STREAM_YIELD] {research_id}: æœ€ç»ˆç»“æœ (100%)")
        logger.info(f"ğŸ“„ [STREAM_REPORT] {research_id}: æŠ¥å‘Šé•¿åº¦ {len(result.final_report or '')} å­—ç¬¦")
        yield result_data
        
        logger.info(f"ğŸ‰ [STREAM_COMPLETE] æµå¼ç ”ç©¶ä»»åŠ¡ {research_id} æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [STREAM_FAILED] æµå¼ç ”ç©¶ä»»åŠ¡ {research_id} å¤±è´¥: {e}")
        import traceback
        logger.error(f"ğŸ” [ERROR_DETAILS] å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
        
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_data = {
            'type': 'error',
            'stage': 'failed',
            'progress': 0.0,
            'message': f'âŒ ç ”ç©¶ä»»åŠ¡å¤±è´¥: {str(e)}',
            'details': 'è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œå‚æ•°é…ç½®',
            'research_id': research_id,
            'error': str(e)
        }
        logger.error(f"ğŸ“¤ [STREAM_ERROR] {research_id}: å‘é€é”™è¯¯å“åº”")
        yield error_data


def create_research_task(research_id: str, question: str, **kwargs) -> ResearchResult:
    """åˆ›å»ºç ”ç©¶ä»»åŠ¡"""
    initial_result = ResearchResult(
        question=question,
        status="initializing",
        progress=0.0,  # åˆå§‹è¿›åº¦ä¸º0
        metadata={
            "research_id": research_id,
            "user_id": "test_user",  # æš‚æ—¶ä½¿ç”¨å›ºå®šç”¨æˆ·ID
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            **kwargs
        }
    )
    research_tasks[research_id] = initial_result
    logger.info(f"ğŸ“ [TASK_CREATED] åˆ›å»ºç ”ç©¶ä»»åŠ¡ {research_id}: {question}")
    return initial_result
